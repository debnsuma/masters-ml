{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJZ-3vAUjCBO"
   },
   "source": [
    "**Content**\n",
    "\n",
    "1.   ADAM (Adaptive Moment Estimation) Optimizer.\n",
    "2.   Weight Initialization.\n",
    "3.   Auto Encoders (AE):\n",
    "     - Dimensionality Reduction using AE.\n",
    "     - Denoising AE.\n",
    "     - Misc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpGMJxaGWzJS"
   },
   "source": [
    "### ADAM (Adaptive Moment Estimation) Optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7gpnlOcVPUg"
   },
   "source": [
    "There are a lot of optimizers available for DL. The most popular and widely used algorithm is ADAM. So we will deep-dive into ADAM optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxmFMMerUzui"
   },
   "source": [
    "#### Quick Recap: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJkTSWHyxKnS"
   },
   "source": [
    "Let's understand working of Stochastic Gradient Descent.\n",
    "\n",
    "- Imagine we have a mini-batch SGD. In each iteration, we take random subset of $k$ points.\n",
    "- GD update is computed as:\n",
    "$$w^k_{ij_{new}} = w^k_{ij_{old}} - \\eta \\frac{\\partial L}{\\partial w^k_{ij}} \\bigg|_{old}$$\n",
    "for $t$ such iterations and $\\eta$ = learning rate.\n",
    "  <img src='https://drive.google.com/uc?id=1nodMadzHegJvUKusWjOjCI4pIvwPCq75'>\n",
    "- In mini-batch, we compute these derivates in back propogation.\n",
    "- Rather than doing it for all $n$ points, we estimate it for batch of $k$ points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQBWG-IN0E1D"
   },
   "source": [
    "#### Motivation behind ADAM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj93p4RhwIE3"
   },
   "source": [
    "- Imagine we have a contour plot of weights ($w_1$, $w_2$) as shown below plotted against $Loss$. This looks like a paraboloid.\n",
    "  <img src='https://drive.google.com/uc?id=1fKMFK2ZokNH5JINA2-ssYv7LsCfsStKy'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsPgNM0Iw-oG"
   },
   "source": [
    "- Initially, the weights ($w_1$, $w_2)$ are initialized randomly and we want to move towards minima.\n",
    "  <img src='https://drive.google.com/uc?id=11iUh7u1f8BiRLigLC_tzE4YSgqJAEmtL'>\n",
    "- In GD if we have to compute Loss for all $n$ points, we will move perfectly/gradually towards minima. We will move $\\perp$ to contour towards minima.\n",
    "- But in reality, we may not be able to do GD. So, we often do mini batch SGD with $k$ points. Then the derivative we have is not perfect, but an approximation $\\frac{\\partial \\hat{L}}{\\partial w^k_{ij}} \\bigg|_{old}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZpdiuxEwfyI"
   },
   "source": [
    "- In SGD, it will not move $\\perp$ to contour. It will keep juggling around an eventually move to minima.\n",
    "  <img src='https://drive.google.com/uc?id=1G3GW--bBaoYFqaSLYH1ds2dgjeQ0rbRj'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2fehUZC4Auw"
   },
   "source": [
    "#### Momentum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuoIIXXk3n_S"
   },
   "source": [
    "- There is an interesting concept behind Momentum.  \n",
    "  <img src='https://drive.google.com/uc?id=1OW28zRiZk0ONVZf3U-Md_ZqUmm2gPsot'>\n",
    "- Imagine we have contour like shown in the figure above. The mini-batch gradient descent will eventually take me to minima as shown.\n",
    "- Let say, we have 100 iterations. $t: 1 \\to 100$.\n",
    "- Idea of momentum is that every step $t$, (for simplicity, we will refer to $\\frac{\\partial \\hat{L}}{\\partial w^k_{ij}} \\bigg|_{old}$ as $g_t$ @ iteration $t$).\n",
    "- In each iteration, we have gradient $g_1, g_2,...g_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDycI4As3oF3"
   },
   "source": [
    "- In SGD, $w_{new}$ is computed as $w_{old} - \\eta* g_t$ for $t: 1 \\to 1000$.\n",
    "  <img src='https://drive.google.com/uc?id=1dLVyY7QCZHd1wOuuzeYC5kanq0Z-e4bM'>\n",
    "- So idea is can we somehow use past information about gradients $(g_1, g_2,...g_{t-1})$ to better update and faster update as they direct towards minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwn0m7YU3oLD"
   },
   "source": [
    "- Let say, we have gradients $g_1, g_2..g_5$ computed and now we want to compute $g_6$. Now that know information about $g_1, g_2..g_5$ we know its net direction.\n",
    "  <img src='https://drive.google.com/uc?id=1nofueiCTlefRILnKqO-LYT72ihi7n_n1'>\n",
    "- So now, rather than moving in some random direction, we can move towards minima in a better and faster manner.\n",
    "- We can say that we are using some sort of weighted average of previous gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oqh5nPTh6xCc"
   },
   "source": [
    "***Question:*** How we get direction because we get only numerical values?\n",
    "\n",
    "***Answer:*** No, its just an intuition we are building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vlpcBMf7TTS"
   },
   "source": [
    "#### First Lesson Learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX7xM-W83M37"
   },
   "source": [
    "*Momentum speeds up convergence*\n",
    "\n",
    "- Older gradients $(g_1, g_2, ..g_{t-1})$ along with $g_t$(with higher weightage to $g_t$) are used.\n",
    "  <img src='https://drive.google.com/uc?id=1MQWL_V8s9A76tAdW_W8f46n3-Bb5CbmD'>\n",
    "- This is done using exponential weightage average (EWA). This concept is called $Momentum$.\n",
    "- So while computing $g_6$, we are using $(g_1,..g_5)$. The momentum is used to move towards minima, which we are using to move faster in that direction.\n",
    "- Using momentum, it speeds up convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFQPhxhp8hH5"
   },
   "source": [
    "***Question:*** Is my understanding correct -  Each of the $g_t$ is calculated on a different minibatch of size $k$, so each of the $g_t$'s will provide a different estimate of the same population gradient? Which we then aggregate to get estimate of overall gradient.\n",
    "\n",
    "***Answer:*** $g_2$ is computed using $g_1$. So for each $g_t$, batch is changing and old $g_t$ is changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWPsFcsD9Fui"
   },
   "source": [
    "#### What is Saddle point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhdRYVNKwaqJ"
   },
   "source": [
    "- Imagine  we are at a Saddle point. We have $w_1$, $w_2$ as shown in diagram.\n",
    "  <img src='https://drive.google.com/uc?id=1AR7rp3Fj8P1yS0G1jL0Rfh_De45noL37'>\n",
    "- Momentum helps us to get out of saddle point.\n",
    "- But the question would be, how to determine if you are at saddle point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXp37mOLwoEB"
   },
   "source": [
    "- We can do higher order derivatives, test. But there is a simpler solution.\n",
    "  <img src='https://drive.google.com/uc?id=1tvcvOKnBo0_qNp-M-XaCMT5JtXz3U_Ln'>\n",
    "- Let say, we have $(w^{10}_1, w^{10}_2)$, where derivative becomes 0. Hack here is to add small random delta $(\\delta_1, \\delta_2)$ and we do it few times.\n",
    "- If we observe output of $f(w_1, w_2)$ less than minima, it means that we have maxima on one dimension and minima on another. This implies that we are at Saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFoZObNP-0Gt"
   },
   "source": [
    "***Question:*** So while calculating $W_{ij}$ we give less weightage to other gradients and more weightage to new $g_t$ and then we aggregate?\n",
    "\n",
    "***Answer:*** Yes, that is correct. This is known as EWA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzMjjrtr_BNI"
   },
   "source": [
    "***Question:*** Can it be possible that  it converges at local minima instead of global minima because of some discrepancy in momentum calculation?\n",
    "\n",
    "***Answer:***\n",
    "- Yes, this happens all the time. In Neural Network/DL, we have loss function that is often non-convex i.e. multiple minima.\n",
    "  <img src='https://drive.google.com/uc?id=1KvRQBbs6n5aGfBiCAPC0B8OAenwWu1ke'>\n",
    "- Even if model gets stuck at local minima, its computational complexity is such that we still get better performance. Hence, it is suggested to take multiple random points and compute whichever loss is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afexTnpE_8We"
   },
   "source": [
    "***Question:*** Does initialization of weights affect the momentum or it overcomes the issue during the process?\n",
    "\n",
    "***Answer:*** Initialization of weights does impact final outcome. Momentum depends on where the random points lie initially. If it lies on a steep curve the momentum is faster than if the point lies on flatter curve, so momentum is slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDm-_LZpA1MU"
   },
   "source": [
    "#### ADAM: Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUedRwKwVPa0"
   },
   "source": [
    "- We compute gradient $g_t$ at each $t$ and also square of gradient $g^2_t$. $g_t$ itself is used for momentum through EWA and $g^2_t$ is use to adjust learning rate adaptively.\n",
    "- Initially, learning rate  is $\\alpha \\to 0.001, \\beta_{1} \\to 0.9, \\beta_{2} \\to 0.999, \\epsilon\n",
    "\\to 10^{-8}$.\n",
    "- $\\alpha$ in this algorithm refers to $\\eta$ in our earlier discussions. $\\beta_1$ and $\\beta_2 \\in (0,1)$ and are exponential decay rate. $f(\\theta)$ refers to loss function $L(w)$.\n",
    "  <img src='https://drive.google.com/uc?id=1mzcm4Z5w9rWBa5j3pLDFRvF4qTb-SjBE'>\n",
    "- Randomly, initialize $\\theta$. Initially, momentum vector $m_0$ is zero and iteration $t$ starts with zero.\n",
    "- In each iteration $t$, we compute $g_t$ and update $m_t$ as $\\beta_1 * m_{t-1} + (1-\\beta_1) * g_t$. Notice, that older momentum is getting $90\\%$ weightage and $10\\%$ to new momentum. The idea is newer momentum is approximation and old momentum we already have done $(t-1)$ iterations. Hence information about $(t-1)$ iterations should have higher weightage.\n",
    "- Note that each iteration might point in random direction, but all these $(t-1)$ iterations in aggregrate would be pointing in right direction. This is slight contrast to time series, where we gave higher weightage to recent values. Here, we are giving more weightage to older momentum as it will be more likely to point in right direction.\n",
    "- $\\hat{m}_t$ is computed as $m_t/(1-{\\beta_1}^t)$ ( read ${\\beta_1}^t$ as $\\beta_1$ raised to power $t$) as we want to correct for bias term. This is similar to how we calculate variance in statistics using formula $\\sigma^2 = \\frac{1}{n-1} * \\sum ({x_i} - \\mu)^2 $. We use $(n-1)$ term to remove bias i.e. correction for bias.\n",
    "- Update $\\theta_t$ as $\\theta_{t-1} - \\frac{\\alpha * \\hat{m}_t}{ \\sqrt{\\hat{v}_t} + \\epsilon}$.\n",
    "\n",
    "*Note:* We will take about denominator term $ \\sqrt{\\hat{v}_t} + \\epsilon$ in a short while.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX25KyS3ls2E"
   },
   "source": [
    "#### Second Lesson Learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_UtFfszldLT"
   },
   "source": [
    "*Adjust learning Rate*\n",
    "- As iteration increases, $\\alpha$ should decrease. If the learning rate is not reduced, then we would overshoot.\n",
    "- $g_t$ could be positve or negative. But $g^2_t$ will always be positive and will be very high. But slowly, gradient will reduce as we come closer to minima.\n",
    "  <img src='https://drive.google.com/uc?id=1Wow5YdgkVC7PTNRKpHBoRzvnonFvavwZ'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fh0Ga52jrKhc"
   },
   "source": [
    "- In iteration $t$ we are adopting $\\alpha$ as\n",
    "$$\\alpha_{adj} = \\frac{\\alpha}{\\sqrt{\\sum_{i=1}^t g^2_t} + \\epsilon}$$\n",
    "here $\\epsilon \\to 10^{-8}$ to avoid divide by zero error.\n",
    "  <img src='https://drive.google.com/uc?id=1cR-rP-r66sZ3kDqPrUNejQV8Xd89xN6a'>\n",
    "- As $t$ increases, $\\alpha_{adj}$ is always reducing.\n",
    "- One could argue that why can't we compute $\\alpha_{adj}$ as $\\frac{\\alpha}{t}$. Note that, if gradient at $t$ is large, then we know we have moved faster to minima which means learning rate should reduce more.\n",
    "- So we want to adjust learning rate base on amount of gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSN-9mTbrj4N"
   },
   "source": [
    "***Question:*** If our gradient $g_i$ lies between 0 to 1, then $g^2_i$ is decreasing, then our logic fails, right?\n",
    "\n",
    "***Answer:*** Yes this a boundary case. But typically, gradients have very large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8yPPqKUr5X-"
   },
   "source": [
    "***Question:*** Why not absolute value of gradient instead of gradient squared? both gives positive value right?\n",
    "\n",
    "***Answer:*** Yes, it could be used theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B23Ih-e3tuqU"
   },
   "source": [
    "#### Adam: Algorithm continued.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8y0U0tLsayV"
   },
   "source": [
    "- Now let's look at the $v_0$ momentum vector.\n",
    "  <img src='https://drive.google.com/uc?id=1pwGp1Z0BBOOylOrPBILFDpBLTcHaGEFr'>\n",
    "- Initially, $v_0 \\to 0$.\n",
    "- In every iteration $t$, $v_t$ is computed as $\\beta_2 * v_{t-1} + (1-\\beta_2)*g^2_t$.\n",
    "- $\\hat{v_t}$ is computed as $\\frac{v_t}{(1-\\beta^t_2)}$.\n",
    "- Update $\\theta_t$ as $\\theta_{t-1} - \\frac{\\alpha * \\hat{m}_t}{ \\sqrt{\\hat{v}_t} + \\epsilon}$.\n",
    "\n",
    "*Note:* Values of $\\beta_1, \\beta_2$ are arrived at $0.9$ and $0.999$ after a lot of experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv7jH_P3t4U3"
   },
   "source": [
    "#### Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDHgVGVkrMf6"
   },
   "source": [
    "ADAM has 2 components:\n",
    "- Exponential Weighted Average (EWA) of $g_t$ that gives ***momentum***.\n",
    "- Exponential Weighted Average (EWA) of $g^2_t$ that gives ***adjusted learning rate***.\n",
    "  <img src='https://drive.google.com/uc?id=1fGVnnCz5nISxZnphh9t_ZtGbjBVLYdfU'>\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fXoACuJuudG"
   },
   "source": [
    "### Weight Initialization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-scXItL0ehe"
   },
   "source": [
    "Here are some of the ideas for weight initialization.  \n",
    "  <img src='https://drive.google.com/uc?id=1MOSnqCGT8cXjAT39MNn4YgPeE3zMoUFm'>\n",
    "- We could either use Normal Distribution with small $\\sigma$ or uniform distribution between $-1$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVUIn-8W0ygW"
   },
   "source": [
    "- One of the strategies used to determine weight initialization is to use strategy of $fan-in$ and $fan-out$.\n",
    "- For e.g. imagine we have a neuron with 4 inputs and 2 outputs. Then $fan-in \\to 4$ and $fan-out \\to 2$.\n",
    "  <img src='https://drive.google.com/uc?id=1uP5u_7BXgD6TH-vybVDiQb7BWWyxoQHN'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUmS663e0Eoa"
   },
   "source": [
    "- There are other better strategies that have worked experimentaly but have weak theory supporting it.\n",
    "- Some of these techniques are Xavier Glorot, He.\n",
    "  <img src='https://drive.google.com/uc?id=19-jyJd_y34mHbwNT_vJ3p8elH7O-qvGl'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDV2YmZ50YCb"
   },
   "source": [
    "- Weights could be initialized using uniform distribution as follows:\n",
    "$$w^k_{ij} \\sim Uniform\\bigg[ \\frac{-1}{\\sqrt{fan-in}}, \\frac{1}{\\sqrt{fan-out}}\\bigg]$$  \n",
    "- These techniques were used around early $2010's$.\n",
    "  <img src='https://drive.google.com/uc?id=1IR1CIZB68-WPAX_5l6aadn2jTao2mQbG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnw2Yy9Lu2sN"
   },
   "source": [
    "- Glorot initialization states that we could initialization weights using either of following approaches:\n",
    "  <img src='https://drive.google.com/uc?id=13EGBQPBRbbc_WafqmvOqS5x_4JkH8nm9'>\n",
    "- Normal Distribution: $w^k_{ij} \\sim N(0,\\sigma_{ij}), where \\sigma_{ij} = \\frac{2}{fanin+fanout}$\n",
    "- Uniform Distribution: $w^k_{ij} \\sim Uniform\\bigg[ \\frac{-\\sqrt{6}}{\\sqrt{fanin+fanout}}, \\frac{\\sqrt{6}}{\\sqrt{fanin+fanout}}\\bigg]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLEb9lxk0mhM"
   },
   "source": [
    "- He initialization was designed around $2015$. The formula is as follows:\n",
    "  <img src='https://drive.google.com/uc?id=1Ozm8i15zpHaTuUNjJ1VRINeUsjA1GEvh'>\n",
    "- Normal Distribution: $N(0,\\sigma)$, where $\\sigma = \\frac{2}{fanin}$\n",
    "- Uniform Distribution: $Uniform\\bigg[ \\frac{-\\sqrt{6}}{\\sqrt{fanin}}, \\frac{\\sqrt{6}}{\\sqrt{fanin}}\\bigg]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIq-hFp_0Pwl"
   },
   "source": [
    "- Keras implementation, weight initializer is *glorot_uniform* and bias is initialized to zero.\n",
    "  <img src='https://drive.google.com/uc?id=19TEgRqK_Qwxm9EGtzpIWAUJIohpPO4wq'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Syzi008w0S"
   },
   "source": [
    "### AutoEncoders:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5plYcn1-CA5O"
   },
   "source": [
    "- Suppose we have MNIST dataset where each image is of size $28*28$ which is converted to $784*1$.\n",
    "- We can somehow pass it to a MLP and get $32-dim$ vector which represents all information in $784- dim$ data.\n",
    "- This $32-dim$ is again passed through another MLP that will give output of $784-dim$ data which is similar to original data.\n",
    "  <img src='https://drive.google.com/uc?id=1Kg_J6FvWXP9X9dcnP61g6I1IuipGr2XZ'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM2sWonyCA_P"
   },
   "source": [
    "- We do similar thing in PCA, where input $x_i$ is multiplied with matrix of eigen vectors $A$ and we get $32*1$ vector of top eigen values.  \n",
    "  <img src='https://drive.google.com/uc?id=1ip9etJFbltNhEr-t_X3GJ0l5XT1ju8_C'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NKh0n3SB84C"
   },
   "source": [
    "- We can think of PCA as special case of AutoEncoders. This could be thought of intuitively.\n",
    "  <img src='https://drive.google.com/uc?id=19Mmrwuu7QIdQILEYNJwlZIPgQrwWbL_M'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knywQ6psF8q7"
   },
   "source": [
    "***Question:*** How we justify 32 represent original? like what loss we used here?\n",
    "\n",
    "***Answer:*** If we can somehow recover information about $784$ dimensions from $32$-dim, then we can say that information about $784$ dimensions is compressed into $32$ dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFSrHcKfCA8t"
   },
   "source": [
    "- Here, is a simple architecture of an AutoEncoder.\n",
    "  <img src='https://drive.google.com/uc?id=1a9z0K4hqHXPycK-CKRZf80N4TNaPssXX'>\n",
    "- Imagine we have MNIST images as input $x_i \\in 784$. Note that these images are normalized. So each pixel value is between 0 and 1.\n",
    "- We have a fully connected layer with $128$ neurons, followed by another fully connected layer with $64$ neurons and another fully connected layer with $32$ neurons. These 3 layers use ReLu activation units. All these 3 layers collectively are known as Encoder.\n",
    "- The output of Encoder is $32$-dim. This is fed to a series of fully connected layers with $64,128,784$ neurons respectively. Final layer has Sigmoid as activation function.\n",
    "- We need Sigmoid because input was normalized between 0 to 1 in the beginning.\n",
    "- If the input values $\\approx$ output values, then the loss will be very small.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4GmDyOlN_x6"
   },
   "source": [
    "#### Code: AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoVw5CBHOF23",
    "outputId": "9d9c8c62-9372-4331-b08e-ae0bd23a00d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Source and Reference: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "#Normalization of input\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "#Reshaping the images to 1D vectors\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "09hgv2e3OF6e"
   },
   "outputs": [],
   "source": [
    "#AutoEncoder model\n",
    "input_img = keras.Input(shape=(784,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_img)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBg3EiyOOk_D"
   },
   "source": [
    "- We have 3 layers in Encoder section. Each with $128,64,32$ neurons and ReLu as activation function.\n",
    "- Decoder section has 3 layers. First 2 layers has ReLu activation function with $64,128$ neurons with Relu activation. Final layer has $784$ neurons with Sigmoid as activation.\n",
    "- We are using binary crossentropy, as we want to compare input and output that lies between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FGAfKHGOF-l",
    "outputId": "6d3c12d3-075b-4b1d-cfaa-46f331850b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3411 - val_loss: 0.1705\n",
      "Epoch 2/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1607 - val_loss: 0.1393\n",
      "Epoch 3/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1363 - val_loss: 0.1261\n",
      "Epoch 4/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1246 - val_loss: 0.1181\n",
      "Epoch 5/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1181 - val_loss: 0.1129\n",
      "Epoch 6/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1129 - val_loss: 0.1085\n",
      "Epoch 7/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1092 - val_loss: 0.1059\n",
      "Epoch 8/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1063 - val_loss: 0.1040\n",
      "Epoch 9/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1047 - val_loss: 0.1020\n",
      "Epoch 10/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1030 - val_loss: 0.1009\n",
      "Epoch 11/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1014 - val_loss: 0.0997\n",
      "Epoch 12/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1001 - val_loss: 0.0992\n",
      "Epoch 13/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0992 - val_loss: 0.0973\n",
      "Epoch 14/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0983 - val_loss: 0.0964\n",
      "Epoch 15/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0971 - val_loss: 0.0959\n",
      "Epoch 16/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0967 - val_loss: 0.0956\n",
      "Epoch 17/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0960 - val_loss: 0.0945\n",
      "Epoch 18/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0952 - val_loss: 0.0940\n",
      "Epoch 19/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0948 - val_loss: 0.0934\n",
      "Epoch 20/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0942 - val_loss: 0.0930\n",
      "Epoch 21/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0932 - val_loss: 0.0924\n",
      "Epoch 22/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0929 - val_loss: 0.0921\n",
      "Epoch 23/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0925 - val_loss: 0.0915\n",
      "Epoch 24/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0920 - val_loss: 0.0912\n",
      "Epoch 25/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0918 - val_loss: 0.0911\n",
      "Epoch 26/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0914 - val_loss: 0.0909\n",
      "Epoch 27/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0912 - val_loss: 0.0903\n",
      "Epoch 28/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0907 - val_loss: 0.0901\n",
      "Epoch 29/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0903 - val_loss: 0.0898\n",
      "Epoch 30/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0903 - val_loss: 0.0894\n",
      "Epoch 31/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0901 - val_loss: 0.0895\n",
      "Epoch 32/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0899 - val_loss: 0.0892\n",
      "Epoch 33/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0896 - val_loss: 0.0890\n",
      "Epoch 34/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0895 - val_loss: 0.0889\n",
      "Epoch 35/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0892 - val_loss: 0.0885\n",
      "Epoch 36/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0888 - val_loss: 0.0881\n",
      "Epoch 37/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0886 - val_loss: 0.0877\n",
      "Epoch 38/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0882 - val_loss: 0.0875\n",
      "Epoch 39/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0880 - val_loss: 0.0871\n",
      "Epoch 40/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0878 - val_loss: 0.0870\n",
      "Epoch 41/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0875 - val_loss: 0.0871\n",
      "Epoch 42/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0875 - val_loss: 0.0869\n",
      "Epoch 43/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0871 - val_loss: 0.0868\n",
      "Epoch 44/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0872 - val_loss: 0.0867\n",
      "Epoch 45/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0869 - val_loss: 0.0866\n",
      "Epoch 46/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0869 - val_loss: 0.0863\n",
      "Epoch 47/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0869 - val_loss: 0.0864\n",
      "Epoch 48/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0867 - val_loss: 0.0862\n",
      "Epoch 49/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0866 - val_loss: 0.0862\n",
      "Epoch 50/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0867 - val_loss: 0.0860\n",
      "Epoch 51/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0865 - val_loss: 0.0860\n",
      "Epoch 52/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0863 - val_loss: 0.0867\n",
      "Epoch 53/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0863 - val_loss: 0.0857\n",
      "Epoch 54/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0861 - val_loss: 0.0857\n",
      "Epoch 55/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0860 - val_loss: 0.0857\n",
      "Epoch 56/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0860 - val_loss: 0.0857\n",
      "Epoch 57/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0858 - val_loss: 0.0856\n",
      "Epoch 58/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0857 - val_loss: 0.0853\n",
      "Epoch 59/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0857 - val_loss: 0.0854\n",
      "Epoch 60/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0856 - val_loss: 0.0854\n",
      "Epoch 61/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0855 - val_loss: 0.0854\n",
      "Epoch 62/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0856 - val_loss: 0.0854\n",
      "Epoch 63/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0854 - val_loss: 0.0854\n",
      "Epoch 64/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0855 - val_loss: 0.0852\n",
      "Epoch 65/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0852 - val_loss: 0.0852\n",
      "Epoch 66/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0853 - val_loss: 0.0851\n",
      "Epoch 67/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0853 - val_loss: 0.0849\n",
      "Epoch 68/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0851 - val_loss: 0.0848\n",
      "Epoch 69/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0852 - val_loss: 0.0848\n",
      "Epoch 70/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0850 - val_loss: 0.0851\n",
      "Epoch 71/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0850 - val_loss: 0.0847\n",
      "Epoch 72/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0850 - val_loss: 0.0851\n",
      "Epoch 73/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0849 - val_loss: 0.0848\n",
      "Epoch 74/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0849 - val_loss: 0.0845\n",
      "Epoch 75/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0847 - val_loss: 0.0847\n",
      "Epoch 76/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0847 - val_loss: 0.0845\n",
      "Epoch 77/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0849 - val_loss: 0.0846\n",
      "Epoch 78/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0846 - val_loss: 0.0844\n",
      "Epoch 79/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0846 - val_loss: 0.0846\n",
      "Epoch 80/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0848 - val_loss: 0.0844\n",
      "Epoch 81/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0846 - val_loss: 0.0844\n",
      "Epoch 82/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0844 - val_loss: 0.0845\n",
      "Epoch 83/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0845 - val_loss: 0.0844\n",
      "Epoch 84/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0845 - val_loss: 0.0845\n",
      "Epoch 85/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0845 - val_loss: 0.0845\n",
      "Epoch 86/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0843 - val_loss: 0.0843\n",
      "Epoch 87/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0845 - val_loss: 0.0841\n",
      "Epoch 88/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0843 - val_loss: 0.0844\n",
      "Epoch 89/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0843 - val_loss: 0.0841\n",
      "Epoch 90/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0843 - val_loss: 0.0841\n",
      "Epoch 91/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0843\n",
      "Epoch 92/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0840\n",
      "Epoch 93/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0844 - val_loss: 0.0842\n",
      "Epoch 94/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0840 - val_loss: 0.0841\n",
      "Epoch 95/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0840 - val_loss: 0.0843\n",
      "Epoch 96/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0840 - val_loss: 0.0839\n",
      "Epoch 97/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0839 - val_loss: 0.0838\n",
      "Epoch 98/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0843\n",
      "Epoch 99/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0838\n",
      "Epoch 100/100\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0840 - val_loss: 0.0838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3099fde90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "UCGVt9VqOGAI",
    "outputId": "57ba3472-5c2d-430a-b047-fcfac55949d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the outputs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFPgMIlmPhJ8"
   },
   "source": [
    "- Comparing the input and output, we observe that $784$-dim were very well compressed into $32$-dim and then decoded into $784$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STVr9jscTiqi"
   },
   "source": [
    "***Question:*** While adding MLPs, we are not doing anything special we are just decreasing dimensions to 32 and increasing to original dimensions and using sigmoid for retrieving orig data. so in a NN, the data is automatically encoded to less dimensions?\n",
    "\n",
    "***Answer:*** Yes, that is correct. We are adjusting weight so that these two are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smY6MUWyQe2Z"
   },
   "source": [
    "***Question:*** In real world scenario how we would determine the shape?\n",
    "\n",
    "***Answer:***\n",
    "- On training data, we can try different values of $d'$ and plot the loss.\n",
    "- In the figure below, we observe that Loss drops significantly for $d'=4$.\n",
    "  <img src='https://drive.google.com/uc?id=19zdjO-3h3TYxr-t7b-5r2iyM91oV-mxT'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH-DuFJDQPet"
   },
   "source": [
    "***Question:*** Is reducing of file size an application of auto encoders?\n",
    "\n",
    "***Answer:***\n",
    "- Yes, it could an application provided you have decoder network for converting to original data.\n",
    "  <img src='https://drive.google.com/uc?id=19RbMIx3QX_-ugr07l_FZOV2E-uv8drS8'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1M82ubAQe-4"
   },
   "source": [
    "***Question:*** We already have t-SNE/UMAP for dimensionality reduction. Then what is advantage of using AE over these techniques?\n",
    "\n",
    "***Answer:***\n",
    "- t-SNE/UMAP preserves neighbourhood and it is used for vizualization.\n",
    "- AutoEncoders, recreate the original data and it uses compression for it.\n",
    "  <img src='https://drive.google.com/uc?id=1vbgrfdSUyQ3NAJNge-H48MC_eoaHdo7J'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cE-MKjZXdUx"
   },
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxy1hsiJXfdZ"
   },
   "source": [
    "We will cover following topics in the next lecture:\n",
    "1.   Denoising AE.\n",
    "2.   Misc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
