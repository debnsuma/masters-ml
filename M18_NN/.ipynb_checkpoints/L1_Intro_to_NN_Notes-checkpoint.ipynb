{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJZ-3vAUjCBO"
   },
   "source": [
    "**Content**\n",
    "\n",
    "1.   Overview of Deep Learning Module.\n",
    "2.   Intuition behind Neural Networks.\n",
    "3.   Programming Languages used in Deep Learning.\n",
    "4.   Biological Neuron Vs Artificial Neuron.\n",
    "5.   Logistic Regression as Neural Network.\n",
    "6.   Perceptron Model.\n",
    "7.   Training of Perceptron Model.\n",
    "8.   Multi-Layered Perceptron (MLP).\n",
    "9.   Training of MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz1d5sBqO8mZ"
   },
   "source": [
    "#### Deep-Learning Module Breakup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VTEdUWtA7Hu"
   },
   "source": [
    "![](imgs/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWx_m2ClSdtj"
   },
   "source": [
    "#### Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfLvqD3BBdEc"
   },
   "source": [
    "Objective of Deep Learning Module is:\n",
    "- Equip learners with strong foundations so as to understand and work with any new model.\n",
    "- Cover most widely used models.\n",
    "- Enable learners to read and understand research papers.\n",
    "  ![](imgs/img2.png)\n",
    "\n",
    "\n",
    "***Assumption:***\n",
    "\n",
    "Learners are comfortable with Classical Machine Learing (ML) and Maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv6GaOTBSZ8F"
   },
   "source": [
    "#### Why Deep Learning?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjppEe8OBIKn"
   },
   "source": [
    "- In practice, when we have tabular data the goto strategy is to use GBDT/XgBoost/RF. (This is not a hard & fast rule. Sometimes even Logistic Regression performs better.\n",
    "- But for Image/Speech/Complex Time series/Textual data, Deep Learning is used where Neural-Networks is the foundation.\n",
    "- We will study Images in Computer Vision (CV) module and Textual data in Natural Language processing (NLP) module.\n",
    "- We will also study Neural Collaborative Filtering (NCF) that is used in State of the Art (SOTA) Recommender Systems (RecSys).\n",
    "  ![](imgs/img3.png)\n",
    "\n",
    "*Note:*\n",
    "- If we have Image/Speech/Textual Data, Deep Learning (DL) is the de-facto approach used nowadays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIdRS-F7TKIq"
   },
   "source": [
    "#### Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaQWkmrxBqlj"
   },
   "source": [
    "- Simple model in Neural-Networks (N-N) is called as Perceptron model built in 1957 by Rosenblatt.\n",
    "- Whole area of N-N is loosely inspired from Human/Mammalian brain. But it cannot simulate understanding of a modern Human brain.\n",
    "  \n",
    "  ![](imgs/img4.png)\n",
    "- Next significant progress happened in 1980 when a paper was released on Back-propogation by Geoff Hinton. He also wrote paper about foundations of DL. He wrote on how to build modern DL algorithms.\n",
    "- This idea only gained traction in 2012, when a seminal paper was released (AlexNet). This is will covered in CV module.\n",
    "\n",
    "***Trivia:***\n",
    "\n",
    "Andrew Ng and his team used graphic processors to speed-up computations in DL around 2007-08.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xGWXXDIVRgO"
   },
   "source": [
    "#### Programming Language used in DL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfGiWPICB25d"
   },
   "source": [
    "- 2 major languages used are:\n",
    "  - TensorFlow (TF) and Keras developed by Google.\n",
    "  - PyTorch developed by Facebook.\n",
    "![](imgs/img5.png)\n",
    "\n",
    "*Note:*\n",
    "- TensorFlow will be used in our Classes.\n",
    "- Conceptually if you understand one, the other is quite simple to work with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plLMePmtWOwk"
   },
   "source": [
    "#### Inspiration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldW_Dh7mCDGI"
   },
   "source": [
    "- N-N as loosely inspired from humans/mammalian brain. This is not an exact connection but an approximate one.\n",
    "- A **Neuron** is basically a brain cell. It has a lot of connections with other neurons. The incoming connection is known as dendrites. They send electro-chemical signals to the neurons, which process this information and send information to other neurons through **Axons**.\n",
    "- This is loosely how brain cells work.\n",
    "  ![](imgs/img6.png)\n",
    "\n",
    "*Note:*\n",
    "\n",
    "We will not be studying this Biological Neuron, but Artificial Neurons which are mathematical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbhMS_AkbuJf"
   },
   "source": [
    "***An example from Biological Domain:***\n",
    "\n",
    "- Imagine we touched a hot plate. Electro-chemical signals go from fingers through spinal cord to the brain. Moment that happens, brain sends a signal to move the muscles.\n",
    "- Over a period of time, brain learns from good and bad experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwD40OhKXmrv"
   },
   "source": [
    "#### Artificial Neuron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXCFNaBjCSdE"
   },
   "source": [
    "- Imagine we have inputs $x_1,x_2, x_3$ coming into a neuron and every edge has weights $w_1,w_2,w_3$ associated with it.\n",
    "  ![](imgs/img7.png)\n",
    "- Each input to the neuron is multiplied by the corresponding weight of the edge. Note, that this weight tells how important any input is.\n",
    "- This function $(f)$ is often referred to as an $Activation \\ Function$. This function takes input $(w_1x_1 +w_2x_2 +w_3x_3)$ and generates output $O_1$\n",
    "- In some cases, it generates more outputs. This output can be sent to lot of other neurons.\n",
    "- If the function was a Sigmoid function, then this would be similar to Logistic Regression. Hence, the bias term $b$.\n",
    "\n",
    "*Note:*\n",
    "\n",
    "$O_1$ is the output of $Neuron_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ0tEsOjZzKt"
   },
   "source": [
    "#### Terminology Alert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ8ER7h9CfPM"
   },
   "source": [
    "- $O_i$ is the output which is a function of: $$\\sum_{j=1}^d x_{ij}, w_j$$ where $x_i$ is a d-dimensional datapoint $x_i\\in \\mathbb{R}^{d}$).\n",
    "- $f$ is called the activation function and $w_j \\to$ weights on edges.\n",
    "- For each $x_i$, there is an output $O_i$.\n",
    "  ![](imgs/img8.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fuSBcBtctV-"
   },
   "source": [
    "#### Artificial Neural Network: Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q2-PdwjCqQ7"
   },
   "source": [
    "- We will first study ANN: Perceptron, then Logistic Regression from N-N perspective, followed by Multi-Layered Perceptron (MLP) and code.\n",
    "  ![](imgs/img9.png)\n",
    "- Logistic Regression and Perceptron are very related concepts.\n",
    "- Fundamental working of Logistic Regression is as follows.\n",
    "- Given $x_i's$ which are d-dimensional $(x_i \\in \\mathbb{R}^{d})$ and $y_i$, we predict $\\hat{y_i}$ which is a Sigmoid function of $(w^Tx_i+b)$. Here $w$ is d-dimensional vector and $b$ is scalar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmf4o4XbC-Ma"
   },
   "source": [
    "- Let's see how can we represent this concept as N-N.\n",
    "- $x_{i1}$ is an input to the activation function. Along with $x_{i1}$ (first feature of $x_i$), we have other features like $x_{i2}, x_{i3}...x_{id}$ as inputs.\n",
    "  ![](imgs/img10.png)\n",
    "- Each of these inputs is associated with weights $w_1, w_2, w_3...w_d$ and constant $1$ with $b$.\n",
    "- The activation function is a Sigmoid Function and we need to learn these weights.\n",
    "- Sigmoid function generates output $O_i$ which is $\\hat{y_i}$.\n",
    "- Parameters to train are $w_1, w_2, w_3,...w_d$ and $b$.\n",
    "- We can find these parameters using Stochastic Gradient Descent (SGD) or batch Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7hlZv4OirIG"
   },
   "source": [
    "#### N-N representation of Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKjuSdrcDRkB"
   },
   "source": [
    "- Let say we have inputs $x_{i1}, x_{i2},...x_{id}$ with weights $(w_1, w_2,...w_d)$, bias $b$ and an activation function Sigmoid. Let $O_i$ be with output.\n",
    "  ![](imgs/img11.png)\n",
    "- $\\forall \\ x_i's$ we can pass it to the activation function and get output. This is also called as **Forward Propogation**.\n",
    "- Sigmoid function is represented as $f_\\sigma (w^Tx+b)$.\n",
    "- In Logistic Regression, we have a loss function Log-loss: $Loss_\\mu(y_i,O_i)$ where $O_i \\to \\hat{y_i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm9lyr5LkruN"
   },
   "source": [
    "##### Dataset Representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4axyUjKxDhDU"
   },
   "source": [
    "  ![](imgs/img12.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNvoiSxmk0uD"
   },
   "source": [
    "##### Evaluation of Querypoint $x_q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPYStXhVDttt"
   },
   "source": [
    "- Let's assume that we found best weights using SGD. We have a querypoint $x_q$, a d-dimensional point such that $x_q \\in \\mathbb{R}^{d}$.\n",
    "  ![](imgs/img13.png)\n",
    "- All we have to do is, take each of the dimensions $(x_{q1}, x_{q2},..x_{qd})$ and pass it to function $f_\\sigma$. We generate output $O_q$ as $O_q = \\hat{y_q} = \\sigma({w^Tx_q+b})$\n",
    "- $\\hat{y_q}$ could also be thought of as $\\hat{y_q} = P(y_q=1 | x_q,w)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsbX2LzMnPi5"
   },
   "source": [
    "#### Diagramatic Representation of Perceptron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99itZ7nsD5Oh"
   },
   "source": [
    "- Let $x_i \\in \\mathbb{R}^{d}, y_i \\in \\{0,1\\}$.\n",
    "  ![](imgs/img14.png)\n",
    "- In Perceptron, the activation function is different, but rest of the structure remains the same.\n",
    "- $O_i = f_{perceptron}(x_i,w,b)$ where\n",
    "\n",
    "\\begin{equation}\n",
    "  f_{perceptron}(x_i,w,b)=\\begin{cases}\n",
    "    1, & \\text{if $w^Tx_i+b>0$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*Note:*\n",
    "\n",
    "- We have a single neuron in Perceptron similar to Logistic Regresson.\n",
    "- Only difference is Logistic Regression uses Sigmoid Activation and Perceptron uses another activation function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2sdIgJ8o82b"
   },
   "source": [
    "#### Various ways to represent Perceptron Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoI6YWsTMczC"
   },
   "source": [
    "We could represent Logistic Regression and Perceptron using either equations, geometrically or using N-N diagram.\n",
    "![](imgs/img40.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fjz3T3rMroD"
   },
   "source": [
    "- Following is the geometric representation of Perceptron Model.\n",
    "- Geometrically, Perceptron is a hyperplane $(\\Pi^d)$ based separator without any squashing of output.\n",
    "  ![](imgs/img15.png)\n",
    "\n",
    "\n",
    "***Disadvantages:***\n",
    "- It has certain disadvantages as well.\n",
    "- Impact of outliers is massive as there is no squashing.\n",
    "- It's a Linear Model, hence we cannot get non-linear decision boundaries.\n",
    "- No probabilities.  For points close to decision boundary and far away will have same values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qvCFDp_q8wi"
   },
   "source": [
    "#### Training of Perceptron Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz-WrljzM8e3"
   },
   "source": [
    "- It is simple to train Perceptron model.\n",
    "  ![](imgs/img16.png)\n",
    "- We have $x_i$ corresponding to each $y_i$, where $y_i \\in \\{0,1\\}$. $\\hat{y_i}$ is computed as $f_p(w^Tx_i+b)$, where $\\hat{y_i} \\in \\{0,1\\}$.\n",
    "- We can come up with a loss function like Mean Squared Error (MSE) that will output only $0$ and $1$ (No probabilities).\n",
    "- Optimization problem would like:\n",
    "$$\\underset{w,b}{min} \\ \\sum_{i-1}^n Loss(y_i,\\hat{y_i}) + \\lambda L_2-reg \\ (w_j)$$\n",
    "- This optimization problem could be solved using Gradient Descent (GD).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqGHEZ1TNOT4"
   },
   "source": [
    "- Till now, we saw a simple Single Neuron model like:\n",
    "  - Logistic Regression with Log-loss and $L_2$ regularization.\n",
    "  - Perceptron (Not used much).\n",
    "- Similarly, a Linear SVM could also be thought of as a single neuron model.\n",
    "  ![](imgs/img17.png)\n",
    "- In SVM, we have hinge loss with some $L_2$ regularization.\n",
    "\n",
    "*Note:*\n",
    "\n",
    "These models have single neuron. What if we had such multiple models? How do we connect them?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unju5lXaumDE"
   },
   "source": [
    "#### Multi-Layered Perceptron (MLP):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_TJlRoPNbqK"
   },
   "source": [
    "- Imagine we have $(x_{i1}, x_{i2}, x_{i3}, x_{i4})$ representation of a single point $x_i$ such that $x_i \\in \\mathbb{R}^{4}$.\n",
    "- Suppose we have multiple activation functions $(f)$ in the 1st layer. Each of the inputs is connected to every activation function in $1^{st}$ Layer.\n",
    "  ![](imgs/img18.png)\n",
    "- Outputs of the $1^{st}$ layer are inputs to the activation functions of the $2^{nd}$ layer.\n",
    "- Theoretically, all $(f's)$ can be different. But in practice, we have same activation function.\n",
    "- Outputs of the $2^{nd}$layer are inputs to the activation functions of the final layer which generates output $O_i$.\n",
    "- This is how a Multi-Layered Perceptron Model (MLP) would look like.\n",
    "- A MLP has multiple neurons arranged in multiple layers.\n",
    "- $1^{st} \\ layer \\to (12 \\ weights + 3 \\ bias)$, $2^{nd} \\ layer \\to (6 \\ weights + 2 \\ bias)$ and $3^{rd} \\ layer \\to (2 \\ weights + 1 \\ bias)$.\n",
    "- All these weights are different. If we know the weights, we can do Forward Propogation (FP).\n",
    "- FP is nothing but going from input layer to output while doing a bunch of matrix multiplication at each layer.\n",
    "\n",
    "*Note:*\n",
    "\n",
    "We are ignoring $b$ for diagramatic simplicity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ReAiuYJNikS"
   },
   "source": [
    "#### Why do we need MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A6zCQpdNtzG"
   },
   "source": [
    "- Suppose we have simple functions in Algebra $ f_1, f_2,....f_7$ as shown below.\n",
    "  ![](imgs/img19.png)\n",
    "- $f_1$ could be written as $f_1(x_{i1}, x_{i2}) \\to x_{i1} + x_{i2}$.\n",
    "- Then, $f_6(f_1(x_{i1}, x_{i2})) \\to Sin(x_{i1} + x_{i2})$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk5G08bvSunt"
   },
   "source": [
    "#### Terminology Alert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GIw3CFJN_F-"
   },
   "source": [
    "- Composition of functions is $fog(x) = f(g(x))$ and $gof(x) = g(f(x))$.\n",
    "- So in MLP, we are doing composition of functions.\n",
    "- This is a great strategy to build complex functions using simple functions.\n",
    "  ![](imgs/img20.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWvuwMP6OO0J"
   },
   "source": [
    "- Note that $(w^Tx_i+b)$ is already linear function on top of which we compose another function.\n",
    "- If we keep on compositing functions, we can build extremely complex functions.\n",
    "  ![](imgs/img21.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzvwWF35mOwO"
   },
   "source": [
    "#### Misc. Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gX67A6QUksu"
   },
   "source": [
    "***Question:*** Till how long we will compose non-linear functions?\n",
    "\n",
    "***Anwser:***\n",
    "- Imagine there is 100-layered MLP, which means we are doing 100 function compositions. There is also a 2-layered MLP.\n",
    "- 100-layered will certainly be more complex.\n",
    "  ![](imgs/img22.png)\n",
    "\n",
    "*Note:* 100 layered MLP will tend to overfit more. So depth is actually a hyper parameter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It_aQd45Vbwd"
   },
   "source": [
    "***Question:*** What happens if each activation function in MLP is a linear function?\n",
    "\n",
    "***Anwser:***\n",
    "- No, these functions will always be non-linear in Deep Learning.\n",
    "  ![](imgs/img23.png)\n",
    "\n",
    "- Imagine we have a MLP as shown below:\n",
    "  ![](imgs/img24.png)\n",
    "- If we expand the output $O_i$, we get:\n",
    "$$O_i = \\alpha^{2}w_1 (w^Tx_i+b) + \\alpha^{2}w_2 (w'^Tx_i+b')$$\n",
    "- This is still a linear function. If all the activation functions are linear, then MLP will be a Linear Model.\n",
    "- Hence, we always use non-linear activation functions in Deep Learning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2V8YGIAV1kp"
   },
   "source": [
    "***Question:*** Should we use different activation functions in different layers?\n",
    "\n",
    "***Anwser:***\n",
    "- Theoretically we can. But studies have shown that it is not of much value.\n",
    "  ![](imgs/img25.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RisV6oQTlgrz"
   },
   "source": [
    "***Question:*** Is number of neurons in a layer also a hyper parameter?\n",
    "\n",
    "***Anwser:***\n",
    "- Yes, number or neurons is also a hyper parameter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdxWNQS5lpd5"
   },
   "source": [
    "***Question:*** Does it make sense to keep all activation functions in every layer the same ?\n",
    "\n",
    "***Anwser:***\n",
    "- Yes in practise, we go for that.\n",
    "\n",
    "***Trivia:***\n",
    "\n",
    "Sigmoid was very popular in $1980's \\ and \\ 90's$. Nowadays RELU is more popular. (We will study this in future classes).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGmiyzlsmUAB"
   },
   "source": [
    "#### Underlying Maths:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqdOLBT_V9Wc"
   },
   "source": [
    "- Imagine we have a Multi-Layered Perceptron. We have have 4-dimensional input $x_i$.\n",
    "  ![](imgs/img26.png)\n",
    "- Weights for each layer is represented as $w^k_{ij}$ where $i\\to$ node from previous layer, $j\\to$ node to next layer and $k\\to$ next layer.\n",
    "- Each activation function has output $O_{ij}$, where $i \\to$ current layer and $j \\to$ index of the nueron.\n",
    "- They are also known as fully connected MLP.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVzCi-5DWD-0"
   },
   "source": [
    "- Here is an example of how weights are labelled at each layer.\n",
    "- In Deep Learning, weights are matrices as shown below.\n",
    "  ![](imgs/img27.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_ybKUiOpOEj"
   },
   "source": [
    "#### How to train MLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4SKpsGzWL7E"
   },
   "source": [
    "- Let, we have a training dataset: $D_{Tr} = \\bigg\\{ (x_i,y_i)\\bigg\\}_{i=1}^{n}$ where $x_i \\in \\mathbb{R}^{d}$.\n",
    "  ![](imgs/img28.png)\n",
    "- $x_i$ is passed through MLP where we do Forward propogation and we get predictions $\\hat{y_i}$.\n",
    "- Forward propogation is basically bunch of matrix multiplications and applying activation function.\n",
    "- We need to define a loss function which takes ground truth and prediction.\n",
    "- The loss function could be MSE for Regression and Log-Loss for Classification.\n",
    "- During training, we determine these unkown weights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy1Y-pYwwaRK"
   },
   "source": [
    "##### Training Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h-VHExyWT-W"
   },
   "source": [
    "1.  Initialize all weights randomly. We want to minimize:\n",
    "$$\\sum_{i=1}^{n} \\ Loss_i$$\n",
    "\n",
    "    (Trick we could use here, is Calculus)\n",
    "    ![](imgs/img29.png)\n",
    "\n",
    "2.  $\\forall \\ x_i$ we will do FP and pass each $x_i$ to MLP and get output $O_i$ which in turn is used to determine $Loss_i$. We want to minimize $(w)$, summating over all losses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmzBTb7iWbBr"
   },
   "source": [
    "3.  $W_{ij}$ is updated as:\n",
    "$${W^k_{ij}}_{new} = {W^k_{ij}}_{old} - \\eta * \\frac{\\partial Loss}{\\partial W^k_{ij}}\\biggr\\vert_{{W^k_{ij}}_{old}}$$\n",
    "  ![](imgs/img30.png)\n",
    "4.  Repeat step 3 $\\forall i,j,k$ till convergence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI4WSXD71VYt"
   },
   "source": [
    "*Note:*\n",
    "\n",
    "- This is a Brute Force approach and highly sub optimal.\n",
    "- For this to work, all activation functions and loss function have to be differentiable.\n",
    "- There is a better strategy, Back Propogation (Will be discussed in next class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPEhBrPN5zuQ"
   },
   "source": [
    "#### Misc. Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD9cJMVG2l4Y"
   },
   "source": [
    "***Question:*** We keep referring to SGD when we mention about GD. In practice is it more common to use SGD or mini batch GD?\n",
    "\n",
    "***Answer:***\n",
    "- Imagine we have 1 million data points.\n",
    "  ![](imgs/img31.png)\n",
    "- In Gradient Descent, we compute $\\frac{\\partial Loss}{\\partial w}$ for all the n data points.\n",
    "- In Stochastic Gradient Descent, we compute $\\frac{\\partial Loss}{\\partial w}$ using one random point.\n",
    "  ![](imgs/img32.png)\n",
    "- In mini-batch $(k)$ Gradient Descent, we compute $\\frac{\\partial Loss}{\\partial w}$ for random batch of k points.\n",
    "- Mini-batch Gradient Descent is widely used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NCTrfEn207N"
   },
   "source": [
    "***Question:*** How is determining weights in Logistic Regression different than MLP while using Gradient Descent in both places?\n",
    "\n",
    "***Answer:***\n",
    "- Imagine we have a simple example as mentioned below.\n",
    "- We have input $x_i$ with 2 activation functions $f_1, f_2$ and we get the predictions $\\hat{y_i}$.\n",
    "  ![](imgs/img33.png)\n",
    "- Assume, we use a simple MSE as loss function. So $Loss_i = (y_i - \\hat{y_i})^2$.\n",
    "$$\\frac{\\partial Loss}{\\partial w_1} = \\sum_{i=1}^{n}\\frac{\\partial Loss_i}{\\partial w_1}$$\n",
    "  ![](imgs/img34.png)\n",
    "- On further ellaborating:\n",
    "$$\\frac{\\partial Loss}{\\partial w_1} = \\frac{\\partial }{\\partial w_1} \\ (y_i - f_2(f_1(x_i,w1)*w_2))^2$$\n",
    "- Note that $f_1 \\ and \\ f_2$ are differentiable and we solve this using chain rule.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjDPxTn23Jhf"
   },
   "source": [
    "***Question:*** Isn't DL similar to Stacking?\n",
    "\n",
    "***Answer:***\n",
    "- In stacking all models $M_1, M_2, M_3$ were built independently.\n",
    "- Outputs of these models is passed to $M_4$ which was built after.\n",
    "  ![](imgs/img35.png)\n",
    "- In DL, we are learning the weights of all activation functions together.\n",
    "  ![](imgs/img36.png)\n",
    "- So in DL, we have more flexibility to fine tune the weights simultaneously.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgCPn0Mh3YOI"
   },
   "source": [
    "***Question:*** Can you give a similar distinction b/w boosting and MLP. In boosting also, we build sequential trees which tries to work upon the mistakes made be previous trees.\n",
    "\n",
    "***Answer:***\n",
    "- In boosting, we have a base learner $h_i(x)$. Using errors made by this model, we build next model $h_{i+1}(x)$.\n",
    "- While building $h_{i+1}(x)$, $h_{i}(x)$ is fixed. Similarly while building $h_{i+2}(x)$, $h_{i+1}(x)$ is fixed.\n",
    "  ![](imgs/img37.png)\n",
    "- In DL, an activation function is connected to another activation. All of them are trained simultaneously.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEwtVKvq3e1e"
   },
   "source": [
    "***Question:*** Since we are using multiple layers, does our loss function remain convex i.e. 1 minima or is there a change here?\n",
    "\n",
    "***Answer:***\n",
    "- Since we have multiple composition functions, it turns out to be very complex and often non-convex.\n",
    "  ![](imgs/img38.png)\n",
    "- Hence, we will multiple local minimas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iHOnq803m7k"
   },
   "source": [
    "***Question:*** How would the TC look like for NN? in comparison to traditional ML algorithms? This looks super expensive.\n",
    "\n",
    "***Answer:***\n",
    "- DL are computationally expensive as there are lot of parameters to train.\n",
    "- A traditional Linear Regression model would take few secs. But DL models take minutes or even hours.\n",
    "  ![](imgs/img39.png)\n",
    "- It needs dedicated hardware (GPUs).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjJWU8yOCsni"
   },
   "source": [
    "***Question:*** Do we also get feature importance for NN?\n",
    "\n",
    "***Answer:***\n",
    "- It is very difficult to get feature importances easily.\n",
    "- There are complex technique like LIME which will be covered in CV module.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
