{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFOVfk-lGI6I"
   },
   "source": [
    "**Lecture link:**    https://www.scaler.com/meetings/i/dsml-advanced-decision-tree-2-2/archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg1jtltRWF1l"
   },
   "source": [
    "## Content\n",
    "\n",
    "\n",
    "1.   ROC and AUC (05:16 - 1:20:55)\n",
    "2.   PR-AUC (1:20:55 - 1:25:28)\n",
    "3. Decision Tree (1:29:50 - 2:28:56 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yGIVcucXlwB"
   },
   "source": [
    "<img src='https://drive.google.com/uc?id=1rKiJsjBvD7HzQAu-2FtVlfCNlyP2EJJ3'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNQd4Yn9WxgE"
   },
   "source": [
    "## ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CVd-HdXYISL"
   },
   "source": [
    "*   ROC is Receiver operating characteristic curve\n",
    "*   AUC is Area Under Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf6SfXYPYsVL"
   },
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCPhdzGzYwCJ"
   },
   "source": [
    "* When you are working on a binary classification model, one of the metric to measure the model's performance is by visualising the ROC curve and compute AUC of the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvf-ZbBOaq3k"
   },
   "source": [
    "### Steps involved :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TI1GV1Ga3of"
   },
   "source": [
    "Let us assume a data set with $x_i$ and $y_i$ data points and $ŷ_i$ be the predicted value of y in the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOZ1s6q_avG5"
   },
   "source": [
    "**Step 1:**\n",
    "* Sort the data in descending order of $ŷ$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaqRWxXlbe0A"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1goX0u5KRGYyyvYPCqkm3_AYeQOt0WVza'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYPYYDxtb2zZ"
   },
   "source": [
    "**Step 2:**\n",
    "* Set each $ŷ_i$ as threshold ($τ_i$) and classify as\n",
    " * If $ŷ_i > τ_i$ then classify as 1, else 0.\n",
    " * let these be $ỹ_{τ_i}$\n",
    "* continue doing the same for all $ŷ_i$ values.\n",
    "* Now calculate True Positive Rate$_i$ (TPR$_i$ ) and False Positive Rate$_i$  (FPR$_i$ ) for  $y$ and $ỹ_{τ_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deSB7z4mfcpg"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1npqw16RQ5CTAqsNheerRWdLMFtj6tCCo'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oanMSikNfm-t"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=12u21QmGmmzYbp5Vm_uxDG2FCNBeeHZ54'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzcVE9lagrAC"
   },
   "source": [
    "**Step 3:**\n",
    "* Plot the graph between all TPR$_i$ and FPR$_i$ as we have $n$ pairs of TPR and FPR's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UgCezO7jNQs"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1504V7rGGA5Ux5QJSuEtBWYzMRXKkJaYI'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Devj1yAejecZ"
   },
   "source": [
    "* This is how the graph looks for good model\n",
    "* TPR$_1$ is close to 0, as there is only 1 true positive because rest all values of $ỹ_{τ_1}$ are considered 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOlbarWpkHSL"
   },
   "source": [
    "* And now the AUC is the area under the curve.\n",
    "* Curve having more area represents good model.\n",
    "* The fundamental difference between other metrics and AU-ROC is that rest all the metrics like **precision, recall, F1-score** are calculated for **certain threshold** which is default=0.5, but **AU-ROC** gives the performance of model for **any set threshold.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zjtcbcol2m7f"
   },
   "source": [
    "* The AUC for a random model is **0.5** and the curve will be **diagnol.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1h9YSC5bUvUqJwKRrNa5IU7HuH0JCmn0j'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jPeXd3i7VeA"
   },
   "source": [
    "### Properties of AU-ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPQdg2FG-ar7"
   },
   "source": [
    "* It does not work well for the **highly imbalanced data set**\n",
    "* AU-ROC does not depend on the actual values of the $ŷ_i$ but only depends on the order of them.\n",
    "* Random Model has AU-ROC of 0.5\n",
    "* When model's AOC is less than 0.5, the simple fix for the model is to invert your predictions. i.e After inversing you will get area of 1-(actual area value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDwVeonnAPIW"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1m5yrFECmTDxtOUMIJFobuQKn-OrWGBwD'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_jigvVmAXIm"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1WttXDqrDQCQg3_MIfw56sj49npF_DH6R'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_FhbUDjR303"
   },
   "source": [
    "## Precision - Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-JnBgUaSa_Z"
   },
   "source": [
    "* Even though AU-ROC fails for highly imbalanced data set, Precision, Recall and F-1 Scores still are applicable for a set threshold.\n",
    "* Hence the precision-Recall curves are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqFzZUrqqon8"
   },
   "source": [
    "*  It is plotted by taking both precision and recall value for every threshold ($ŷ_i$) and plotting them.\n",
    "* The area under this curve is called AU-PRC, Area under precision recall curve, which is a very good metric for very highly imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nec2IkeEtzIb"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1jX5X6OYBZyTC_6WSkaugtgfZ4ve0EOQn'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwUtlJZKtMAT"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-K9goYytTb0"
   },
   "source": [
    "\n",
    "* Pure Node: If a node consists of points from only 1 class, it is called as **Pure node**\n",
    "* When there are points from 2 classes it is called as **Impure node**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z2FSSY9yrBI"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1dLxpMxVv_wRlUvPLhdycQafUBUctQ0-M'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cT4k-MryKlH"
   },
   "source": [
    " * Our main ***motive*** when creating a decision tree is to create decisions or conditions such that  we **have purer nodes as we keep splitting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXikssK4yhh2"
   },
   "source": [
    "**How to measure the purity of the node.?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xc3yBLCz_Ri"
   },
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT2y9O8i0Dc9"
   },
   "source": [
    "* Entropy helps us measure purity which helps us to pick the best decision\n",
    "\n",
    "Let $y$ be the set of discrete random variable and $p(y_i) $ be the probability of $y_i$\n",
    "* Entropy is given by\n",
    " * $H(y) = -∑_{i=1}^k p(y_i)*log(p(y_i))$\n",
    "\n",
    "If $y$ is the set of discrete random variable and $p(y_i) $ be the probability of $y_i$\n",
    "* Entropy is given by\n",
    " * $H(y) = -\\int_{-∞}^{∞} p(y_i)*log(p(y_i) dy)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O__l0Y5oJFQC"
   },
   "source": [
    "* If we calculate entropy for a set where $y=$ 0 or 1, by using the above formula wwe get **entropy is similar to negative of log-loss**\n",
    "* So, **minimising the log-loss is similar to maximising the entropy**, therefore logistic regression is also called as **maximum entropy model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thAaWvc-Jear"
   },
   "source": [
    "<img src='https://drive.google.com/uc?id=1Dn9P1WsgkTwIIzhg_Wraayroe_vIdBon'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkfVHxQyJ4fr"
   },
   "source": [
    "* Now, plotting the graph you can see that,\n",
    " * Entropy is maximum when $p(y=1)$ is 0.5\n",
    " * Entropy is minimum when $p(y=1)$ is 1\n",
    " * Entropy is minimum when $p(y=1)$ is 0\n",
    "* So, minimum entropy means we only observe one class label, so this can be used to measure purity\n",
    " * i.e Our purest mode has minimum entropy\n",
    "\n",
    "So, we split so that we get minimum entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z--Alii-MIcg"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=17_WvonEZu652rgC93RvcsPwQY0bMKT5c'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcmMTOsyMetv"
   },
   "source": [
    "<img src='https://drive.google.com/uc?id=1OXhDsKAd7zFSeiJCmjpBdV41QRUMtIOj'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD6YFT-TQNEY"
   },
   "source": [
    "Let's see for the case of Binary classification where log of base 2 is considered\n",
    "* Entropy is 1, when the split is 50% each\n",
    "* Entropy is 0, when the node is pure\n",
    "* Entropy is 0.0801 when the split is as 99% and 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55rjCHYkQ2Br"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1S8vsmSWU2TqVt39yz1hfg4ueqj0F8ATi'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-ahgwmDRlv9"
   },
   "source": [
    "* So, by using this concept of entropy, we spilt the nodes of decicion tree by using a feature in such a way that **entropy decreases after each split**.\n",
    "* So that we can attain **maximum purity**.\n",
    "* And we keep doing this, which is called **Recursive partition.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1taHNhs13UsglTO8FzAAySAuzsjLvSgN_'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9QWXQW6TcT3"
   },
   "source": [
    "### Play Tennis Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-ewepcAVUAT"
   },
   "source": [
    "Let us consider four features like Outlook, Temperature, Humidity and Windy.\n",
    "\n",
    "Basing on these, we decide whether people play tennis in these situations or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI5THEV9b7Xn"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1XIrYCftn07ScBTFmZ3_L-YHoZGpiFnic'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgDOT-vJbHQB"
   },
   "source": [
    "* Entropy at root node would be $-\\frac{9}{14}log\\frac{9}{14}- \\frac{5}{14}log\\frac{5}{14}$ from the data, which is 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdYHmClxcWfx"
   },
   "source": [
    "Now, let's split the data basing on Outlook data.\n",
    "and see the purity of the nodes and decide the features in which further split is made.\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1a4jxkDzrJbFqeW1wHdk7GWZOkhW_xXaU'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl7sqSgPqtQu"
   },
   "source": [
    "* Now, do the same for other features also to find their entropy\n",
    "* We have to **choose the feature with less entropy** to classsify or split the data points further\n",
    "* In this case it's outlook as it has least entropy and more information gain.\n",
    "* **Information gain** = entropy of parent node - weighted entropy of child nodes\n",
    "* Information should  be high for an ideal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbZExs0ss2R5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kok9M0ozrW4t"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1UB0OZg1Kl5hOsBBrUjJwobBOn_zn0HN2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tktiQ5cpsqPO"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=17GdFsLdbxvGWSy7Ag2ZI1ful57S0r0gR'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggj3nXbltPUw"
   },
   "source": [
    "* We have to do the **Recursive partition till the entropy becomes 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRKV86oUtZly"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1AHstiYdRj31MvRFPyUQlVomTMQ7pwzfj'>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
