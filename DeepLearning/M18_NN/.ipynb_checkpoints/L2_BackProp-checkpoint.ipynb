{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on September 05, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/34692/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "1.   Multi-Class Classification: (Softmax)\n",
        "2.   Cross Entropy (Generalization of log-loss)\n",
        "3.   Python code for Neural Network.\n",
        "4.   TF-2 and Keras code for Neural Network."
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How does simple function like ReLu create non-linearity?\n",
        "\n",
        "***Answer:***\n",
        "- Imagine we have a dataset and a Simple Neural Network setup.\n",
        "- The layer uses activation function $ReLu(Ƶ) = max(0,Ƶ), where \\ Ƶ \\to w^Tx+b$.\n",
        "- Here $w \\to weight \\ matrix $ and $b \\to bias$.\n",
        "- The ReLu output becomes and input to next neurons and there is a weight associated with it.\n",
        "- In the example below we piece wise linear function that represents application of ReLu. $1.5*max(0,2*x+1) + 1*max(0,x+2) + 1*max(0.x+5)$\n",
        "  <img src='https://drive.google.com/uc?id=18DzmTpKAOZYoLRThL-2AOpiCOhAd5UtK'>\n",
        "- So we get non linearity because of bunch of piece wise linear function.\n",
        "- In this example we are simulating 3 ReLu functions. If there were $100's \\ or \\ 1000's$ of such ReLu functions, then we can create an extremely complex decision surface."
      ],
      "metadata": {
        "id": "rKO3Z1WRthHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary-Class Setup:"
      ],
      "metadata": {
        "id": "XUKPDk91vxvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine we have Binary Classification where output is either 0 or 1 i.e. $y_i \\in \\{0,1 \\}$.\n",
        "- $Z_i$ is represented as $w^Tx_i+b$. This is passed to Sigmoid activation function.\n",
        "  <img src='https://drive.google.com/uc?id=1xdn_vzjntn-J_4qOMhE14r3NFEASTcKz'>\n",
        "- Sigmoid function is represented as $Sigmoid(Ƶ) = P(y_i=1 \\bigg| x_i,w,b)$ that gives probabilities $(P_i)$.\n",
        "$$Sigmoid(Ƶ) = \\frac{1}{1+e^{-Ƶ_i}} = \\frac{e^{Ƶ_i}}{1+e^{Ƶ_i}}$$\n",
        "- When we have binary classification, we use sigmoid and its shape is given by above equation.\n",
        "\n",
        "*Note:*  $P(y_i=0\\bigg|x_i,w,b) = 1-P_i$"
      ],
      "metadata": {
        "id": "01A28RCLlrtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Binary case, we also use log-loss.\n",
        "- Log-loss is also referred to as Binary Cross Entropy (CE).\n",
        "  <img src='https://drive.google.com/uc?id=1TmQ9JnJ1jEy2SD5jQF6D8ZlcYglFp0Lf'>\n",
        "- It looks like: $-\\bigg[  \\ y_i log(\\hat{y_i}) + (1-y_i) log(1-\\hat{y_i})\\bigg]$\n",
        "- In binary-case, Sigmoid converts $Ƶ_i$ into probabilities."
      ],
      "metadata": {
        "id": "ATbJW4v3lzUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Class Setup:"
      ],
      "metadata": {
        "id": "ZwthjXZtzJQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can extend the concept of log-loss in Binary Class to Multi-class setup as well.\n",
        "  <img src='https://drive.google.com/uc?id=1OT2zBpIfUc5zJekp3cOj-Gh7SjKwivX9'>\n",
        "- Imagine, we have inputs $x_i$ with 3 set of classes as output. We have a bunch of layers $(L_1,L_2,..)$ in between and 3 neuron in the output that computes $(Ƶ_1, Ƶ_2, Ƶ_3)$.  \n",
        "- We want to convert $(Ƶ_i)$ to probabilities such that:\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "  P_1 &= P(y_i=1 |x_i, \\theta ) \\\\\n",
        "  P_2 &= P(y_i=2 |x_i, \\theta ) \\\\\n",
        "  P_3 &= P(y_i=3 |x_i, \\theta )  \n",
        "   \\ where \\ \\theta \\to \\ all \\ parameters \\ of \\ Neural \\ Network.\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "*Note:*\n",
        "- $P_1, P_2, P_3$ have an important property. $0\\le P_i \\le 1 \\ and \\ \\sum_{i=1}^{3} P_i = 1$\n",
        "- Probabilities are mutually exclusive and exhaustive."
      ],
      "metadata": {
        "id": "NUuwUotll6fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax:"
      ],
      "metadata": {
        "id": "akRxh0pX2S_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Similar to Sigmoid, there is a Softmax funtion people have come-up with. It outputs:\n",
        "$$ P_1 = \\frac{e^{Ƶ_1}}{\\sum_{i=1}^{k} e^{Ƶ_i}}, \\ \\ P_2 = \\frac{e^{Ƶ_2}}{\\sum_{i=1}^{k} e^{Ƶ_i}}, \\ \\ P_1 = \\frac{e^{Ƶ_3}}{\\sum_{i=3}^{k} e^{Ƶ_i}}$$\n",
        "- if $k=3$, then $P_1+P_2+P_3 = 1$.\n",
        "  <img src='https://drive.google.com/uc?id=1d_p2ukqsJ8yST3J2PeKYLXI8ksBm7cU8'>\n",
        "- This is very popular for multi-class classification. Softmax can be thought of as Sigmoid like function for multi-class setup.\n",
        "- In Binary setup, we have Sigmoid and log-loss. Similarly, we have Softmax and CE in multi-class setup."
      ],
      "metadata": {
        "id": "zwMG8xYDmCm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why couldn't we simply use $Ƶ_i$ and normalize it using sum of $Ƶ_i$?\n",
        "\n",
        "***Answer:***\n",
        "- We could it. But using exponents has certain advantages.\n",
        "  <img src='https://drive.google.com/uc?id=1eddKrE6smrFzO__vgKytTITD_1u7f5Ra'>\n",
        "- If we took $P_i$ as $\\frac{Ƶ_i}{\\sumƵ_i}$ where $-\\infty \\le Ƶ_i \\le \\infty$. So $P_i$ can be -ve. In the above example if we had used $P_i = \\frac{Ƶ_i}{\\sumƵ_i}$, then the output would have been $1:3:6$.\n",
        "- When we use $P_i = \\frac{e^{Ƶ_i}}{\\sum e^{Ƶ_i}}$, then it would have been $4.5*10^{-5}, 0.00247, 0.997$. So whichever has maximum value, gets assigned highest probability because of exponents.\n"
      ],
      "metadata": {
        "id": "lRa1NbHDmg2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why we use only the exponential function & not any other?\n",
        "\n",
        "***Answer:***\n",
        "- Exponents are easily differentiable: $\\frac{\\partial e^Ƶ}{\\partial Ƶ} = e^Ƶ$.\n",
        "- That's why exponents are so common in DL.\n",
        "  <img src='https://drive.google.com/uc?id=1_GLrJz45llF6xdIf6TzNeHS68H77Irec'>"
      ],
      "metadata": {
        "id": "NT-U7Nqvmn1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Previously we all dealt with one neuron at end. Why can't we have a setup where single neuron predicts class which it belongs to?\n",
        "\n",
        "***Answer:***\n",
        "- Actually, we don't want it to return an integer.\n",
        "  <img src='https://drive.google.com/uc?id=1DQ5gdvKKo74zw-IndRbFPjeiJo36WzOS'>\n",
        "- Imagine, we have $x_i$ that belongs to class $3$ and $k=3$.\n",
        "- What we want is probability distribution of this point $x_i$ belonging to these classes.\n",
        "- If we get integer values, then designing loss function is difficult.\n",
        "- In the below example, model barely classifies this point as belonging to class $2$. But difference in probability of class $2$ and class $3$ is very small.\n",
        "- Having probabilistic class labels helps to make such decisions.\n",
        "  <img src='https://drive.google.com/uc?id=1-d6RitucylNBmeRAg710pySUp-Pd10w2'>"
      ],
      "metadata": {
        "id": "_IPqT0pXmzs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss function in Multi-class setup:"
      ],
      "metadata": {
        "id": "fl8Xwa--C8x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Similar to log-loss function in Binary classification, we can simply extend it to Multi-class setup.\n",
        "- This simple extension using concept of Cross Entropy.\n",
        "  <img src='https://drive.google.com/uc?id=1HohfhyUGaXbOIGqQ-msRts9iUW2VhV4r'>"
      ],
      "metadata": {
        "id": "Ynq65xEHnE3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Formula:"
      ],
      "metadata": {
        "id": "-69o9kISDZBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suppose we have point $x_i,y_i$, where $x_i$ could belong to either of $1,2,.j,..k$ class.\n",
        "- We have probabilities $P_{i1},P_{i2},.P_{ij},..P_{ik} \\forall_{j} = 1\\to k$\n",
        "  <img src='https://drive.google.com/uc?id=1zhPMIEQEESfhybzhzZc9aAZr-G5NBpBD'>\n",
        "- $y_i$ are represented a one-hot-encoding. So if $y_i \\to class \\ 2$ then, it is represented as $(0,1,0,0,..0)$ vector.\n",
        "- Cross Entropy is represented as:\n",
        "$$CE_i = - \\sum_{j=1}^{k} y_{ij} \\ log(P_{ij})$$\n"
      ],
      "metadata": {
        "id": "DmgY0_TUnN07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This looks like log-loss.\n",
        "- Imagine $k=2$ then $CE_i = - [y_{i1} log(P_{i1}) + y_{i2} log(P_{i2})]$.\n",
        "  <img src='https://drive.google.com/uc?id=1UGI2B2mu1C4RLVdP2gwueeTFbT_OuaSY'>\n",
        "- If $y_i \\in class \\ 1$, then $y_{i2} = 0$ and $y_{i1} = 1$. $P_{i2}$ can also be represented as $P_{i2} = (1-P_{i1})$.\n",
        "- This is nothing but log-loss. Hence, log-loss is often referred to as Binary Cross Entropy."
      ],
      "metadata": {
        "id": "jvCPIP3HnU_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Binary classification, Sigmoid is used and in K-ary Softmax is used to determine probabilities.\n",
        "  <img src='https://drive.google.com/uc?id=1e3ehdpcsHXX-3Yd9zXTLHmpM-Bgxlv9c'>\n",
        "- In Binary classification, Binary cross entropy/log-loss is used and in k-ary Cross entropy is used as loss function."
      ],
      "metadata": {
        "id": "xyuRQgTonjUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Could you provide some examples of multi-classification using three neurons?\n",
        "\n",
        "***Answer:***\n",
        "- Let $x_i$ has 2 features and we have simple Neural network for 3 class classifier.\n",
        "- $L_1$ has 100 units of ReLU. Output of these ReLu functions is passed to simple linear functions (for simplicity).\n",
        "- These linear functions produces output $Ƶ_{i1}, Ƶ_{i2}, Ƶ_{i3}$ and is passed to Softmax.\n",
        "- Softmax produces probabilities $P_{i1}, P_{i2}, P_{i3}$. Using these 3 proabilities, we compute loss using cross entropy function $L_{CE}(y^{OHE}_i, P_{ij})$.\n",
        "  <img src='https://drive.google.com/uc?id=1loxvoCN9WG7UIMlfn-A24HV5xfiK03SL'>\n",
        "- In this example, Softmax, Linear Activation function, ReLu and matrix multiplication $(w^Tx_i+b)$ are differentiable.\n",
        "- When we get a query point $x_q \\in \\mathbb{R}^2$, $x_q$ is passed through network and probabilities are generated.\n",
        "- If $P_{q3}$ is high, then we say that $q_p$ belongs to class 3."
      ],
      "metadata": {
        "id": "s7RJ2n0Gnqiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why don't we pass the softmax as the activation function itself instead of a ReLu function?\n",
        "\n",
        "***Answer:***\n",
        "- Softmax has multiple inputs and multiple outputs. But an activation function gets input as $w^Tx+b$ and its output is passed to further layers.\n",
        "- Activation functions take scaler as inputs and gives scaler as output whereas Softmax takes vector as input and gives vector as output.\n",
        "  <img src='https://drive.google.com/uc?id=1ywh1GIUf_gR5620_M4PL40m-_blz5CCd'>"
      ],
      "metadata": {
        "id": "KziHY9Q-JeGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Python Code: Simple Neural Network with Back propogation"
      ],
      "metadata": {
        "id": "UkyHYS1cIvHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1dLOPwh01o3k8p_hK633ixhD1ehz6nNWk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzUdJ0W3I6O0",
        "outputId": "31eefde6-08fc-4d95-ea46-3247df673b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dLOPwh01o3k8p_hK633ixhD1ehz6nNWk\n",
            "To: /content/spiral.csv\n",
            "\r  0% 0.00/12.9k [00:00<?, ?B/s]\r100% 12.9k/12.9k [00:00<00:00, 17.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9RLp9eGkcFT",
        "outputId": "e8efabfc-8319-4cb6-cf82-95b08c44310a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root  4096 Aug 31 13:47 sample_data\n",
            "-rw-r--r-- 1 root root 12867 Sep  6 16:11 spiral.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Very simple case of a back-prop\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"spiral.csv\")\n",
        "plt.scatter(df[\"x1\"], df[\"x2\"], c=df[\"y\"], s=40, cmap=plt.cm.Spectral)\n",
        "plt.show()\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "rRbplj4oI6Rv",
        "outputId": "64d48df3-fab9-41db-d0f1-ffa1e65e4eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hwG-P8l-I6U2",
        "outputId": "be496271-15b5-45b3-cbb2-c45e2cc5c53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         x1        x2  y\n",
              "0  0.000000  0.000000  0\n",
              "1 -0.000650  0.010080  0\n",
              "2  0.009809  0.017661  0\n",
              "3  0.007487  0.029364  0\n",
              "4 -0.000027  0.040404  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed7181bb-5b69-4f14-83a3-848577560366\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.000650</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.009809</td>\n",
              "      <td>0.017661</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007487</td>\n",
              "      <td>0.029364</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.000027</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed7181bb-5b69-4f14-83a3-848577560366')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed7181bb-5b69-4f14-83a3-848577560366 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed7181bb-5b69-4f14-83a3-848577560366');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "\n",
        "    def __init__(self, n_features, n_hidden, n_classes):\n",
        "        self.d = n_features\n",
        "        self.h = n_hidden\n",
        "        self.n = n_classes\n",
        "        self.W1 = 0.01 * np.random.randn(self.d, self.h)\n",
        "        self.b1 = np.zeros((1,self.h))\n",
        "        self.W2 = 0.01 * np.random.randn(self.h,self.n)\n",
        "        self.b2 = np.zeros((1,self.n))\n",
        "\n",
        "    def fwd_prop(self, X):\n",
        "        Z1 = np.dot(X, self.W1) + self.b1\n",
        "        A1 = np.maximum(0, Z1)\n",
        "        Z2 = np.dot(A1, self.W2) + self.b2\n",
        "        Z2 = np.exp(Z2)\n",
        "        A2 = Z2 / np.sum(Z2, axis=1, keepdims=True)\n",
        "        return A1, A2\n",
        "\n",
        "    def cce_loss(self, y, probs):\n",
        "        num_examples = y.shape[0]\n",
        "        correct_logprobs = -np.log(probs[range(num_examples),y])\n",
        "        loss = np.sum(correct_logprobs)/num_examples\n",
        "        return loss\n",
        "\n",
        "    def back_prop(self, X, A1, A2, y):\n",
        "        # compute the gradient on scores\n",
        "        num_examples = y.shape[0]\n",
        "        dZ2 = A2\n",
        "        dZ2[range(num_examples),y] -= 1\n",
        "        dZ2 /= num_examples\n",
        "        # first backprop into parameters W2 and b2\n",
        "        dW2 = np.dot(A1.T, dZ2)\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "        # next backprop into hidden layer, A1\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        # backprop the ReLU non-linearity\n",
        "        dA1[A1 <= 0] = 0\n",
        "        # finally into W,b\n",
        "        dZ1 = dA1\n",
        "        dW1 = np.dot(X.T, dZ1)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def fit(self, X, lr, reg, max_iters):\n",
        "        num_examples = X.shape[0]\n",
        "        for i in range(max_iters):\n",
        "            #foward prop\n",
        "            A1, A2 = self.fwd_prop(X)\n",
        "            # calculate loss\n",
        "            data_loss = self.cce_loss(y, A2)\n",
        "            reg_loss = 0.5*reg*np.sum(self.W1*self.W1) + 0.5*reg*np.sum(self.W2*self.W2)\n",
        "            loss = data_loss + reg_loss\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(\"iteration %d: loss %f\" % (i, loss))\n",
        "\n",
        "            dW1, db1, dW2, db2  = self.back_prop(X, A1, A2, y)\n",
        "\n",
        "            # add regularization gradient contribution\n",
        "            dW2 += reg * self.W2\n",
        "            dW1 += reg * self.W1\n",
        "\n",
        "            # perform a parameter update\n",
        "            self.W1 += -lr * dW1\n",
        "            self.b1 += -lr * db1\n",
        "            self.W2 += -lr * dW2\n",
        "            self.b2 += -lr * db2\n",
        "\n",
        "    def predict(self, X):\n",
        "        A1 = np.maximum(0, np.dot(X, self.W1) + self.b1)\n",
        "        Z2 = np.dot(A1, self.W2) + self.b2\n",
        "        y_hat = np.argmax(Z2, axis=1)\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "VjWATD_TJByY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = NN(n_features=2, n_hidden=100, n_classes=3)\n",
        "nn_model.fit(X, lr=1, reg=1e-3, max_iters=10000)\n",
        "print('training accuracy: %.2f' % (np.mean(nn_model.predict(X) == y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ossu32QkJB1T",
        "outputId": "4192a919-e448-4a34-b3e4-f92ceb4ec46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0: loss 1.098621\n",
            "iteration 1000: loss 0.366071\n",
            "iteration 2000: loss 0.515557\n",
            "iteration 3000: loss 0.262882\n",
            "iteration 4000: loss 0.263056\n",
            "iteration 5000: loss 0.279968\n",
            "iteration 6000: loss 0.258492\n",
            "iteration 7000: loss 0.259911\n",
            "iteration 8000: loss 0.264455\n",
            "iteration 9000: loss 0.258125\n",
            "training accuracy: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a 2D grid\n",
        "step = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
        "\n",
        "# predict for all the points in the grid\n",
        "y_hat = nn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "y_hat = y_hat.reshape(xx.shape)\n",
        "\n",
        "# plot\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, y_hat, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Nwp4496CJB4k",
        "outputId": "172803cb-9fb5-4da4-8dea-a9cb8e2564ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Neural Network Architecture:"
      ],
      "metadata": {
        "id": "IGuG-zDbNSTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this example we have input with 2 features $(d=2)$ which is passed to hidden layers with 100 units $(h=100)$.\n",
        "- Output from these hidden layers is connected to softmax layers with 3 outputs $(n=3)$.\n",
        "- Weight matrix $W_1$ has dimension $d*h$, bias $b_1$ has dimension $1*h$.\n",
        "- Weight matrix $W_2$ has dimension $h*n$, bias $b_2$ has dimension $1*n$.\n",
        "  <img src='https://drive.google.com/uc?id=1GNq7xFbRHiT5uJnxHCYcK9Mox8W_pqly'>"
      ],
      "metadata": {
        "id": "lxEJV2a0K6B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Forward Propogation:"
      ],
      "metadata": {
        "id": "KayG_DDONY3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $Z1$ is computed as $Z1 = X^TW_1+b_1$.\n",
        "- $A1$ represented ReLu function.\n",
        "- $Z2$ is computed as $Z2 = A1^TW_2+b_2$. We further take exponent of $Z2$ and compute Softmax.\n",
        "- Output of this Softmax is $A_2$.\n",
        "  <img src='https://drive.google.com/uc?id=1gkPxDh1J32xbVF-gPERSOVxAMO1nHi2F'>\n"
      ],
      "metadata": {
        "id": "-4Jqw2wEK6Ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loss Function:"
      ],
      "metadata": {
        "id": "3jPSq6IHNeUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cross Entropy is computed as:\n",
        "$$CE = \\frac{1}{m} \\sum_{i=1}^m CE_i  \\ \\ where \\ CE_i = - \\sum_{j=1}^{k} y_{ij} \\ log(P_{ij})$$\n",
        "- If $y_i \\in$ Class 2, then $y_{i2} = 1$ and $y_{ij\\neq2} = 0$.\n",
        "  <img src='https://drive.google.com/uc?id=1C8Rr6qbLHrBbQvrFvbOUtNVL2JDvSMc9'>\n"
      ],
      "metadata": {
        "id": "firjHl-INeUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Summary of Code so far:"
      ],
      "metadata": {
        "id": "_-uJFzxzPfVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $X$ is the input using which a weighted sum $Z_1$ is computed using $w_1,b_1$.\n",
        "- $Z_1$ is passed through ReLu unit to compute output $A_1$.\n",
        "- Using $A_1$ and $w_1,b_1$ a weighted sum is computed $Z_2$.\n",
        "- Softmax is applied to $Z_2$ and $A_2$ is computed which is passed to Cross Entropy.\n",
        "  <img src='https://drive.google.com/uc?id=15x2A4UxJEYDfe1p3Cd41W4bV5UHQhyHv'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "anQ2Cy5MPfV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Back Propogation:"
      ],
      "metadata": {
        "id": "CIUSE2A1QpUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Softmax layer generates probabilities $P_1, P_2, P_3$.\n",
        "- These probabilites along with one-hot encoding outputs $Y^1_{OHE},Y^2_{OHE},Y^3_{OHE}$ are passed to loss function $L_{CE}$.\n",
        "  <img src='https://drive.google.com/uc?id=1cVFRhFbfrQZFhkp4MXXxRRsUlfVUufql'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NiAu6RRRQpUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Softmax layer generates probabilities $P_1, P_2, P_3$.\n",
        "- These probabilites along with one-hot encoding outputs $Y^1_{OHE},Y^2_{OHE},Y^3_{OHE}$ are passed to loss function $L_{CE}$.\n",
        "- While computing derivative of loss function $L_{CE}$, we need to calculate partial derivatives for $\\frac{\\partial L_{CE}}{\\partial Ƶ_1}, \\frac{\\partial L_{CE}}{\\partial Ƶ_2} \\ and \\ \\frac{\\partial L_{CE}}{\\partial Ƶ_3}$.\n",
        "  <img src='https://drive.google.com/uc?id=1cVFRhFbfrQZFhkp4MXXxRRsUlfVUufql'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g-915BQWRsb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Derivative of cross entropy loss function wrt $Ƶ_j$ is represented as:\n",
        "$$\\frac{\\partial L_{CE}}{\\partial Ƶ_{j}} = P_j - Y^{OHE}_j$$\n",
        "where $P_j$ is probability that $x_i \\in class \\ j$.\n",
        "  <img src='https://drive.google.com/uc?id=1VeTTQuBzUGYh5-zSoESX2dSeK9gF_6Ea'>\n",
        "\n",
        "\n",
        "*Note:* There is a detailed explanation of above representation:\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
        "\n"
      ],
      "metadata": {
        "id": "L-Yrek78SuaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial Z_2} * \\frac{\\partial Z_2}{\\partial W_2}$\n",
        "- $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} * \\frac{\\partial Z_2}{\\partial A_1}$\n",
        "- $\\frac{\\partial Z_2}{\\partial W_2} = A_1$\n",
        "- $\\frac{\\partial L}{\\partial Z_2} = P_j - y_j$\n",
        "- $\\frac{\\partial Z_2}{\\partial A_1} = W_2$\n",
        "  <img src='https://drive.google.com/uc?id=1ZB5oYsrBsZZReVsSONiImdvWrhsZDYqr'>\n",
        "- Using the above partial derivatives, back propogation is computed.\n",
        "\n"
      ],
      "metadata": {
        "id": "Skt2lFD_RAB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fit Function:"
      ],
      "metadata": {
        "id": "axf3rrgBbP_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In fit function, we iterate over the input $max\\_iters$ times.\n",
        "- Using forward propogation, $A_1$ and $A_2$ are computed.\n",
        "- Croos entropy loss is computed which is regularized further using $L_2$ regularization technique.\n",
        "- After every 1000 iterations, loss is printed.\n",
        "- Partial derivatives are computed using back propogation on entire dataset.\n",
        "  <img src='https://drive.google.com/uc?id=1nIhVFbzcIvXdO8vJij6GzX7vI9utK7jQ'>\n",
        "  \n"
      ],
      "metadata": {
        "id": "1ClJjiTbbEll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using these partial derivatives, weights and biases are updated.\n",
        "  <img src='https://drive.google.com/uc?id=1R-zn3--bQ2BVm9FQZ59qkJin0TqfvYCj'>\n",
        "\n"
      ],
      "metadata": {
        "id": "f0hkTJ1ecD2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Predict Function:"
      ],
      "metadata": {
        "id": "danPBHafci6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Whenever we have a query point $x_q$, the output is computed as:\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "  A_1 &= \\max(0, W_1^Tx_q + B_1) \\\\\n",
        "  Z_2 &= W_2^TA_1 + B_2 \\\\  \n",
        "  \\hat{y} &= Argmax(Z_2)\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1RGV9bU-LCgdOUOI3yFB0VW9uaXvCYFpF'>\n"
      ],
      "metadata": {
        "id": "S8YpECRtchZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Output of predict function is training accuracy.\n",
        "- After first 1000 iterations, there is a significant drop in the loss.\n",
        "- But after 3000 iterations it plateaus.\n",
        "  <img src='https://drive.google.com/uc?id=1EAOLF7qe7MySFUhxH0zFG_2h2sxIJHRn'>\n"
      ],
      "metadata": {
        "id": "rMxU__fyeYD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Plot output graph:"
      ],
      "metadata": {
        "id": "TMkV7W3KfU6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For this simple dataset, we see clear separation boundaries.\n",
        "  <img src='https://drive.google.com/uc?id=1mR6DuW0BnAcy4lbN-I1xc97nffi9v3Vn'>\n"
      ],
      "metadata": {
        "id": "WV_KdcsLfOsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Coding Tracks for Deep Learning:"
      ],
      "metadata": {
        "id": "u3qV3rIzgH4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are 2 popular programming tracks for Deep Learning.\n",
        "- Tensorflow 2 that includes Keras and Py-Torch.\n",
        "  <img src='https://drive.google.com/uc?id=1YlqXLIyIOvDJP1wTUYKAuNlbOllRn9vu'>\n",
        "\n",
        "\n",
        "*Note:* During lectures, we will use Tensorflow 2."
      ],
      "metadata": {
        "id": "JP9d540-gLHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF 2 and Keras: Sequential API"
      ],
      "metadata": {
        "id": "DVC5qeKQi5Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcKRDcPJcBVz",
        "outputId": "8c4681ea-9199-499c-8cfa-ea54b52768c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras Sequential API\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(2,)))\n",
        "model.add(layers.Dense(100, activation=\"relu\"))\n",
        "model.add(layers.Dense(3, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "c6sF6m3OjAWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, we have input $x_i \\in \\mathbb{R}^2$.\n",
        "- In keras, we build a sequential fully connected dense network.\n",
        "- In first dense layer, we have 100 units with ReLu as activation function.\n",
        "- Second layer has 3 units an 'Softmax' function.\n",
        "  <img src='https://drive.google.com/uc?id=1byzRbm-qJgHPoB07me6rPwOQlRpBZrMU'>"
      ],
      "metadata": {
        "id": "NGN-CyqxlLQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gVfU34_jAZ_",
        "outputId": "3bb6b94f-9cea-4aad-dac7-b647fa909ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 100)               300       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 603\n",
            "Trainable params: 603\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First layer has 300 parameters ($2*100$ weights and $100$ biases).\n",
        "- Second layer has 303 parameters ($300$ weights and $3$ biases).\n",
        "  <img src='https://drive.google.com/uc?id=18VdYWt0icHclgwtQcXDqfms4gO5Vc3Az'>\n"
      ],
      "metadata": {
        "id": "OVnxyLDLl9F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(), # suggest in colab : Option+Esc in Mac\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])"
      ],
      "metadata": {
        "id": "Fdx0vuzijAc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note:*\n",
        "- We will study Adam Optimizer in future lectures.\n",
        "- BinaryAccuracy just computes how many classification were correct (True) and how many were incorrect (False).\n"
      ],
      "metadata": {
        "id": "RLjwh84fmgH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8qykpF_jFyz",
        "outputId": "d140a2b4-bee1-4ea9-8132-e9f8c7595163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 2)\n",
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBl4bvk0jG2X",
        "outputId": "75be7a73-d00e-41cf-94db-ec967092da8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to One-hot encode $y_i$"
      ],
      "metadata": {
        "id": "uoM7hSTzmmsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_OHE = to_categorical(y)\n",
        "print(y_OHE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfJjBDiNjJUv",
        "outputId": "511f685a-13e5-4937-be5e-f584480da475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we have 300 datapoints, then in Gradient Descent, we pass all these points to compute loss.  \n",
        "  <img src='https://drive.google.com/uc?id=1a3u0i15sF1jZz3-Goez_pmiuNrUIV9Hc'>\n",
        "- So partial derivative of Loss wrt $w_j$ is computed on all points.\n",
        "  <img src='https://drive.google.com/uc?id=1t-6oisCTieSn7zkjTP1AVLlaZHBEH-7q'>\n",
        "- In mini-batch GD, we create batch-size = 32 i.e. out of 300 datapoints 32 points are sampled randomly.\n",
        "- But in this approach there is a problem of missing out on some of the points not being sampled at all."
      ],
      "metadata": {
        "id": "3JJFJPv0nU7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To overcome this, dataset is broken into batch-size of 32 and each set is passed iteratively to the model.\n",
        "- This is called as Epoch. In a single Epoch, the model sees the entire dataset.\n",
        "  <img src='https://drive.google.com/uc?id=1PdCTcQgmm0cQk7l9Ndgzj4O4th6Gvr3n'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F8yELQovofaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "history = model.fit(X, y_OHE, epochs=10, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WCwNTMCjLrB",
        "outputId": "d9edc2db-f80d-4c50-dd9d-536b155b1ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.0989 - binary_accuracy: 0.6667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.0904 - binary_accuracy: 0.6667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0821 - binary_accuracy: 0.6667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0741 - binary_accuracy: 0.6667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0662 - binary_accuracy: 0.6667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0585 - binary_accuracy: 0.6667\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.0510 - binary_accuracy: 0.6667\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0436 - binary_accuracy: 0.6667\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.0362 - binary_accuracy: 0.6667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.0290 - binary_accuracy: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "history = model.fit(X, y_OHE, epochs=100, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvC6MtwxjOZB",
        "outputId": "e26f1035-d152-4c14-ef05-7c4a1583afec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.0217 - binary_accuracy: 0.6667\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.0146 - binary_accuracy: 0.6667\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1.0076 - binary_accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.0004 - binary_accuracy: 0.6667\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9935 - binary_accuracy: 0.6667\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.9870 - binary_accuracy: 0.6667\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.9802 - binary_accuracy: 0.6678\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9737 - binary_accuracy: 0.6678\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9674 - binary_accuracy: 0.6667\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.9611 - binary_accuracy: 0.6678\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.9550 - binary_accuracy: 0.6711\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.9489 - binary_accuracy: 0.6711\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.9430 - binary_accuracy: 0.6756\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9370 - binary_accuracy: 0.6822\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.9311 - binary_accuracy: 0.6867\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.9253 - binary_accuracy: 0.6967\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.9196 - binary_accuracy: 0.7056\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.9138 - binary_accuracy: 0.7144\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.9082 - binary_accuracy: 0.7211\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.9025 - binary_accuracy: 0.7256\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8971 - binary_accuracy: 0.7333\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8916 - binary_accuracy: 0.7389\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8863 - binary_accuracy: 0.7467\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8810 - binary_accuracy: 0.7578\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8758 - binary_accuracy: 0.7656\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8707 - binary_accuracy: 0.7689\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.8656 - binary_accuracy: 0.7744\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8607 - binary_accuracy: 0.7767\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.8557 - binary_accuracy: 0.7789\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8509 - binary_accuracy: 0.7867\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.8459 - binary_accuracy: 0.7878\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8413 - binary_accuracy: 0.7856\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8367 - binary_accuracy: 0.7889\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.8321 - binary_accuracy: 0.7922\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.8276 - binary_accuracy: 0.7944\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8231 - binary_accuracy: 0.7933\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.8187 - binary_accuracy: 0.7933\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8145 - binary_accuracy: 0.7956\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.8101 - binary_accuracy: 0.7978\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8060 - binary_accuracy: 0.8000\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8019 - binary_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7979 - binary_accuracy: 0.8000\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7939 - binary_accuracy: 0.7978\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7902 - binary_accuracy: 0.7967\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.7865 - binary_accuracy: 0.7978\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7829 - binary_accuracy: 0.7978\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7795 - binary_accuracy: 0.7967\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7762 - binary_accuracy: 0.7956\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7729 - binary_accuracy: 0.7944\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7698 - binary_accuracy: 0.7944\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.7667 - binary_accuracy: 0.7944\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7637 - binary_accuracy: 0.7956\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7607 - binary_accuracy: 0.7944\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7579 - binary_accuracy: 0.7944\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7551 - binary_accuracy: 0.7933\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7523 - binary_accuracy: 0.7956\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7497 - binary_accuracy: 0.7933\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7470 - binary_accuracy: 0.7933\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7444 - binary_accuracy: 0.7922\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.7419 - binary_accuracy: 0.7911\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7393 - binary_accuracy: 0.7900\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7367 - binary_accuracy: 0.7922\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7342 - binary_accuracy: 0.7900\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7318 - binary_accuracy: 0.7911\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7293 - binary_accuracy: 0.7911\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.7270 - binary_accuracy: 0.7900\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7248 - binary_accuracy: 0.7889\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7226 - binary_accuracy: 0.7889\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7205 - binary_accuracy: 0.7878\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7184 - binary_accuracy: 0.7889\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7163 - binary_accuracy: 0.7900\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7144 - binary_accuracy: 0.7889\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7124 - binary_accuracy: 0.7878\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7104 - binary_accuracy: 0.7867\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7084 - binary_accuracy: 0.7878\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7065 - binary_accuracy: 0.7878\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7045 - binary_accuracy: 0.7900\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7027 - binary_accuracy: 0.7889\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7009 - binary_accuracy: 0.7911\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6992 - binary_accuracy: 0.7900\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6975 - binary_accuracy: 0.7900\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6957 - binary_accuracy: 0.7878\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6941 - binary_accuracy: 0.7900\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6923 - binary_accuracy: 0.7900\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6907 - binary_accuracy: 0.7889\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6890 - binary_accuracy: 0.7900\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6874 - binary_accuracy: 0.7933\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6858 - binary_accuracy: 0.7933\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6842 - binary_accuracy: 0.7911\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6827 - binary_accuracy: 0.7922\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6812 - binary_accuracy: 0.7956\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6798 - binary_accuracy: 0.7978\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6784 - binary_accuracy: 0.7978\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6770 - binary_accuracy: 0.7989\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6756 - binary_accuracy: 0.8022\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6742 - binary_accuracy: 0.8022\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6726 - binary_accuracy: 0.8000\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6711 - binary_accuracy: 0.8011\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6696 - binary_accuracy: 0.8011\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6682 - binary_accuracy: 0.8022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "history = model.fit(X, y_OHE, epochs=1000, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b_9cI2UjQ6g",
        "outputId": "050f1268-b125-4995-82c2-520d74ac905a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6667 - binary_accuracy: 0.8033\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6654 - binary_accuracy: 0.8022\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6639 - binary_accuracy: 0.8022\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6625 - binary_accuracy: 0.8056\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6610 - binary_accuracy: 0.8056\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.6596 - binary_accuracy: 0.8089\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.6583 - binary_accuracy: 0.8078\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6570 - binary_accuracy: 0.8078\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6556 - binary_accuracy: 0.8089\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6543 - binary_accuracy: 0.8089\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6529 - binary_accuracy: 0.8089\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6517 - binary_accuracy: 0.8100\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6504 - binary_accuracy: 0.8100\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6493 - binary_accuracy: 0.8078\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6479 - binary_accuracy: 0.8067\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6466 - binary_accuracy: 0.8078\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6454 - binary_accuracy: 0.8111\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6441 - binary_accuracy: 0.8133\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6428 - binary_accuracy: 0.8133\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6415 - binary_accuracy: 0.8144\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6399 - binary_accuracy: 0.8167\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6385 - binary_accuracy: 0.8156\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.6370 - binary_accuracy: 0.8133\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6358 - binary_accuracy: 0.8100\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6345 - binary_accuracy: 0.8111\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6333 - binary_accuracy: 0.8122\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6320 - binary_accuracy: 0.8133\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6308 - binary_accuracy: 0.8133\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6295 - binary_accuracy: 0.8144\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6283 - binary_accuracy: 0.8144\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6270 - binary_accuracy: 0.8156\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6256 - binary_accuracy: 0.8156\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6243 - binary_accuracy: 0.8167\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6230 - binary_accuracy: 0.8189\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6215 - binary_accuracy: 0.8222\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6201 - binary_accuracy: 0.8222\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6188 - binary_accuracy: 0.8222\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6176 - binary_accuracy: 0.8200\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6163 - binary_accuracy: 0.8200\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6149 - binary_accuracy: 0.8211\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6136 - binary_accuracy: 0.8222\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6121 - binary_accuracy: 0.8233\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6108 - binary_accuracy: 0.8244\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.6093 - binary_accuracy: 0.8256\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6081 - binary_accuracy: 0.8278\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.6065 - binary_accuracy: 0.8289\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6053 - binary_accuracy: 0.8311\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6038 - binary_accuracy: 0.8322\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6026 - binary_accuracy: 0.8322\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.6012 - binary_accuracy: 0.8333\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.5998 - binary_accuracy: 0.8344\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5985 - binary_accuracy: 0.8356\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5972 - binary_accuracy: 0.8367\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.5960 - binary_accuracy: 0.8378\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5949 - binary_accuracy: 0.8367\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5937 - binary_accuracy: 0.8333\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5926 - binary_accuracy: 0.8333\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5914 - binary_accuracy: 0.8333\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5901 - binary_accuracy: 0.8344\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5888 - binary_accuracy: 0.8344\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5874 - binary_accuracy: 0.8356\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5861 - binary_accuracy: 0.8356\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5848 - binary_accuracy: 0.8356\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.5835 - binary_accuracy: 0.8367\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5821 - binary_accuracy: 0.8378\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5808 - binary_accuracy: 0.8378\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5795 - binary_accuracy: 0.8389\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5782 - binary_accuracy: 0.8389\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5769 - binary_accuracy: 0.8389\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5756 - binary_accuracy: 0.8389\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5743 - binary_accuracy: 0.8411\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5730 - binary_accuracy: 0.8411\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5717 - binary_accuracy: 0.8411\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5705 - binary_accuracy: 0.8411\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5691 - binary_accuracy: 0.8433\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5678 - binary_accuracy: 0.8444\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5665 - binary_accuracy: 0.8433\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5651 - binary_accuracy: 0.8444\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5637 - binary_accuracy: 0.8444\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5623 - binary_accuracy: 0.8467\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5608 - binary_accuracy: 0.8478\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5595 - binary_accuracy: 0.8500\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5580 - binary_accuracy: 0.8511\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5566 - binary_accuracy: 0.8522\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5553 - binary_accuracy: 0.8533\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5539 - binary_accuracy: 0.8533\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5526 - binary_accuracy: 0.8533\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5513 - binary_accuracy: 0.8533\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5499 - binary_accuracy: 0.8533\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5486 - binary_accuracy: 0.8544\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5473 - binary_accuracy: 0.8544\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5459 - binary_accuracy: 0.8544\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5446 - binary_accuracy: 0.8556\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5432 - binary_accuracy: 0.8556\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5418 - binary_accuracy: 0.8556\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5407 - binary_accuracy: 0.8567\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5395 - binary_accuracy: 0.8578\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5384 - binary_accuracy: 0.8578\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5371 - binary_accuracy: 0.8589\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5357 - binary_accuracy: 0.8578\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5343 - binary_accuracy: 0.8589\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5329 - binary_accuracy: 0.8589\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5315 - binary_accuracy: 0.8600\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5301 - binary_accuracy: 0.8633\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5288 - binary_accuracy: 0.8656\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5275 - binary_accuracy: 0.8656\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5263 - binary_accuracy: 0.8678\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5250 - binary_accuracy: 0.8678\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5237 - binary_accuracy: 0.8678\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5226 - binary_accuracy: 0.8667\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5212 - binary_accuracy: 0.8678\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5198 - binary_accuracy: 0.8678\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5184 - binary_accuracy: 0.8689\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5169 - binary_accuracy: 0.8678\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5156 - binary_accuracy: 0.8689\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5144 - binary_accuracy: 0.8667\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5132 - binary_accuracy: 0.8678\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5120 - binary_accuracy: 0.8667\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5108 - binary_accuracy: 0.8667\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5096 - binary_accuracy: 0.8678\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5083 - binary_accuracy: 0.8667\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5070 - binary_accuracy: 0.8678\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5057 - binary_accuracy: 0.8689\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5044 - binary_accuracy: 0.8689\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5031 - binary_accuracy: 0.8700\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5019 - binary_accuracy: 0.8700\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5005 - binary_accuracy: 0.8711\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4993 - binary_accuracy: 0.8711\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4980 - binary_accuracy: 0.8722\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.4969 - binary_accuracy: 0.8722\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4956 - binary_accuracy: 0.8733\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4944 - binary_accuracy: 0.8733\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4931 - binary_accuracy: 0.8733\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4919 - binary_accuracy: 0.8744\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4906 - binary_accuracy: 0.8733\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4892 - binary_accuracy: 0.8733\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4880 - binary_accuracy: 0.8733\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4869 - binary_accuracy: 0.8744\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4856 - binary_accuracy: 0.8767\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4846 - binary_accuracy: 0.8756\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4835 - binary_accuracy: 0.8756\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4825 - binary_accuracy: 0.8756\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4812 - binary_accuracy: 0.8767\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4800 - binary_accuracy: 0.8756\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4787 - binary_accuracy: 0.8756\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4773 - binary_accuracy: 0.8756\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4760 - binary_accuracy: 0.8756\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4747 - binary_accuracy: 0.8767\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4735 - binary_accuracy: 0.8822\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4721 - binary_accuracy: 0.8811\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4709 - binary_accuracy: 0.8811\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4697 - binary_accuracy: 0.8800\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4684 - binary_accuracy: 0.8778\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.4672 - binary_accuracy: 0.8800\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4660 - binary_accuracy: 0.8778\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4649 - binary_accuracy: 0.8778\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4637 - binary_accuracy: 0.8778\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4625 - binary_accuracy: 0.8778\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4614 - binary_accuracy: 0.8800\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4602 - binary_accuracy: 0.8822\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4590 - binary_accuracy: 0.8844\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4579 - binary_accuracy: 0.8856\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4568 - binary_accuracy: 0.8856\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4556 - binary_accuracy: 0.8856\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4543 - binary_accuracy: 0.8867\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4531 - binary_accuracy: 0.8878\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4518 - binary_accuracy: 0.8889\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4507 - binary_accuracy: 0.8900\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4495 - binary_accuracy: 0.8911\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4483 - binary_accuracy: 0.8911\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4473 - binary_accuracy: 0.8911\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4461 - binary_accuracy: 0.8922\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4450 - binary_accuracy: 0.8922\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4441 - binary_accuracy: 0.8944\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4429 - binary_accuracy: 0.8956\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4419 - binary_accuracy: 0.8956\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4407 - binary_accuracy: 0.8967\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4396 - binary_accuracy: 0.8989\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4384 - binary_accuracy: 0.8978\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4374 - binary_accuracy: 0.8978\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4363 - binary_accuracy: 0.8978\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4352 - binary_accuracy: 0.8989\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4341 - binary_accuracy: 0.9011\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4334 - binary_accuracy: 0.9000\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4323 - binary_accuracy: 0.9011\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4313 - binary_accuracy: 0.8989\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4303 - binary_accuracy: 0.8989\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4293 - binary_accuracy: 0.8989\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4282 - binary_accuracy: 0.8989\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4272 - binary_accuracy: 0.8989\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4262 - binary_accuracy: 0.8989\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4252 - binary_accuracy: 0.8989\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.4242 - binary_accuracy: 0.8989\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4231 - binary_accuracy: 0.9000\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4220 - binary_accuracy: 0.9011\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4209 - binary_accuracy: 0.9022\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4197 - binary_accuracy: 0.9033\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4186 - binary_accuracy: 0.9033\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4175 - binary_accuracy: 0.9033\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4165 - binary_accuracy: 0.9044\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4155 - binary_accuracy: 0.9033\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4144 - binary_accuracy: 0.9033\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4134 - binary_accuracy: 0.9044\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4124 - binary_accuracy: 0.9044\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4115 - binary_accuracy: 0.9033\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4104 - binary_accuracy: 0.9033\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4094 - binary_accuracy: 0.9033\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4085 - binary_accuracy: 0.9044\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4076 - binary_accuracy: 0.9056\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4066 - binary_accuracy: 0.9067\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4057 - binary_accuracy: 0.9078\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4046 - binary_accuracy: 0.9089\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4037 - binary_accuracy: 0.9089\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4027 - binary_accuracy: 0.9078\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4018 - binary_accuracy: 0.9089\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4009 - binary_accuracy: 0.9100\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3999 - binary_accuracy: 0.9100\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3989 - binary_accuracy: 0.9100\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3980 - binary_accuracy: 0.9111\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3971 - binary_accuracy: 0.9111\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3962 - binary_accuracy: 0.9133\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3953 - binary_accuracy: 0.9133\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3944 - binary_accuracy: 0.9111\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3937 - binary_accuracy: 0.9144\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3928 - binary_accuracy: 0.9133\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3919 - binary_accuracy: 0.9133\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.3912 - binary_accuracy: 0.9144\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3905 - binary_accuracy: 0.9144\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3897 - binary_accuracy: 0.9133\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3887 - binary_accuracy: 0.9133\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3877 - binary_accuracy: 0.9144\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3864 - binary_accuracy: 0.9156\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3853 - binary_accuracy: 0.9167\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3841 - binary_accuracy: 0.9156\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3832 - binary_accuracy: 0.9144\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3822 - binary_accuracy: 0.9133\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3815 - binary_accuracy: 0.9133\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3810 - binary_accuracy: 0.9144\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3800 - binary_accuracy: 0.9156\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3792 - binary_accuracy: 0.9156\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3784 - binary_accuracy: 0.9156\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3774 - binary_accuracy: 0.9167\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3764 - binary_accuracy: 0.9167\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3754 - binary_accuracy: 0.9167\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3745 - binary_accuracy: 0.9167\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3735 - binary_accuracy: 0.9156\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3729 - binary_accuracy: 0.9167\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3720 - binary_accuracy: 0.9167\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3712 - binary_accuracy: 0.9167\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3703 - binary_accuracy: 0.9167\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3695 - binary_accuracy: 0.9167\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3687 - binary_accuracy: 0.9167\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3679 - binary_accuracy: 0.9178\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3668 - binary_accuracy: 0.9178\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3659 - binary_accuracy: 0.9178\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3651 - binary_accuracy: 0.9189\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.3645 - binary_accuracy: 0.9178\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3636 - binary_accuracy: 0.9189\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3627 - binary_accuracy: 0.9211\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3618 - binary_accuracy: 0.9222\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3609 - binary_accuracy: 0.9222\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.3602 - binary_accuracy: 0.9211\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3594 - binary_accuracy: 0.9244\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3586 - binary_accuracy: 0.9244\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3577 - binary_accuracy: 0.9256\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3569 - binary_accuracy: 0.9256\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3561 - binary_accuracy: 0.9256\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3553 - binary_accuracy: 0.9256\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3544 - binary_accuracy: 0.9256\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3537 - binary_accuracy: 0.9256\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3530 - binary_accuracy: 0.9256\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3522 - binary_accuracy: 0.9256\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3514 - binary_accuracy: 0.9267\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3506 - binary_accuracy: 0.9267\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3498 - binary_accuracy: 0.9267\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3491 - binary_accuracy: 0.9278\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3483 - binary_accuracy: 0.9278\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3474 - binary_accuracy: 0.9278\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3467 - binary_accuracy: 0.9267\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3461 - binary_accuracy: 0.9267\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3454 - binary_accuracy: 0.9267\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3446 - binary_accuracy: 0.9267\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3437 - binary_accuracy: 0.9278\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3430 - binary_accuracy: 0.9289\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3421 - binary_accuracy: 0.9289\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3413 - binary_accuracy: 0.9289\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3405 - binary_accuracy: 0.9311\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3398 - binary_accuracy: 0.9322\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3390 - binary_accuracy: 0.9322\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3383 - binary_accuracy: 0.9322\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3375 - binary_accuracy: 0.9322\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3368 - binary_accuracy: 0.9322\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3362 - binary_accuracy: 0.9311\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3354 - binary_accuracy: 0.9322\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3347 - binary_accuracy: 0.9311\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3339 - binary_accuracy: 0.9311\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3331 - binary_accuracy: 0.9311\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3323 - binary_accuracy: 0.9322\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3316 - binary_accuracy: 0.9322\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3309 - binary_accuracy: 0.9322\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3301 - binary_accuracy: 0.9322\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3295 - binary_accuracy: 0.9322\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3287 - binary_accuracy: 0.9322\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3280 - binary_accuracy: 0.9333\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3276 - binary_accuracy: 0.9344\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3272 - binary_accuracy: 0.9344\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3271 - binary_accuracy: 0.9333\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3266 - binary_accuracy: 0.9333\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3260 - binary_accuracy: 0.9333\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3253 - binary_accuracy: 0.9333\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3245 - binary_accuracy: 0.9333\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3235 - binary_accuracy: 0.9333\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3225 - binary_accuracy: 0.9333\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3217 - binary_accuracy: 0.9322\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3211 - binary_accuracy: 0.9322\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3204 - binary_accuracy: 0.9311\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3197 - binary_accuracy: 0.9311\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3191 - binary_accuracy: 0.9311\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3183 - binary_accuracy: 0.9311\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3175 - binary_accuracy: 0.9322\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3167 - binary_accuracy: 0.9333\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.3159 - binary_accuracy: 0.9333\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3152 - binary_accuracy: 0.9333\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3145 - binary_accuracy: 0.9356\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3139 - binary_accuracy: 0.9367\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3133 - binary_accuracy: 0.9378\n",
            "Epoch 327/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3126 - binary_accuracy: 0.9378\n",
            "Epoch 328/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3120 - binary_accuracy: 0.9389\n",
            "Epoch 329/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.3115 - binary_accuracy: 0.9389\n",
            "Epoch 330/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3109 - binary_accuracy: 0.9389\n",
            "Epoch 331/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3102 - binary_accuracy: 0.9389\n",
            "Epoch 332/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3095 - binary_accuracy: 0.9389\n",
            "Epoch 333/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3089 - binary_accuracy: 0.9389\n",
            "Epoch 334/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3082 - binary_accuracy: 0.9389\n",
            "Epoch 335/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3076 - binary_accuracy: 0.9389\n",
            "Epoch 336/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3068 - binary_accuracy: 0.9389\n",
            "Epoch 337/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3062 - binary_accuracy: 0.9400\n",
            "Epoch 338/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3056 - binary_accuracy: 0.9400\n",
            "Epoch 339/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3054 - binary_accuracy: 0.9378\n",
            "Epoch 340/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3051 - binary_accuracy: 0.9400\n",
            "Epoch 341/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3048 - binary_accuracy: 0.9400\n",
            "Epoch 342/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3041 - binary_accuracy: 0.9400\n",
            "Epoch 343/1000\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.3033 - binary_accuracy: 0.9389\n",
            "Epoch 344/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3023 - binary_accuracy: 0.9389\n",
            "Epoch 345/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.3014 - binary_accuracy: 0.9389\n",
            "Epoch 346/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3006 - binary_accuracy: 0.9400\n",
            "Epoch 347/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2998 - binary_accuracy: 0.9400\n",
            "Epoch 348/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2992 - binary_accuracy: 0.9389\n",
            "Epoch 349/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2986 - binary_accuracy: 0.9411\n",
            "Epoch 350/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.2981 - binary_accuracy: 0.9400\n",
            "Epoch 351/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2975 - binary_accuracy: 0.9411\n",
            "Epoch 352/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2970 - binary_accuracy: 0.9411\n",
            "Epoch 353/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2965 - binary_accuracy: 0.9411\n",
            "Epoch 354/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2957 - binary_accuracy: 0.9411\n",
            "Epoch 355/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2951 - binary_accuracy: 0.9400\n",
            "Epoch 356/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2944 - binary_accuracy: 0.9411\n",
            "Epoch 357/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2936 - binary_accuracy: 0.9411\n",
            "Epoch 358/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2930 - binary_accuracy: 0.9411\n",
            "Epoch 359/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2925 - binary_accuracy: 0.9411\n",
            "Epoch 360/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2919 - binary_accuracy: 0.9422\n",
            "Epoch 361/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2915 - binary_accuracy: 0.9411\n",
            "Epoch 362/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2909 - binary_accuracy: 0.9411\n",
            "Epoch 363/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2902 - binary_accuracy: 0.9411\n",
            "Epoch 364/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2895 - binary_accuracy: 0.9422\n",
            "Epoch 365/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2887 - binary_accuracy: 0.9422\n",
            "Epoch 366/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2882 - binary_accuracy: 0.9422\n",
            "Epoch 367/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2875 - binary_accuracy: 0.9422\n",
            "Epoch 368/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2870 - binary_accuracy: 0.9422\n",
            "Epoch 369/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2864 - binary_accuracy: 0.9411\n",
            "Epoch 370/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2858 - binary_accuracy: 0.9411\n",
            "Epoch 371/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2854 - binary_accuracy: 0.9422\n",
            "Epoch 372/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2849 - binary_accuracy: 0.9422\n",
            "Epoch 373/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2844 - binary_accuracy: 0.9422\n",
            "Epoch 374/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2839 - binary_accuracy: 0.9422\n",
            "Epoch 375/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2833 - binary_accuracy: 0.9422\n",
            "Epoch 376/1000\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2827 - binary_accuracy: 0.9422\n",
            "Epoch 377/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2820 - binary_accuracy: 0.9422\n",
            "Epoch 378/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2815 - binary_accuracy: 0.9422\n",
            "Epoch 379/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2808 - binary_accuracy: 0.9433\n",
            "Epoch 380/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2802 - binary_accuracy: 0.9422\n",
            "Epoch 381/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2796 - binary_accuracy: 0.9433\n",
            "Epoch 382/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2790 - binary_accuracy: 0.9422\n",
            "Epoch 383/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2785 - binary_accuracy: 0.9422\n",
            "Epoch 384/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2779 - binary_accuracy: 0.9422\n",
            "Epoch 385/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2774 - binary_accuracy: 0.9433\n",
            "Epoch 386/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2769 - binary_accuracy: 0.9433\n",
            "Epoch 387/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2764 - binary_accuracy: 0.9433\n",
            "Epoch 388/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2759 - binary_accuracy: 0.9433\n",
            "Epoch 389/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2752 - binary_accuracy: 0.9456\n",
            "Epoch 390/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2747 - binary_accuracy: 0.9456\n",
            "Epoch 391/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2742 - binary_accuracy: 0.9456\n",
            "Epoch 392/1000\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.2736 - binary_accuracy: 0.9456\n",
            "Epoch 393/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2731 - binary_accuracy: 0.9456\n",
            "Epoch 394/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2726 - binary_accuracy: 0.9456\n",
            "Epoch 395/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2719 - binary_accuracy: 0.9456\n",
            "Epoch 396/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2713 - binary_accuracy: 0.9478\n",
            "Epoch 397/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2708 - binary_accuracy: 0.9478\n",
            "Epoch 398/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2704 - binary_accuracy: 0.9467\n",
            "Epoch 399/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2700 - binary_accuracy: 0.9444\n",
            "Epoch 400/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2698 - binary_accuracy: 0.9433\n",
            "Epoch 401/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2693 - binary_accuracy: 0.9444\n",
            "Epoch 402/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2687 - binary_accuracy: 0.9456\n",
            "Epoch 403/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2682 - binary_accuracy: 0.9456\n",
            "Epoch 404/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2675 - binary_accuracy: 0.9467\n",
            "Epoch 405/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2668 - binary_accuracy: 0.9456\n",
            "Epoch 406/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2664 - binary_accuracy: 0.9467\n",
            "Epoch 407/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2658 - binary_accuracy: 0.9478\n",
            "Epoch 408/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2654 - binary_accuracy: 0.9478\n",
            "Epoch 409/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2649 - binary_accuracy: 0.9478\n",
            "Epoch 410/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2644 - binary_accuracy: 0.9478\n",
            "Epoch 411/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2638 - binary_accuracy: 0.9478\n",
            "Epoch 412/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2632 - binary_accuracy: 0.9478\n",
            "Epoch 413/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2624 - binary_accuracy: 0.9478\n",
            "Epoch 414/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2620 - binary_accuracy: 0.9478\n",
            "Epoch 415/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2616 - binary_accuracy: 0.9489\n",
            "Epoch 416/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2611 - binary_accuracy: 0.9489\n",
            "Epoch 417/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2605 - binary_accuracy: 0.9500\n",
            "Epoch 418/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2600 - binary_accuracy: 0.9489\n",
            "Epoch 419/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2595 - binary_accuracy: 0.9489\n",
            "Epoch 420/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2590 - binary_accuracy: 0.9489\n",
            "Epoch 421/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2585 - binary_accuracy: 0.9489\n",
            "Epoch 422/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2579 - binary_accuracy: 0.9489\n",
            "Epoch 423/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.2574 - binary_accuracy: 0.9478\n",
            "Epoch 424/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2568 - binary_accuracy: 0.9478\n",
            "Epoch 425/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2563 - binary_accuracy: 0.9478\n",
            "Epoch 426/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2558 - binary_accuracy: 0.9478\n",
            "Epoch 427/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2553 - binary_accuracy: 0.9489\n",
            "Epoch 428/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2548 - binary_accuracy: 0.9500\n",
            "Epoch 429/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2543 - binary_accuracy: 0.9511\n",
            "Epoch 430/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2540 - binary_accuracy: 0.9511\n",
            "Epoch 431/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2535 - binary_accuracy: 0.9511\n",
            "Epoch 432/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2530 - binary_accuracy: 0.9511\n",
            "Epoch 433/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2527 - binary_accuracy: 0.9500\n",
            "Epoch 434/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2522 - binary_accuracy: 0.9500\n",
            "Epoch 435/1000\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.2518 - binary_accuracy: 0.9511\n",
            "Epoch 436/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2512 - binary_accuracy: 0.9511\n",
            "Epoch 437/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2507 - binary_accuracy: 0.9511\n",
            "Epoch 438/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2502 - binary_accuracy: 0.9511\n",
            "Epoch 439/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2498 - binary_accuracy: 0.9511\n",
            "Epoch 440/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2492 - binary_accuracy: 0.9511\n",
            "Epoch 441/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2487 - binary_accuracy: 0.9511\n",
            "Epoch 442/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2484 - binary_accuracy: 0.9511\n",
            "Epoch 443/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2479 - binary_accuracy: 0.9511\n",
            "Epoch 444/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2474 - binary_accuracy: 0.9511\n",
            "Epoch 445/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2470 - binary_accuracy: 0.9511\n",
            "Epoch 446/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2465 - binary_accuracy: 0.9489\n",
            "Epoch 447/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.2460 - binary_accuracy: 0.9478\n",
            "Epoch 448/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2456 - binary_accuracy: 0.9478\n",
            "Epoch 449/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2451 - binary_accuracy: 0.9478\n",
            "Epoch 450/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2445 - binary_accuracy: 0.9478\n",
            "Epoch 451/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2440 - binary_accuracy: 0.9478\n",
            "Epoch 452/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2434 - binary_accuracy: 0.9511\n",
            "Epoch 453/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2430 - binary_accuracy: 0.9511\n",
            "Epoch 454/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2425 - binary_accuracy: 0.9511\n",
            "Epoch 455/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2423 - binary_accuracy: 0.9511\n",
            "Epoch 456/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2418 - binary_accuracy: 0.9511\n",
            "Epoch 457/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2414 - binary_accuracy: 0.9511\n",
            "Epoch 458/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2409 - binary_accuracy: 0.9511\n",
            "Epoch 459/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2405 - binary_accuracy: 0.9511\n",
            "Epoch 460/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2401 - binary_accuracy: 0.9511\n",
            "Epoch 461/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2397 - binary_accuracy: 0.9511\n",
            "Epoch 462/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2393 - binary_accuracy: 0.9511\n",
            "Epoch 463/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2388 - binary_accuracy: 0.9511\n",
            "Epoch 464/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2383 - binary_accuracy: 0.9511\n",
            "Epoch 465/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2379 - binary_accuracy: 0.9511\n",
            "Epoch 466/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.2375 - binary_accuracy: 0.9511\n",
            "Epoch 467/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2372 - binary_accuracy: 0.9511\n",
            "Epoch 468/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2368 - binary_accuracy: 0.9511\n",
            "Epoch 469/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2362 - binary_accuracy: 0.9511\n",
            "Epoch 470/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2357 - binary_accuracy: 0.9511\n",
            "Epoch 471/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2351 - binary_accuracy: 0.9511\n",
            "Epoch 472/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2348 - binary_accuracy: 0.9511\n",
            "Epoch 473/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2344 - binary_accuracy: 0.9511\n",
            "Epoch 474/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2340 - binary_accuracy: 0.9511\n",
            "Epoch 475/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2336 - binary_accuracy: 0.9511\n",
            "Epoch 476/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2331 - binary_accuracy: 0.9522\n",
            "Epoch 477/1000\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.2327 - binary_accuracy: 0.9522\n",
            "Epoch 478/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2322 - binary_accuracy: 0.9522\n",
            "Epoch 479/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2318 - binary_accuracy: 0.9522\n",
            "Epoch 480/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2313 - binary_accuracy: 0.9522\n",
            "Epoch 481/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2308 - binary_accuracy: 0.9522\n",
            "Epoch 482/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2304 - binary_accuracy: 0.9522\n",
            "Epoch 483/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2299 - binary_accuracy: 0.9511\n",
            "Epoch 484/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2294 - binary_accuracy: 0.9522\n",
            "Epoch 485/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2291 - binary_accuracy: 0.9533\n",
            "Epoch 486/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2291 - binary_accuracy: 0.9544\n",
            "Epoch 487/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2289 - binary_accuracy: 0.9533\n",
            "Epoch 488/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2286 - binary_accuracy: 0.9544\n",
            "Epoch 489/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2282 - binary_accuracy: 0.9544\n",
            "Epoch 490/1000\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.2276 - binary_accuracy: 0.9556\n",
            "Epoch 491/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.2272 - binary_accuracy: 0.9556\n",
            "Epoch 492/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2266 - binary_accuracy: 0.9556\n",
            "Epoch 493/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2260 - binary_accuracy: 0.9567\n",
            "Epoch 494/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2256 - binary_accuracy: 0.9567\n",
            "Epoch 495/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2249 - binary_accuracy: 0.9556\n",
            "Epoch 496/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2245 - binary_accuracy: 0.9544\n",
            "Epoch 497/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2241 - binary_accuracy: 0.9533\n",
            "Epoch 498/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2235 - binary_accuracy: 0.9533\n",
            "Epoch 499/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2231 - binary_accuracy: 0.9533\n",
            "Epoch 500/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2225 - binary_accuracy: 0.9533\n",
            "Epoch 501/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2222 - binary_accuracy: 0.9533\n",
            "Epoch 502/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2218 - binary_accuracy: 0.9533\n",
            "Epoch 503/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2215 - binary_accuracy: 0.9544\n",
            "Epoch 504/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2211 - binary_accuracy: 0.9556\n",
            "Epoch 505/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2206 - binary_accuracy: 0.9544\n",
            "Epoch 506/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2202 - binary_accuracy: 0.9544\n",
            "Epoch 507/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2197 - binary_accuracy: 0.9544\n",
            "Epoch 508/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2192 - binary_accuracy: 0.9556\n",
            "Epoch 509/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2186 - binary_accuracy: 0.9556\n",
            "Epoch 510/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2182 - binary_accuracy: 0.9567\n",
            "Epoch 511/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2179 - binary_accuracy: 0.9567\n",
            "Epoch 512/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2175 - binary_accuracy: 0.9589\n",
            "Epoch 513/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2171 - binary_accuracy: 0.9578\n",
            "Epoch 514/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2168 - binary_accuracy: 0.9578\n",
            "Epoch 515/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2164 - binary_accuracy: 0.9567\n",
            "Epoch 516/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2160 - binary_accuracy: 0.9589\n",
            "Epoch 517/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2156 - binary_accuracy: 0.9578\n",
            "Epoch 518/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2151 - binary_accuracy: 0.9589\n",
            "Epoch 519/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.2148 - binary_accuracy: 0.9589\n",
            "Epoch 520/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2144 - binary_accuracy: 0.9589\n",
            "Epoch 521/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2141 - binary_accuracy: 0.9589\n",
            "Epoch 522/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2137 - binary_accuracy: 0.9567\n",
            "Epoch 523/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2133 - binary_accuracy: 0.9567\n",
            "Epoch 524/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2129 - binary_accuracy: 0.9556\n",
            "Epoch 525/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2125 - binary_accuracy: 0.9578\n",
            "Epoch 526/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2121 - binary_accuracy: 0.9578\n",
            "Epoch 527/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2117 - binary_accuracy: 0.9589\n",
            "Epoch 528/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2114 - binary_accuracy: 0.9600\n",
            "Epoch 529/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2111 - binary_accuracy: 0.9600\n",
            "Epoch 530/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2109 - binary_accuracy: 0.9600\n",
            "Epoch 531/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2104 - binary_accuracy: 0.9589\n",
            "Epoch 532/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2099 - binary_accuracy: 0.9600\n",
            "Epoch 533/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2095 - binary_accuracy: 0.9589\n",
            "Epoch 534/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2090 - binary_accuracy: 0.9578\n",
            "Epoch 535/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2086 - binary_accuracy: 0.9578\n",
            "Epoch 536/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2081 - binary_accuracy: 0.9589\n",
            "Epoch 537/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2078 - binary_accuracy: 0.9589\n",
            "Epoch 538/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2074 - binary_accuracy: 0.9589\n",
            "Epoch 539/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2073 - binary_accuracy: 0.9578\n",
            "Epoch 540/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2069 - binary_accuracy: 0.9578\n",
            "Epoch 541/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2065 - binary_accuracy: 0.9578\n",
            "Epoch 542/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2061 - binary_accuracy: 0.9567\n",
            "Epoch 543/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2057 - binary_accuracy: 0.9567\n",
            "Epoch 544/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2054 - binary_accuracy: 0.9578\n",
            "Epoch 545/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2050 - binary_accuracy: 0.9589\n",
            "Epoch 546/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2048 - binary_accuracy: 0.9578\n",
            "Epoch 547/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2046 - binary_accuracy: 0.9600\n",
            "Epoch 548/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.2043 - binary_accuracy: 0.9589\n",
            "Epoch 549/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2041 - binary_accuracy: 0.9589\n",
            "Epoch 550/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.2038 - binary_accuracy: 0.9589\n",
            "Epoch 551/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2033 - binary_accuracy: 0.9589\n",
            "Epoch 552/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.2029 - binary_accuracy: 0.9600\n",
            "Epoch 553/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.2024 - binary_accuracy: 0.9600\n",
            "Epoch 554/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.2019 - binary_accuracy: 0.9611\n",
            "Epoch 555/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2015 - binary_accuracy: 0.9589\n",
            "Epoch 556/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.2011 - binary_accuracy: 0.9589\n",
            "Epoch 557/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.2007 - binary_accuracy: 0.9589\n",
            "Epoch 558/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2004 - binary_accuracy: 0.9600\n",
            "Epoch 559/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2001 - binary_accuracy: 0.9611\n",
            "Epoch 560/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1999 - binary_accuracy: 0.9600\n",
            "Epoch 561/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1998 - binary_accuracy: 0.9600\n",
            "Epoch 562/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1997 - binary_accuracy: 0.9611\n",
            "Epoch 563/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1995 - binary_accuracy: 0.9611\n",
            "Epoch 564/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1994 - binary_accuracy: 0.9622\n",
            "Epoch 565/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1988 - binary_accuracy: 0.9622\n",
            "Epoch 566/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1983 - binary_accuracy: 0.9611\n",
            "Epoch 567/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1977 - binary_accuracy: 0.9611\n",
            "Epoch 568/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1972 - binary_accuracy: 0.9611\n",
            "Epoch 569/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1968 - binary_accuracy: 0.9600\n",
            "Epoch 570/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1964 - binary_accuracy: 0.9600\n",
            "Epoch 571/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1959 - binary_accuracy: 0.9600\n",
            "Epoch 572/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1954 - binary_accuracy: 0.9600\n",
            "Epoch 573/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1950 - binary_accuracy: 0.9611\n",
            "Epoch 574/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1946 - binary_accuracy: 0.9622\n",
            "Epoch 575/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1944 - binary_accuracy: 0.9633\n",
            "Epoch 576/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1942 - binary_accuracy: 0.9644\n",
            "Epoch 577/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1942 - binary_accuracy: 0.9644\n",
            "Epoch 578/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1940 - binary_accuracy: 0.9644\n",
            "Epoch 579/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1937 - binary_accuracy: 0.9644\n",
            "Epoch 580/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1933 - binary_accuracy: 0.9633\n",
            "Epoch 581/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1929 - binary_accuracy: 0.9633\n",
            "Epoch 582/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1926 - binary_accuracy: 0.9633\n",
            "Epoch 583/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1921 - binary_accuracy: 0.9633\n",
            "Epoch 584/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1917 - binary_accuracy: 0.9633\n",
            "Epoch 585/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1913 - binary_accuracy: 0.9633\n",
            "Epoch 586/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1908 - binary_accuracy: 0.9633\n",
            "Epoch 587/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1906 - binary_accuracy: 0.9633\n",
            "Epoch 588/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1902 - binary_accuracy: 0.9633\n",
            "Epoch 589/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1899 - binary_accuracy: 0.9633\n",
            "Epoch 590/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1896 - binary_accuracy: 0.9633\n",
            "Epoch 591/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1893 - binary_accuracy: 0.9633\n",
            "Epoch 592/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1889 - binary_accuracy: 0.9633\n",
            "Epoch 593/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1886 - binary_accuracy: 0.9633\n",
            "Epoch 594/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1883 - binary_accuracy: 0.9644\n",
            "Epoch 595/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1880 - binary_accuracy: 0.9644\n",
            "Epoch 596/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1876 - binary_accuracy: 0.9644\n",
            "Epoch 597/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1872 - binary_accuracy: 0.9644\n",
            "Epoch 598/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1869 - binary_accuracy: 0.9644\n",
            "Epoch 599/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1865 - binary_accuracy: 0.9644\n",
            "Epoch 600/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1862 - binary_accuracy: 0.9644\n",
            "Epoch 601/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1859 - binary_accuracy: 0.9644\n",
            "Epoch 602/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1857 - binary_accuracy: 0.9644\n",
            "Epoch 603/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1853 - binary_accuracy: 0.9644\n",
            "Epoch 604/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1849 - binary_accuracy: 0.9644\n",
            "Epoch 605/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1847 - binary_accuracy: 0.9644\n",
            "Epoch 606/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1843 - binary_accuracy: 0.9644\n",
            "Epoch 607/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1840 - binary_accuracy: 0.9644\n",
            "Epoch 608/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1837 - binary_accuracy: 0.9656\n",
            "Epoch 609/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1835 - binary_accuracy: 0.9656\n",
            "Epoch 610/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1834 - binary_accuracy: 0.9656\n",
            "Epoch 611/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1833 - binary_accuracy: 0.9667\n",
            "Epoch 612/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1831 - binary_accuracy: 0.9656\n",
            "Epoch 613/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1829 - binary_accuracy: 0.9667\n",
            "Epoch 614/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1824 - binary_accuracy: 0.9667\n",
            "Epoch 615/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1820 - binary_accuracy: 0.9667\n",
            "Epoch 616/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1816 - binary_accuracy: 0.9667\n",
            "Epoch 617/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1813 - binary_accuracy: 0.9656\n",
            "Epoch 618/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1809 - binary_accuracy: 0.9656\n",
            "Epoch 619/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1805 - binary_accuracy: 0.9656\n",
            "Epoch 620/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1804 - binary_accuracy: 0.9656\n",
            "Epoch 621/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1801 - binary_accuracy: 0.9656\n",
            "Epoch 622/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1798 - binary_accuracy: 0.9656\n",
            "Epoch 623/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1794 - binary_accuracy: 0.9667\n",
            "Epoch 624/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1791 - binary_accuracy: 0.9667\n",
            "Epoch 625/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1788 - binary_accuracy: 0.9656\n",
            "Epoch 626/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1783 - binary_accuracy: 0.9656\n",
            "Epoch 627/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1780 - binary_accuracy: 0.9656\n",
            "Epoch 628/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1775 - binary_accuracy: 0.9678\n",
            "Epoch 629/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1773 - binary_accuracy: 0.9667\n",
            "Epoch 630/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1771 - binary_accuracy: 0.9667\n",
            "Epoch 631/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1767 - binary_accuracy: 0.9678\n",
            "Epoch 632/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1764 - binary_accuracy: 0.9678\n",
            "Epoch 633/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1762 - binary_accuracy: 0.9689\n",
            "Epoch 634/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1758 - binary_accuracy: 0.9689\n",
            "Epoch 635/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1755 - binary_accuracy: 0.9689\n",
            "Epoch 636/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1754 - binary_accuracy: 0.9678\n",
            "Epoch 637/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1753 - binary_accuracy: 0.9678\n",
            "Epoch 638/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1750 - binary_accuracy: 0.9689\n",
            "Epoch 639/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1748 - binary_accuracy: 0.9689\n",
            "Epoch 640/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1744 - binary_accuracy: 0.9689\n",
            "Epoch 641/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1741 - binary_accuracy: 0.9700\n",
            "Epoch 642/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1738 - binary_accuracy: 0.9700\n",
            "Epoch 643/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1735 - binary_accuracy: 0.9700\n",
            "Epoch 644/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1732 - binary_accuracy: 0.9700\n",
            "Epoch 645/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1728 - binary_accuracy: 0.9700\n",
            "Epoch 646/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1725 - binary_accuracy: 0.9700\n",
            "Epoch 647/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1722 - binary_accuracy: 0.9700\n",
            "Epoch 648/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1719 - binary_accuracy: 0.9700\n",
            "Epoch 649/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1716 - binary_accuracy: 0.9700\n",
            "Epoch 650/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1713 - binary_accuracy: 0.9711\n",
            "Epoch 651/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1709 - binary_accuracy: 0.9700\n",
            "Epoch 652/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1707 - binary_accuracy: 0.9689\n",
            "Epoch 653/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1704 - binary_accuracy: 0.9700\n",
            "Epoch 654/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1702 - binary_accuracy: 0.9700\n",
            "Epoch 655/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1701 - binary_accuracy: 0.9711\n",
            "Epoch 656/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1700 - binary_accuracy: 0.9711\n",
            "Epoch 657/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1699 - binary_accuracy: 0.9711\n",
            "Epoch 658/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1697 - binary_accuracy: 0.9711\n",
            "Epoch 659/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1695 - binary_accuracy: 0.9711\n",
            "Epoch 660/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1691 - binary_accuracy: 0.9711\n",
            "Epoch 661/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1688 - binary_accuracy: 0.9722\n",
            "Epoch 662/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1684 - binary_accuracy: 0.9722\n",
            "Epoch 663/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1679 - binary_accuracy: 0.9711\n",
            "Epoch 664/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1675 - binary_accuracy: 0.9711\n",
            "Epoch 665/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1672 - binary_accuracy: 0.9711\n",
            "Epoch 666/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1669 - binary_accuracy: 0.9700\n",
            "Epoch 667/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1667 - binary_accuracy: 0.9700\n",
            "Epoch 668/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1664 - binary_accuracy: 0.9700\n",
            "Epoch 669/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1661 - binary_accuracy: 0.9700\n",
            "Epoch 670/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1658 - binary_accuracy: 0.9711\n",
            "Epoch 671/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1656 - binary_accuracy: 0.9711\n",
            "Epoch 672/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1653 - binary_accuracy: 0.9733\n",
            "Epoch 673/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1651 - binary_accuracy: 0.9733\n",
            "Epoch 674/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1648 - binary_accuracy: 0.9722\n",
            "Epoch 675/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1646 - binary_accuracy: 0.9722\n",
            "Epoch 676/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1643 - binary_accuracy: 0.9722\n",
            "Epoch 677/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1641 - binary_accuracy: 0.9744\n",
            "Epoch 678/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1638 - binary_accuracy: 0.9744\n",
            "Epoch 679/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1636 - binary_accuracy: 0.9744\n",
            "Epoch 680/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1633 - binary_accuracy: 0.9744\n",
            "Epoch 681/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1631 - binary_accuracy: 0.9733\n",
            "Epoch 682/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1628 - binary_accuracy: 0.9733\n",
            "Epoch 683/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1625 - binary_accuracy: 0.9733\n",
            "Epoch 684/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1623 - binary_accuracy: 0.9733\n",
            "Epoch 685/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1620 - binary_accuracy: 0.9722\n",
            "Epoch 686/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1616 - binary_accuracy: 0.9722\n",
            "Epoch 687/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1613 - binary_accuracy: 0.9744\n",
            "Epoch 688/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1609 - binary_accuracy: 0.9733\n",
            "Epoch 689/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1607 - binary_accuracy: 0.9733\n",
            "Epoch 690/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1604 - binary_accuracy: 0.9722\n",
            "Epoch 691/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1601 - binary_accuracy: 0.9722\n",
            "Epoch 692/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1599 - binary_accuracy: 0.9722\n",
            "Epoch 693/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1597 - binary_accuracy: 0.9711\n",
            "Epoch 694/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1594 - binary_accuracy: 0.9722\n",
            "Epoch 695/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1592 - binary_accuracy: 0.9722\n",
            "Epoch 696/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.1589 - binary_accuracy: 0.9733\n",
            "Epoch 697/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1587 - binary_accuracy: 0.9733\n",
            "Epoch 698/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1586 - binary_accuracy: 0.9733\n",
            "Epoch 699/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1583 - binary_accuracy: 0.9733\n",
            "Epoch 700/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1581 - binary_accuracy: 0.9733\n",
            "Epoch 701/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1578 - binary_accuracy: 0.9744\n",
            "Epoch 702/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1575 - binary_accuracy: 0.9744\n",
            "Epoch 703/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1572 - binary_accuracy: 0.9744\n",
            "Epoch 704/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1569 - binary_accuracy: 0.9744\n",
            "Epoch 705/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1566 - binary_accuracy: 0.9744\n",
            "Epoch 706/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1565 - binary_accuracy: 0.9733\n",
            "Epoch 707/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1563 - binary_accuracy: 0.9722\n",
            "Epoch 708/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1561 - binary_accuracy: 0.9722\n",
            "Epoch 709/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1559 - binary_accuracy: 0.9722\n",
            "Epoch 710/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1557 - binary_accuracy: 0.9711\n",
            "Epoch 711/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1556 - binary_accuracy: 0.9722\n",
            "Epoch 712/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1553 - binary_accuracy: 0.9722\n",
            "Epoch 713/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1549 - binary_accuracy: 0.9733\n",
            "Epoch 714/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1546 - binary_accuracy: 0.9722\n",
            "Epoch 715/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1544 - binary_accuracy: 0.9744\n",
            "Epoch 716/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1541 - binary_accuracy: 0.9744\n",
            "Epoch 717/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1539 - binary_accuracy: 0.9744\n",
            "Epoch 718/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1536 - binary_accuracy: 0.9744\n",
            "Epoch 719/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1533 - binary_accuracy: 0.9744\n",
            "Epoch 720/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1531 - binary_accuracy: 0.9744\n",
            "Epoch 721/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1529 - binary_accuracy: 0.9756\n",
            "Epoch 722/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1526 - binary_accuracy: 0.9756\n",
            "Epoch 723/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1526 - binary_accuracy: 0.9756\n",
            "Epoch 724/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1525 - binary_accuracy: 0.9756\n",
            "Epoch 725/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1524 - binary_accuracy: 0.9733\n",
            "Epoch 726/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1522 - binary_accuracy: 0.9744\n",
            "Epoch 727/1000\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1520 - binary_accuracy: 0.9733\n",
            "Epoch 728/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1518 - binary_accuracy: 0.9733\n",
            "Epoch 729/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1515 - binary_accuracy: 0.9744\n",
            "Epoch 730/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1512 - binary_accuracy: 0.9744\n",
            "Epoch 731/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1509 - binary_accuracy: 0.9744\n",
            "Epoch 732/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1505 - binary_accuracy: 0.9733\n",
            "Epoch 733/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1502 - binary_accuracy: 0.9733\n",
            "Epoch 734/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1500 - binary_accuracy: 0.9733\n",
            "Epoch 735/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1501 - binary_accuracy: 0.9733\n",
            "Epoch 736/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1498 - binary_accuracy: 0.9733\n",
            "Epoch 737/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1496 - binary_accuracy: 0.9733\n",
            "Epoch 738/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1493 - binary_accuracy: 0.9722\n",
            "Epoch 739/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1490 - binary_accuracy: 0.9733\n",
            "Epoch 740/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1487 - binary_accuracy: 0.9744\n",
            "Epoch 741/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1485 - binary_accuracy: 0.9744\n",
            "Epoch 742/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1483 - binary_accuracy: 0.9744\n",
            "Epoch 743/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1480 - binary_accuracy: 0.9744\n",
            "Epoch 744/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1478 - binary_accuracy: 0.9767\n",
            "Epoch 745/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1475 - binary_accuracy: 0.9767\n",
            "Epoch 746/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1473 - binary_accuracy: 0.9767\n",
            "Epoch 747/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1471 - binary_accuracy: 0.9767\n",
            "Epoch 748/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1469 - binary_accuracy: 0.9767\n",
            "Epoch 749/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1467 - binary_accuracy: 0.9767\n",
            "Epoch 750/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1465 - binary_accuracy: 0.9767\n",
            "Epoch 751/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1464 - binary_accuracy: 0.9767\n",
            "Epoch 752/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1462 - binary_accuracy: 0.9767\n",
            "Epoch 753/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1461 - binary_accuracy: 0.9767\n",
            "Epoch 754/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1461 - binary_accuracy: 0.9767\n",
            "Epoch 755/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1461 - binary_accuracy: 0.9756\n",
            "Epoch 756/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1463 - binary_accuracy: 0.9756\n",
            "Epoch 757/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1461 - binary_accuracy: 0.9756\n",
            "Epoch 758/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1458 - binary_accuracy: 0.9756\n",
            "Epoch 759/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1454 - binary_accuracy: 0.9767\n",
            "Epoch 760/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1450 - binary_accuracy: 0.9778\n",
            "Epoch 761/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1446 - binary_accuracy: 0.9778\n",
            "Epoch 762/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1443 - binary_accuracy: 0.9767\n",
            "Epoch 763/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1439 - binary_accuracy: 0.9767\n",
            "Epoch 764/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1436 - binary_accuracy: 0.9767\n",
            "Epoch 765/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1433 - binary_accuracy: 0.9767\n",
            "Epoch 766/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1431 - binary_accuracy: 0.9778\n",
            "Epoch 767/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1428 - binary_accuracy: 0.9778\n",
            "Epoch 768/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1426 - binary_accuracy: 0.9767\n",
            "Epoch 769/1000\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1424 - binary_accuracy: 0.9778\n",
            "Epoch 770/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1424 - binary_accuracy: 0.9789\n",
            "Epoch 771/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1422 - binary_accuracy: 0.9778\n",
            "Epoch 772/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1419 - binary_accuracy: 0.9778\n",
            "Epoch 773/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1417 - binary_accuracy: 0.9778\n",
            "Epoch 774/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1414 - binary_accuracy: 0.9778\n",
            "Epoch 775/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1412 - binary_accuracy: 0.9778\n",
            "Epoch 776/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1409 - binary_accuracy: 0.9778\n",
            "Epoch 777/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1407 - binary_accuracy: 0.9767\n",
            "Epoch 778/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1407 - binary_accuracy: 0.9756\n",
            "Epoch 779/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1406 - binary_accuracy: 0.9756\n",
            "Epoch 780/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1405 - binary_accuracy: 0.9756\n",
            "Epoch 781/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.1404 - binary_accuracy: 0.9756\n",
            "Epoch 782/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1402 - binary_accuracy: 0.9756\n",
            "Epoch 783/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1399 - binary_accuracy: 0.9756\n",
            "Epoch 784/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1396 - binary_accuracy: 0.9767\n",
            "Epoch 785/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1394 - binary_accuracy: 0.9767\n",
            "Epoch 786/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1391 - binary_accuracy: 0.9767\n",
            "Epoch 787/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1388 - binary_accuracy: 0.9767\n",
            "Epoch 788/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1388 - binary_accuracy: 0.9767\n",
            "Epoch 789/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1387 - binary_accuracy: 0.9778\n",
            "Epoch 790/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1385 - binary_accuracy: 0.9778\n",
            "Epoch 791/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1382 - binary_accuracy: 0.9778\n",
            "Epoch 792/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1379 - binary_accuracy: 0.9767\n",
            "Epoch 793/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1377 - binary_accuracy: 0.9767\n",
            "Epoch 794/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1374 - binary_accuracy: 0.9767\n",
            "Epoch 795/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1371 - binary_accuracy: 0.9767\n",
            "Epoch 796/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1369 - binary_accuracy: 0.9767\n",
            "Epoch 797/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1366 - binary_accuracy: 0.9767\n",
            "Epoch 798/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1365 - binary_accuracy: 0.9767\n",
            "Epoch 799/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1362 - binary_accuracy: 0.9767\n",
            "Epoch 800/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1361 - binary_accuracy: 0.9778\n",
            "Epoch 801/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1360 - binary_accuracy: 0.9778\n",
            "Epoch 802/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1358 - binary_accuracy: 0.9778\n",
            "Epoch 803/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1356 - binary_accuracy: 0.9789\n",
            "Epoch 804/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1353 - binary_accuracy: 0.9778\n",
            "Epoch 805/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1351 - binary_accuracy: 0.9778\n",
            "Epoch 806/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1350 - binary_accuracy: 0.9778\n",
            "Epoch 807/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1348 - binary_accuracy: 0.9767\n",
            "Epoch 808/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1347 - binary_accuracy: 0.9767\n",
            "Epoch 809/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1345 - binary_accuracy: 0.9767\n",
            "Epoch 810/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1342 - binary_accuracy: 0.9767\n",
            "Epoch 811/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1342 - binary_accuracy: 0.9778\n",
            "Epoch 812/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1339 - binary_accuracy: 0.9778\n",
            "Epoch 813/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1336 - binary_accuracy: 0.9789\n",
            "Epoch 814/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1335 - binary_accuracy: 0.9789\n",
            "Epoch 815/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1333 - binary_accuracy: 0.9789\n",
            "Epoch 816/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1333 - binary_accuracy: 0.9789\n",
            "Epoch 817/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1330 - binary_accuracy: 0.9789\n",
            "Epoch 818/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1329 - binary_accuracy: 0.9789\n",
            "Epoch 819/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1327 - binary_accuracy: 0.9789\n",
            "Epoch 820/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1326 - binary_accuracy: 0.9778\n",
            "Epoch 821/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1324 - binary_accuracy: 0.9778\n",
            "Epoch 822/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1322 - binary_accuracy: 0.9789\n",
            "Epoch 823/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1319 - binary_accuracy: 0.9789\n",
            "Epoch 824/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1317 - binary_accuracy: 0.9789\n",
            "Epoch 825/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1316 - binary_accuracy: 0.9789\n",
            "Epoch 826/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1314 - binary_accuracy: 0.9778\n",
            "Epoch 827/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1313 - binary_accuracy: 0.9778\n",
            "Epoch 828/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1311 - binary_accuracy: 0.9789\n",
            "Epoch 829/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1310 - binary_accuracy: 0.9789\n",
            "Epoch 830/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1308 - binary_accuracy: 0.9789\n",
            "Epoch 831/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1307 - binary_accuracy: 0.9789\n",
            "Epoch 832/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1305 - binary_accuracy: 0.9789\n",
            "Epoch 833/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1304 - binary_accuracy: 0.9789\n",
            "Epoch 834/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1302 - binary_accuracy: 0.9778\n",
            "Epoch 835/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1299 - binary_accuracy: 0.9778\n",
            "Epoch 836/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1298 - binary_accuracy: 0.9778\n",
            "Epoch 837/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1296 - binary_accuracy: 0.9778\n",
            "Epoch 838/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1296 - binary_accuracy: 0.9778\n",
            "Epoch 839/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1296 - binary_accuracy: 0.9778\n",
            "Epoch 840/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1296 - binary_accuracy: 0.9778\n",
            "Epoch 841/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1295 - binary_accuracy: 0.9789\n",
            "Epoch 842/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1294 - binary_accuracy: 0.9789\n",
            "Epoch 843/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1292 - binary_accuracy: 0.9789\n",
            "Epoch 844/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1290 - binary_accuracy: 0.9789\n",
            "Epoch 845/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1289 - binary_accuracy: 0.9789\n",
            "Epoch 846/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1287 - binary_accuracy: 0.9789\n",
            "Epoch 847/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1283 - binary_accuracy: 0.9789\n",
            "Epoch 848/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1280 - binary_accuracy: 0.9778\n",
            "Epoch 849/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1276 - binary_accuracy: 0.9778\n",
            "Epoch 850/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1273 - binary_accuracy: 0.9789\n",
            "Epoch 851/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1270 - binary_accuracy: 0.9789\n",
            "Epoch 852/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1268 - binary_accuracy: 0.9789\n",
            "Epoch 853/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.1265 - binary_accuracy: 0.9789\n",
            "Epoch 854/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1263 - binary_accuracy: 0.9789\n",
            "Epoch 855/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1262 - binary_accuracy: 0.9789\n",
            "Epoch 856/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1260 - binary_accuracy: 0.9800\n",
            "Epoch 857/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1259 - binary_accuracy: 0.9800\n",
            "Epoch 858/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1257 - binary_accuracy: 0.9800\n",
            "Epoch 859/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1255 - binary_accuracy: 0.9800\n",
            "Epoch 860/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1253 - binary_accuracy: 0.9800\n",
            "Epoch 861/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1252 - binary_accuracy: 0.9800\n",
            "Epoch 862/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1250 - binary_accuracy: 0.9800\n",
            "Epoch 863/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1248 - binary_accuracy: 0.9789\n",
            "Epoch 864/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1247 - binary_accuracy: 0.9789\n",
            "Epoch 865/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1246 - binary_accuracy: 0.9789\n",
            "Epoch 866/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1245 - binary_accuracy: 0.9789\n",
            "Epoch 867/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1243 - binary_accuracy: 0.9789\n",
            "Epoch 868/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1242 - binary_accuracy: 0.9789\n",
            "Epoch 869/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1240 - binary_accuracy: 0.9789\n",
            "Epoch 870/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1238 - binary_accuracy: 0.9789\n",
            "Epoch 871/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1235 - binary_accuracy: 0.9789\n",
            "Epoch 872/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1233 - binary_accuracy: 0.9800\n",
            "Epoch 873/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1231 - binary_accuracy: 0.9800\n",
            "Epoch 874/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1230 - binary_accuracy: 0.9800\n",
            "Epoch 875/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1228 - binary_accuracy: 0.9800\n",
            "Epoch 876/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1226 - binary_accuracy: 0.9800\n",
            "Epoch 877/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1225 - binary_accuracy: 0.9800\n",
            "Epoch 878/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1223 - binary_accuracy: 0.9789\n",
            "Epoch 879/1000\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.1221 - binary_accuracy: 0.9800\n",
            "Epoch 880/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1221 - binary_accuracy: 0.9800\n",
            "Epoch 881/1000\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1219 - binary_accuracy: 0.9800\n",
            "Epoch 882/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1217 - binary_accuracy: 0.9822\n",
            "Epoch 883/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1216 - binary_accuracy: 0.9822\n",
            "Epoch 884/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1213 - binary_accuracy: 0.9822\n",
            "Epoch 885/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1212 - binary_accuracy: 0.9822\n",
            "Epoch 886/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1209 - binary_accuracy: 0.9822\n",
            "Epoch 887/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1208 - binary_accuracy: 0.9822\n",
            "Epoch 888/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1208 - binary_accuracy: 0.9822\n",
            "Epoch 889/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1205 - binary_accuracy: 0.9822\n",
            "Epoch 890/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1203 - binary_accuracy: 0.9822\n",
            "Epoch 891/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1202 - binary_accuracy: 0.9822\n",
            "Epoch 892/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1201 - binary_accuracy: 0.9822\n",
            "Epoch 893/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1199 - binary_accuracy: 0.9822\n",
            "Epoch 894/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1197 - binary_accuracy: 0.9822\n",
            "Epoch 895/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1195 - binary_accuracy: 0.9822\n",
            "Epoch 896/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1195 - binary_accuracy: 0.9822\n",
            "Epoch 897/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1193 - binary_accuracy: 0.9822\n",
            "Epoch 898/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1192 - binary_accuracy: 0.9822\n",
            "Epoch 899/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1191 - binary_accuracy: 0.9833\n",
            "Epoch 900/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1190 - binary_accuracy: 0.9833\n",
            "Epoch 901/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1188 - binary_accuracy: 0.9833\n",
            "Epoch 902/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1187 - binary_accuracy: 0.9833\n",
            "Epoch 903/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1185 - binary_accuracy: 0.9833\n",
            "Epoch 904/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1183 - binary_accuracy: 0.9833\n",
            "Epoch 905/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1181 - binary_accuracy: 0.9833\n",
            "Epoch 906/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1179 - binary_accuracy: 0.9833\n",
            "Epoch 907/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1177 - binary_accuracy: 0.9833\n",
            "Epoch 908/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1176 - binary_accuracy: 0.9833\n",
            "Epoch 909/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1174 - binary_accuracy: 0.9833\n",
            "Epoch 910/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1173 - binary_accuracy: 0.9833\n",
            "Epoch 911/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1171 - binary_accuracy: 0.9833\n",
            "Epoch 912/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1170 - binary_accuracy: 0.9833\n",
            "Epoch 913/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1168 - binary_accuracy: 0.9833\n",
            "Epoch 914/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1166 - binary_accuracy: 0.9833\n",
            "Epoch 915/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1164 - binary_accuracy: 0.9833\n",
            "Epoch 916/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1163 - binary_accuracy: 0.9833\n",
            "Epoch 917/1000\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1161 - binary_accuracy: 0.9822\n",
            "Epoch 918/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1159 - binary_accuracy: 0.9822\n",
            "Epoch 919/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1159 - binary_accuracy: 0.9822\n",
            "Epoch 920/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1158 - binary_accuracy: 0.9822\n",
            "Epoch 921/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1158 - binary_accuracy: 0.9822\n",
            "Epoch 922/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1157 - binary_accuracy: 0.9833\n",
            "Epoch 923/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1155 - binary_accuracy: 0.9833\n",
            "Epoch 924/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1154 - binary_accuracy: 0.9833\n",
            "Epoch 925/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1153 - binary_accuracy: 0.9822\n",
            "Epoch 926/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1152 - binary_accuracy: 0.9822\n",
            "Epoch 927/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1151 - binary_accuracy: 0.9822\n",
            "Epoch 928/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1150 - binary_accuracy: 0.9822\n",
            "Epoch 929/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1149 - binary_accuracy: 0.9833\n",
            "Epoch 930/1000\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1147 - binary_accuracy: 0.9833\n",
            "Epoch 931/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1144 - binary_accuracy: 0.9833\n",
            "Epoch 932/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1142 - binary_accuracy: 0.9822\n",
            "Epoch 933/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1140 - binary_accuracy: 0.9822\n",
            "Epoch 934/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1138 - binary_accuracy: 0.9822\n",
            "Epoch 935/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1135 - binary_accuracy: 0.9833\n",
            "Epoch 936/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1135 - binary_accuracy: 0.9833\n",
            "Epoch 937/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1132 - binary_accuracy: 0.9833\n",
            "Epoch 938/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1131 - binary_accuracy: 0.9833\n",
            "Epoch 939/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1129 - binary_accuracy: 0.9833\n",
            "Epoch 940/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1128 - binary_accuracy: 0.9833\n",
            "Epoch 941/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1127 - binary_accuracy: 0.9833\n",
            "Epoch 942/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1125 - binary_accuracy: 0.9833\n",
            "Epoch 943/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1124 - binary_accuracy: 0.9833\n",
            "Epoch 944/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1123 - binary_accuracy: 0.9833\n",
            "Epoch 945/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1121 - binary_accuracy: 0.9833\n",
            "Epoch 946/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1120 - binary_accuracy: 0.9833\n",
            "Epoch 947/1000\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1119 - binary_accuracy: 0.9833\n",
            "Epoch 948/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1118 - binary_accuracy: 0.9833\n",
            "Epoch 949/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1117 - binary_accuracy: 0.9833\n",
            "Epoch 950/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1115 - binary_accuracy: 0.9833\n",
            "Epoch 951/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1113 - binary_accuracy: 0.9833\n",
            "Epoch 952/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1112 - binary_accuracy: 0.9833\n",
            "Epoch 953/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1110 - binary_accuracy: 0.9833\n",
            "Epoch 954/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1109 - binary_accuracy: 0.9833\n",
            "Epoch 955/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1108 - binary_accuracy: 0.9833\n",
            "Epoch 956/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1107 - binary_accuracy: 0.9833\n",
            "Epoch 957/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1106 - binary_accuracy: 0.9833\n",
            "Epoch 958/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1105 - binary_accuracy: 0.9833\n",
            "Epoch 959/1000\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1104 - binary_accuracy: 0.9833\n",
            "Epoch 960/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1103 - binary_accuracy: 0.9833\n",
            "Epoch 961/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1100 - binary_accuracy: 0.9833\n",
            "Epoch 962/1000\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1099 - binary_accuracy: 0.9833\n",
            "Epoch 963/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1099 - binary_accuracy: 0.9833\n",
            "Epoch 964/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1098 - binary_accuracy: 0.9833\n",
            "Epoch 965/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1098 - binary_accuracy: 0.9833\n",
            "Epoch 966/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1097 - binary_accuracy: 0.9833\n",
            "Epoch 967/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1096 - binary_accuracy: 0.9833\n",
            "Epoch 968/1000\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1094 - binary_accuracy: 0.9833\n",
            "Epoch 969/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1092 - binary_accuracy: 0.9833\n",
            "Epoch 970/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1090 - binary_accuracy: 0.9833\n",
            "Epoch 971/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1088 - binary_accuracy: 0.9833\n",
            "Epoch 972/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1086 - binary_accuracy: 0.9822\n",
            "Epoch 973/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1084 - binary_accuracy: 0.9822\n",
            "Epoch 974/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1084 - binary_accuracy: 0.9822\n",
            "Epoch 975/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1084 - binary_accuracy: 0.9833\n",
            "Epoch 976/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1083 - binary_accuracy: 0.9833\n",
            "Epoch 977/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1082 - binary_accuracy: 0.9833\n",
            "Epoch 978/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1080 - binary_accuracy: 0.9833\n",
            "Epoch 979/1000\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1079 - binary_accuracy: 0.9833\n",
            "Epoch 980/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1077 - binary_accuracy: 0.9844\n",
            "Epoch 981/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1075 - binary_accuracy: 0.9844\n",
            "Epoch 982/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1074 - binary_accuracy: 0.9844\n",
            "Epoch 983/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1072 - binary_accuracy: 0.9833\n",
            "Epoch 984/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1072 - binary_accuracy: 0.9833\n",
            "Epoch 985/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1072 - binary_accuracy: 0.9833\n",
            "Epoch 986/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1072 - binary_accuracy: 0.9833\n",
            "Epoch 987/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1071 - binary_accuracy: 0.9833\n",
            "Epoch 988/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1070 - binary_accuracy: 0.9833\n",
            "Epoch 989/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1068 - binary_accuracy: 0.9833\n",
            "Epoch 990/1000\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1066 - binary_accuracy: 0.9833\n",
            "Epoch 991/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1063 - binary_accuracy: 0.9833\n",
            "Epoch 992/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1062 - binary_accuracy: 0.9833\n",
            "Epoch 993/1000\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1060 - binary_accuracy: 0.9833\n",
            "Epoch 994/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1058 - binary_accuracy: 0.9844\n",
            "Epoch 995/1000\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1056 - binary_accuracy: 0.9844\n",
            "Epoch 996/1000\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1055 - binary_accuracy: 0.9833\n",
            "Epoch 997/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1053 - binary_accuracy: 0.9833\n",
            "Epoch 998/1000\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1052 - binary_accuracy: 0.9833\n",
            "Epoch 999/1000\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1051 - binary_accuracy: 0.9833\n",
            "Epoch 1000/1000\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1050 - binary_accuracy: 0.9833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As number of iterations increases, loss keeps on increasing and we get a very good accuracy."
      ],
      "metadata": {
        "id": "jq1Cc9ZapFl2"
      }
    }
  ]
}