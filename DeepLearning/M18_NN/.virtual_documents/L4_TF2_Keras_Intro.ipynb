


import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt



print(tf.__version__)


# load mnist dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()



print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)


idx=1234
plt.imshow(x_train[idx], cmap='gray', interpolation='none')
print(y_train[idx])


# number of class labels
num_labels = len(np.unique(y_train))
print(num_labels)


# convert y_i's to one-hot vectors
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)


# Image: 2D matrix to 1D vector
image_size = x_train.shape[1]
input_size = image_size * image_size

# 2D to 1D + Normalize (0-255)
x_train = np.reshape(x_train, [-1, input_size])
x_train = x_train.astype('float32') / 255
x_test = np.reshape(x_test, [-1, input_size])
x_test = x_test.astype('float32') / 255


# Hyper-params of NN
batch_size = 128 # typically 2^n
hidden_units = 256


# Sequential API

# 3-layer MLP with ReLU
model = Sequential()
model.add(Dense(hidden_units, input_dim=input_size))
model.add(Activation('relu'))
model.add(Dense(hidden_units))
model.add(Activation('relu'))
model.add(Dense(num_labels))
# this is the output for one-hot vector
model.add(Activation('softmax'))
model.summary()


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


# train the network
%%time
model.fit(x_train, y_train, epochs=20, batch_size=batch_size)

#60000/128 ~ 469


# test dataset  performance
loss, acc = model.evaluate(x_test,
                        y_test,
                        batch_size=batch_size,
                        verbose=0)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))
print(loss)

# Overfit or Underfit?





print(x_train.shape)


inputs = keras.Input(shape=(784,))
inputs.shape


# Functional API: much more intuitive
# Refer: https://www.tensorflow.org/guide/keras/functional
x1 = layers.Dense(hidden_units, activation="relu")(inputs)
x2 = layers.Dense(hidden_units, activation="relu")(x1)
outputs = layers.Dense(num_labels, activation="softmax")(x2)

model = keras.Model(inputs=inputs, outputs=outputs, name="simple_model")
model.summary()



model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


# train the network
%%time
model.fit(x_train, y_train, epochs=20, batch_size=batch_size)






# Model with Dropout
x = layers.Dense(hidden_units, activation="relu")(inputs)
x = Dropout(0.3)(x)
x = layers.Dense(hidden_units, activation="relu")(x)
x = Dropout(0.3)(x)
outputs = layers.Dense(num_labels, activation="softmax")(x)

model = keras.Model(inputs=inputs, outputs=outputs, name="simple_model")
model.summary()


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


%%time
# Increased epochs as we added dropouts
model.fit(x_train, y_train, epochs=30, batch_size=batch_size)



# test dataset  performance
loss, acc = model.evaluate(x_test,
                        y_test,
                        batch_size=batch_size,
                        verbose=0)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))
print(loss)

# Overfit or Underfit?
# Can we add more layers?











# CPU: 5 layered model
x = layers.Dense(256, activation="relu")(inputs)
x = Dropout(0.3)(x)
x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
outputs = layers.Dense(num_labels, activation="softmax")(x)

model = keras.Model(inputs=inputs, outputs=outputs, name="simple_model")
model.summary()


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


%%time
model.fit(x_train, y_train, epochs=30, batch_size=batch_size)



# test dataset  performance
loss, acc = model.evaluate(x_test,
                        y_test,
                        batch_size=batch_size,
                        verbose=0)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))
print(loss)





# with GPU: Menu--> Runtime--> Change runtime type --> Hardware Accelrator (=GPU )
# The whole runtime changes and you may have to rerun some older cells.
# GPU: 5 layered model
x = layers.Dense(256, activation="relu")(inputs)
x = Dropout(0.3)(x)
x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
outputs = layers.Dense(num_labels, activation="softmax")(x)

model = keras.Model(inputs=inputs, outputs=outputs, name="simple_model")
model.summary()


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


%%time
model.fit(x_train, y_train, epochs=30, batch_size=batch_size)



## 3min23sec (CPU) ----VS---- 1min23sec (GPU)
## 14ms/step vs 4ms/step

# ALWAYS USE GPU/TPU for DL, if you can.

#Assignment: Try this with TPUs(from Google)





# Load the TensorBoard notebook extension.
%load_ext tensorboard

# Define the Keras TensorBoard callback.
from datetime import datetime

logdir="logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)

# what is a callback?

history = model.fit(x_train, y_train, epochs=30, batch_size=batch_size, callbacks=[tensorboard_callback])


%tensorboard --logdir logs

# SCALARS --> accuracy and loss
# GRAPHS --> Op graph and Conceptual graph





# Fewer epochs due to avoiding "internal covariance shift" and can accomodate larger learning rates.
# Regualarization
# Can afford to have less dropout
# Refer: https://arxiv.org/pdf/1502.03167.pdf


from tensorflow.keras.layers import BatchNormalization

x = layers.Dense(256, activation="relu")(inputs)
x = Dropout(0.1)(x)
x = BatchNormalization()(x)

x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.1)(x)
x = BatchNormalization()(x)

x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.1)(x)
x = BatchNormalization()(x)

x = layers.Dense(256, activation="relu")(x)
x = Dropout(0.1)(x)
outputs = layers.Dense(num_labels, activation="softmax")(x)

model = keras.Model(inputs=inputs, outputs=outputs, name="simple_model")
model.summary()


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


# Trainable Params for BatchNorm:  beta and gamma for each neuron: 256*3*2 = 1,536
# Non trainable Params: 256(neurons)*3(layers)*2 (mean vector and std-dev vector)= 1536
# 1536 + 1536 = 3072: total additional params as compared to non BatchNorm model.


%%time
model.fit(x_train, y_train, epochs=10, batch_size=batch_size)


# BatchNorm: 10 epochs to reach loss of 0.0404
# Without BatchNorm, 21 epochs to reach a loss of close to 0.0404 (above)









# Source: https://machinelearningmastery.com/adam-optimization-from-scratch/
from numpy import arange,asarray
from numpy import meshgrid
from matplotlib import pyplot

# objective function
def objective(x, y):
	return x**2.0 + y**2.0

# Countour Plot
# define range for input
bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])
# sample input range uniformly at 0.1 increments
xaxis = arange(bounds[0,0], bounds[0,1], 0.1)
yaxis = arange(bounds[1,0], bounds[1,1], 0.1)
# create a mesh from the axis
x, y = meshgrid(xaxis, yaxis)
# compute targets
results = objective(x, y)
# create a filled contour plot with 50 levels and jet color scheme
pyplot.contourf(x, y, results, levels=50, cmap='jet')
# show the plot
pyplot.show()


# derivative of objective function
def derivative(x, y):
	return asarray([x * 2.0, y * 2.0])



# gradient descent optimization with adam for a two-dimensional test function
from math import sqrt

def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):
	# generate an initial point
	x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])
	score = objective(x[0], x[1])
	# initialize first and second moments
	m = [0.0 for _ in range(bounds.shape[0])]
	v = [0.0 for _ in range(bounds.shape[0])]
	# run the gradient descent updates
	for t in range(n_iter):
		# calculate gradient g(t)
		g = derivative(x[0], x[1])
		# build a solution one variable at a time
		for i in range(x.shape[0]):
			# m(t) = beta1 * m(t-1) + (1 - beta1) * g(t)
			m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]
			# v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2
			v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2
			# mhat(t) = m(t) / (1 - beta1(t))
			mhat = m[i] / (1.0 - beta1**(t+1))
			# vhat(t) = v(t) / (1 - beta2(t))
			vhat = v[i] / (1.0 - beta2**(t+1))
			# x(t) = x(t-1) - alpha * mhat(t) / (sqrt(vhat(t)) + eps)
			x[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)
		# evaluate candidate point
		score = objective(x[0], x[1])
		# report progress
		print('>%d f(%s) = %.5f' % (t, x, score))
	return [x, score]



from numpy.random import rand
from numpy.random import seed

# seed the pseudo random number generator
seed(1)
# define range for input
bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])
# define the total iterations
n_iter = 200
# steps size
alpha = 0.01
# factor for average gradient
beta1 = 0.9
# factor for average squared gradient
beta2 = 0.999
# perform the gradient descent search with adam
best, score = adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2)
print('Done!')
print('f(%s) = %f' % (best, score))












#Source and Reference: https://blog.keras.io/building-autoencoders-in-keras.html


import keras
from keras import layers
from keras.datasets import mnist
import numpy as np


(x_train, _), (x_test, _) = mnist.load_data()

#Normalization of input
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

#Reshaping the images to 1D vectors
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print(x_train.shape)
print(x_test.shape)


#AutoEncoder model
input_img = keras.Input(shape=(784,))
encoded = layers.Dense(128, activation='relu')(input_img)
encoded = layers.Dense(64, activation='relu')(encoded)
encoded = layers.Dense(32, activation='relu')(encoded)

decoded = layers.Dense(64, activation='relu')(encoded)
decoded = layers.Dense(128, activation='relu')(decoded)
decoded = layers.Dense(784, activation='sigmoid')(decoded)


autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

autoencoder.fit(x_train, x_train,
                epochs=100,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))


#Visualize the outputs
import matplotlib.pyplot as plt

decoded_imgs = autoencoder.predict(x_test)

n = 10
plt.figure(figsize=(20, 4))
for i in range(1, n + 1):
    # Display original
    ax = plt.subplot(2, n, i)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # Display reconstruction
    ax = plt.subplot(2, n, i + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()





from keras.datasets import mnist
import numpy as np

(x_train, _), (x_test, _) = mnist.load_data()

#Normalize
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

#Reshape
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

#add NOISE
noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

print(x_train.shape)
print(x_train_noisy.shape)
print(x_test.shape)
print(x_test_noisy.shape)


n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n + 1):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test_noisy[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()



#AutoEncoder model
input_img = keras.Input(shape=(784,))
encoded = layers.Dense(128, activation='relu')(input_img)
encoded = layers.Dense(64, activation='relu')(encoded)
encoded = layers.Dense(32, activation='relu')(encoded)

decoded = layers.Dense(64, activation='relu')(encoded)
decoded = layers.Dense(128, activation='relu')(decoded)
decoded = layers.Dense(784, activation='sigmoid')(decoded)

# Compile and Fit
autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

autoencoder.fit(x_train_noisy, x_train, # NOTE: input is noisy, output is non-noisy
                epochs=100,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test_noisy, x_test))


#Visualize the outputs
import matplotlib.pyplot as plt

decoded_imgs = autoencoder.predict(x_test_noisy)

n = 10
plt.figure(figsize=(20, 4))
for i in range(1, n + 1):
    # Display original
    ax = plt.subplot(2, n, i)
    plt.imshow(x_test_noisy[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # Display reconstruction
    ax = plt.subplot(2, n, i + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()









