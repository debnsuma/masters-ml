{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYCz-t15fTzC"
   },
   "source": [
    "**lecture link :** https://www.scaler.com/meetings/i/dsml-advanced-ensemble-bagging-2/archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cjlgrAefbwZ"
   },
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_C5a606ff3I"
   },
   "source": [
    "1. Recap (2:00 - 24:06)\n",
    "2. Gini-Impurity (24:10 - 56:52)\n",
    "3. Overfit Vs Underfit (56:52 - 1:06:56)\n",
    "4. Sklearn Library ( 1:07:21 - 1:15:00)\n",
    "4. Geometrical Interpretation (1:16:20 - 1:20:27)\n",
    "5. Run time Complexity (1:36:20 - 1:45:00)\n",
    "6. Regression using a decision tree (1:46:31 - 1:59:45)\n",
    "7. Multi class classification (2:01:00)\n",
    "8. Interpretability (-)\n",
    "9. Feature Importance (2:03:00 - 2:11:00)\n",
    "10. Summary (2:11:00 - 2:12:20)\n",
    "11. Ensembles (2:12:20 - 2:28:30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6cc9gDofifX"
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3MFkye-fn8E"
   },
   "source": [
    "* In the previous lecture we saw how a decision tree is built basing on the concepts like **Entropy** and **Information Gain.**\n",
    "* We also saw how **Recursive Partitioning** works in building a decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5ptmFSkf5le"
   },
   "source": [
    "Let's take a quick racap of the concepts and application of the concepts using the example which we saw in previous class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qIbeThDhR8K"
   },
   "source": [
    "* Given a dataset where we have categorical features like Outlook, Temperature, Humidity and Windy basing on which we have to determine whether we can play tennis or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL6ELEHQjfb9"
   },
   "source": [
    "So, here what we do is split the data basing on the **Information Gain**\n",
    "* Where information gain is give by :\n",
    " * IG = Entropy at parent node - Weighted entropy at the child nodes\n",
    "* We pick feature which has maximum information gain\n",
    "* Here in the example we can see Information Gain for feature \"Outlook\" is maximum. So, we choose the same for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nYjQ-jEoPbG"
   },
   "source": [
    "<img src='https://drive.google.com/uc?id=1E2ZsduDaD-onw1ZAJKVgNeqLAqlP68cp'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gejQHxNpi2l"
   },
   "source": [
    "**What happens next.?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncu37BSno9KJ"
   },
   "source": [
    "\n",
    "\n",
    "* We keep on splitting further the each child node in such way that the information gain is maximum for the next child nodes.\n",
    "* We keep on doing the same till we get a pure node.\n",
    "\n",
    "* This is called as **Recursive Partitioning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeNDtovYqW7A"
   },
   "source": [
    "**What is a pure node.?**\n",
    "* A node which has data belonging to only one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCh_RvfRBhP-"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=14ivyDPcnRcXFzqWPQtgOgXoYHhVsZI00'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT-mbNXnp5kH"
   },
   "source": [
    "The **key lesson** here is\n",
    "* At every node of the decision tree we have to try every featutre and possible split to pick the best decision to split on\n",
    "* The best criteria is based on Information Gain, decision which has maximum information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txjcnnRlB4y9"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1qHlnRBD281GCH0gTp9Lkv_G-VdaZGRt3'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObC3BG5G5fUJ"
   },
   "source": [
    "**What if there are lots of features?**\n",
    "\n",
    "* If there are lots of features it takes a lot of time to check every possible split and compute entropy\n",
    "\n",
    "**So, what can be done about that.?**\n",
    "* We can use distributed computing or multi-processing\n",
    " * **Distributed Computing** is a process in which the data is split and worked on different cores.\n",
    "* We avoid using Decision trees as when the dimensionality is high a simple linear seperator can be used, by using logistic regression\n",
    "* Either use Random forest or GBDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFXV7jZs-Alg"
   },
   "source": [
    "**Can we use PCA for dimensioanlity reductionality.?**\n",
    "* PCA is principal component analysis which is used for dimensionality reduction\n",
    " * it is used to reduce data from $d$ dimensions to $d'$ dimensions where $d' < d$\n",
    "* Yes we can use it, but it might not be helpful always as PCA do not look at our y values\n",
    "* So, we can try and see if it works\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcSmXRuKCcdd"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1E4OjOt-0MJ-8Abebr6IOElYhnV3L2W_z'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9B5pIIGC6Ws"
   },
   "source": [
    "## Gini - Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hlISRDXEP0o"
   },
   "source": [
    "Gini impurity is also a measure like entropy which is used to measure purity\n",
    "let GI be gini impurity\n",
    "* Gini Impurity is given by\n",
    " * GI(y) = $1-∑_{i=1}^k(p(y_i)^2)$\n",
    " * For a binary classification:\n",
    "   * GI(y) = $1-[(p(y_i=1))^2+(p(y_i=0))^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwSOElN6GOFt"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1zBVRJWvFWt3PPMCqqDIlrc9MWVorSNQZ'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNtD_CGlGUtp"
   },
   "source": [
    "### How Gini-Impurity compares with entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KGDXKZDGkTD"
   },
   "source": [
    "**case 1:**\n",
    "\n",
    " let p($y_+$) be 0.5 and  p($y_-$) be 0.5\n",
    "* Here the entropy is 1\n",
    "* GI = 1 - (0.25 + 0.25) = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctVsczzoHhGI"
   },
   "source": [
    "**case 2:**\n",
    "\n",
    " let p($y_+$) be 1 and  p($y_-$) be 0\n",
    "* Here the entropy is 0\n",
    "* GI = 1 - (1 + 0) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa8ubKf-HxyT"
   },
   "source": [
    "* From the above two examples we can see that Gini-Impurity is high when entropy is high and it is low when the entropy is low\n",
    "* When the nodes are pure i.e for\n",
    " * if p($y_+$=1) and p($y_-$=0)\n",
    " * if p($y_+$=0) and p($y_-$=1)\n",
    "\n",
    " the Entropy and Gini-Impurity are zero\n",
    "* when the pronanility of p($y_+=\\frac{1}{2}$) and p($y_-=\\frac{1}{2}$) the entropy and Gini- Impurity are maximum\n",
    "\n",
    "* So, we can conclude that Gini-Impurity has same behaviour as entropy.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccCZ0LltIGrh"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1ei3q-Wt-Ts9_9BEV_FOKJZ1rqH68X3ck'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov_rxw_MKUJg"
   },
   "source": [
    "**Why Gini-Impurty is preffered over Entropy?**\n",
    "\n",
    "* Let's see the formula for both\n",
    " * Entropy = H(y) = $-∑_{i=1}^k p(y_i)*log(p(y_i))$\n",
    " * Gini-Impurity = GI(y) = $1-∑_{i=1}^k(p(y_i)^2)$\n",
    "* Here, as log is more computionally expensive than simple squaring, we choose GI and avoid entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlVlR4m8MYh-"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1w4R3c9MIzxstTALQkEE1pfCnrIrZzR1S'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNdMpd8fMw2_"
   },
   "source": [
    "* We can get Information gain by using Gini-Impurity also,\n",
    " * Information Gain = Weighted GI of the child nodes - GI of parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_wrwHyIOotR"
   },
   "source": [
    "### How do you split for numerical features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC1hYpFMPyWu"
   },
   "source": [
    "If we have categorical features we can simply split basing on features, but how do we do the splitting for numerical features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlF4VB3NQJVj"
   },
   "source": [
    "let us consider a numerical feature $f_1$ of $n$ values and categorical feature $f_2$\n",
    "* We compare the each value of $f_1$ with a threshold and split them basing on the threshold.\n",
    "\n",
    "\n",
    "**But, how do we choose the threshold?**\n",
    "* First we arrange $f_1$ in increasing order and set each value of $f_1$ as threshold and calculate the IG of that split. Which gives $n$ IG values say\n",
    "IG$_1^1$,IG$_2^1$,IG$_3^1$.....IG$_n^1$\n",
    "\n",
    "* Now, we compare these $n$ GI values of numerical feature and IG values of categorical feature (IG$_1^2$) and choose the split has maximum Information Gain (IG).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr4aPFDJWeX6"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1CD3wKwzwFOdFwwEXtsfp-GHD3Bo9BWYW'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaoucTltWlNN"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1VTifugVGsRwCTRbncdKhHobuthrNCk7w'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kAgu9RBV19i"
   },
   "source": [
    "* If the IG's of two different features are minimal you can pick any one and do the splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXMxhYI_U0co"
   },
   "source": [
    "* Computing IG for every feature is not very computationally efficient. so, algorithms follow set of rules to bin it carefuuly.\n",
    "* The purpose of binning is to make it more computationally efficient.\n",
    "* There are some techniques to do binning but the simplest binning method is to use Quantiles (Q1,Q2,Q3,Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgABfTXDWUzf"
   },
   "source": [
    "## Overfit Vs Underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQN6eRoSW5OM"
   },
   "source": [
    "**When do you think we will overfit a decision tree model?**\n",
    "\n",
    "* We can say that the model is overfit when we go on splitting, which increases the Depth of the tree.\n",
    " * **Depth** is the distance from the root node to the farthest leaf.\n",
    "* In simple terms we can say that , as the depth increses we overfit more and more\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1VHFXJCFKHwmiHTnBDAXS1MUMyVmLo0dw'>\n",
    "\n",
    "**But why?**\n",
    "\n",
    "* As the depth increases the data set becomes smaller.\n",
    "* At the leaf node i.e (pure node) the number of data points will be very less in number, which might be the outliers or noise.\n",
    "* This results in Overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJsfo0gIZf28"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1QdjAJqW1aUd4DGUXEHMlLloyvu1-rytJ'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncN1il-6ZuMH"
   },
   "source": [
    "**Now, what if the depth is too low?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71PaeiyNZ42H"
   },
   "source": [
    "* When the depth is too low it results in, **shallow tree**, a tree which has a very low depth.\n",
    "Let us assume a tree with depth 2, and has 500 +ve points and 200 -ve points at it's leaf node.\n",
    "* so for every query point $x_q$ we label it as +ve.\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=17CK5fc4hjWJoVQOLzs1RKL8iopA480jD'>\n",
    "\n",
    "**What if the depth is 0? (Extreme case)**\n",
    "\n",
    "We just decide on the root node's data points which is not efficient answer.\n",
    "This results in **Underfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxOZUOeLbjO1"
   },
   "source": [
    "So, here depth is the control\n",
    "* There methods to control depth, by giving some conditions like\n",
    " * Split only if atleast $n$ data points are there\n",
    " * Split only if IG is greater than some threshold value $x$\n",
    "* so, here **Depth** is our **Hyper parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdGe7uJ8cwfB"
   },
   "source": [
    "## Now, let's go through the library and see the terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXEbToB4eUYR"
   },
   "source": [
    "* **Criterion = gini**, states that make splits basing on gini impurity\n",
    "* **Splitter = best**, states that make the split basing on the best computed value instea of random values, these random values are used in Extremely Randomised Trees.\n",
    "<img src='https://drive.google.com/uc?id=1m1xst72zil-C-lTsiorVRP8IcidHK5Hx'>\n",
    "\n",
    "* We can set maximium depth upto which a tree can split by using **'max_depth'**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QnSJF8viqwZ"
   },
   "source": [
    "* **min_samples_split** is used to set the minimum number of data points required to split further. which helps us to control depth which therefore prevents **overfit**.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1eLlkvweWTXbjoIEkjvITReAtV8F_5jXx'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcA4o8Cwi50s"
   },
   "source": [
    "* **min_samples_leaf** helps us to set the minimum number of samples which a leaf node can have\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1RkFfAuVD5vEzhUoCciRVboLu2MT4Mo0_'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aScvhgoLjAhl"
   },
   "source": [
    "* **max_leaf_nodes** helps us to set the maximum number of leaf nodes that a tree can have. This also used to control depth of the tree.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1vnP-HaMRnNAc3a3AfRKlHulln8up2h1E'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epKABjgTjHtN"
   },
   "source": [
    "\n",
    "* **max_features** is used to set the features to be considered while deciding the best split.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1_7SOrt9M_aHLqbFtzSG_tpL8gmcHXgyg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLdB2Yo-jI7X"
   },
   "source": [
    "* **n_jobs** in random forest uses this paraemeter to give number of  processes that can be used to split these, in multi core system.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1Wmeo10UCzSsvtt5scPGSSStxT1I3x-33'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G8cXuyHkCbd"
   },
   "source": [
    "### Let's see another term **\"Decision Stump\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiaGZzuHu1Fh"
   },
   "source": [
    "* A Decision Stump is a Decision Tree with depth 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSh0pNLDvXf6"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1LW9DYPPyKUx5v50TxBAeirQOcEnUZT_F'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA9XSoUxkOsm"
   },
   "source": [
    "##Geometrical Interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBvLZRRBv4y3"
   },
   "source": [
    "Lets assume we have 2 features $f_1,f_2$\n",
    "* Geometrically every split is a axis parallel hyper-plane which devides your data space\n",
    "\n",
    "* In case of a **shallow tree**, the depth is less which means the splits are less which there by infer that the **number of hyper-planes deviding the data space are less** i.e less chances of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Viyn6RIxPIW"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1Yxhita92ZYwIHSUbIoNWZGygemAI76m5'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khyQb4JwxXlf"
   },
   "source": [
    "**What happes when the depth is high.?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q9y2gAGxewW"
   },
   "source": [
    "When the depth is high, we are breaking the  data space into many small divisions.\n",
    "* These small spaces very few data points , those might also be outliers or noise.\n",
    "* Hence the higher chances of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoNk0cX4yAoT"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1JMVOQ5vfvujC4ViAM2k9HcRGamMXNz80'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPq9Mzx7yPQT"
   },
   "source": [
    "**What is the impact of the outliers?**\n",
    "* Outliers impact a decision tree when the **depth is high**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcyPVYbaWMNj"
   },
   "source": [
    "**Is Standardisation  required for Decision tree?**\n",
    "\n",
    "* Though standardisation is required in optimisation based problems like Linear Regression, Logistic Regression, PCA, as standardisation **doesn't effect the entropy or the Information Gain** of the data it doesn't add any value in Decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aqO0LIAXPdJ"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1l44qKs25iQvz5qJfaQ2rRNB19RBc9SiA'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbVwTXbJXZBp"
   },
   "source": [
    "* How can we encode  a categorical feature with mnay categories like a zip code to a decision tree?  \n",
    "\n",
    "* We can do **\"no encoding\"**\n",
    " * if we did not do encoding, data set becomes too small\n",
    "* We can do **target encoding**\n",
    " * covert to numerical and then give it to decision tree\n",
    "* We cannot do **one hot encoding**\n",
    "  * Because that increases the dimensionality of the data\n",
    "\n",
    "**key lesson:**\n",
    "* Appropriate feature encoding depends on the model that you are using like if you are using logistic regression you can use one hat encoding but not in decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAQzP0YoDDAK"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1gtSEg36jCNkhdaAYOPuctlWEqJEH7l2j'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdCwgPjQ4ur3"
   },
   "source": [
    "## Run time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4uADt855zMo"
   },
   "source": [
    "let n be the number of data points , m be the number of nodes and d be the depth of the tree\n",
    "\n",
    "* The **Time complexity** of the tree of depth d is O(depth)\n",
    "* The **Space complexity** of the tree is O(m)\n",
    "\n",
    "Here,  depth is the function of number of nodes i.e log(m)   \n",
    " * If $d_{best}$ is computated using cross validation the decision tree will be very efficient at runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEd1KnQfB0CM"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1lgpPR9T2IxGxWsmUH9D8hO-5NTQtJFqE'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qthT9V2UBVCb"
   },
   "source": [
    "## How to do Regression using a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlfrJMoiBg8b"
   },
   "source": [
    "let's see what we were doing in Classifiaction first\n",
    "* In classification we had entropy and Gini Impurity but here fro regression we have to find an alternative for the same.\n",
    "* In leaf node when we had few values we given the query value the label of **majority data points** in the leaf node\n",
    "\n",
    "**What do we do in the Regression?**\n",
    "* In regression we take the mean or median of all values in the leaf node and give that value to the query node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXkBdbYpH_ae"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1-yRG4noCrjUYUll3iKna7lsZ5flGofYO'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njYXMnDqFtom"
   },
   "source": [
    "**But now, what is the alternative for Entropy**?\n",
    "* here, as we saw in linear regression already,we can use **Mean Squared Error or Median Absolute error**\n",
    "\n",
    "Let us assume a data D at root node with $y_i$ points which is split into D1 and D2\n",
    "* Now we calculate the MSE of these $y_i$ points in parent node and also the weighted MSE for the child nodes\n",
    " * Now, the difference between the MSE of parent node and weighted MSE of child nodes can be used as the criteria.\n",
    " * (weighted MSE of child nodes) - (MSE of parent node)\n",
    "\n",
    "* MSE is lowest when all $y_i$'s are same and high when they are diverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdr81NSnIKiT"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=10cOcLPE1gu2uwGkR91UdhjVa5WQSe-l_'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4QxqIriIiYe"
   },
   "source": [
    "Let's now see how the MSE is calculated.\n",
    "\n",
    "let us assume we have a data of $y$ data points\n",
    "* We consider the mean or medain of these points as predicted value i.e $ŷ_i$\n",
    "* Now we calculate the MSE or MAE for these points, this will be the MSE of parent node.\n",
    "* After the data is split\n",
    " * We now consider the mean of all values ($y_i'$) in child node as predicted value $ŷ_i'$\n",
    " * we now calculate the MSE or MAE for these in each child node and then calculate the weighted  MSE of the child nodes.\n",
    "* Then we find the difference and use this as the critearia to be compared among the features to decide the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPDDoE7IL4fA"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1vjLpifPMjCs6P7-iIrUsdHyviiFW0f7p'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxN_rwRjQ0bU"
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUcQpacdNZu0"
   },
   "source": [
    "**Is MAE better or MSE?**\n",
    "* MSAE would be a better choice as it is **resistant to outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTOeAiXfa3sA"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1a5FsNMltDHiteGJeER5aUrcwEq3N2kFU'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODkNXTitN74U"
   },
   "source": [
    " **What do we do if we have imbalanced data?**\n",
    "\n",
    " We have to rebalance the data beacuse the Entropy and Gini Impurity are fucntions of probability of classes\n",
    "* Assume  a condition where there is an imbalance in the root node\n",
    "  * having 99% positive data\n",
    "  * This skews the entropy or Gini Impurity\n",
    "* We can rebalance in many ways like\n",
    " * using class weights\n",
    " * up - samplinig\n",
    " * SMOTE can be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp0Z2b7BbGzO"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1DapAf45-GyB4NZ_jjB2Mu-sDgFgk7q3R'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hel_aeClPzzf"
   },
   "source": [
    "## Multi class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFh6NmauF8_a"
   },
   "source": [
    "#### How will DT work for multi class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1s9aViHQFlp"
   },
   "source": [
    "Whether it is binary or multiclass, DT works the same.\n",
    "\n",
    "We just need to calculate the Entropy or Gini Impurity.\n",
    "\n",
    "However, there is a catch\n",
    "* at the leaf node if we have more classes we take the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wot1LN-RWnrs"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=17owOUzkFT4ba2LkkU23M_x5Xlkeb9eIA'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm9XDwaqQrJQ"
   },
   "source": [
    "## Interpretability\n",
    "\n",
    "* Decision trees are very easy to interpret because they can be written as combination if-else statements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7Xt7WagcJHI"
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xni3XadmXMX8"
   },
   "source": [
    "\n",
    "\n",
    "* In case of logistic and linear regression after standardisation we can just consider  absolute value of weights which give us feature importance\n",
    "\n",
    "* In decision tree we compute a **Normalised Infromation Gain**\n",
    "\n",
    "Let us consider a feature $f_i$ which is used twice in splitting a decision tree with 10,000 points at the root node\n",
    "\n",
    "* At first split let the number of data points be 5000 and Infromation Gain is IG$_1$\n",
    "* Let number of data points at second split be 500 and Infromation Gain is IG$_2$\n",
    "\n",
    "* We calculate feature importaance of $f_i$ by\n",
    " * $f_i$ = IG$_1$ * $\\frac{5000}{10,000}$ + IG$_2$ * $\\frac{500}{10,000}$\n",
    "* These values of normalised infromation gain gives the feature importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTSKeOpqbaE1"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=12gJv36lGDumsuJ0bdafTB1Iq8bPpbs68'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXeqnEt5b9-x"
   },
   "source": [
    "* **Building an optimal Decicion Tree is a NP-Complete problem which is exponentially hard time complexity.**\n",
    "* So, we do **Greedy approximation**, which picks the feature which gives the best Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhHtVq5kB-bs"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1MPGvP0RVuw_yiXomwYH_D1DRms2VyzjH'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWMzXsKF7Y21"
   },
   "source": [
    "## Summary of Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTxatbr07ecp"
   },
   "source": [
    "To summarise everything what we have learnt about decision tree:\n",
    "\n",
    "1.   Decision Trees work well **when the d (depth) is not too large**\n",
    "2. They have a very low **Run-time complexity**\n",
    "3. **No standardisation** is needed.\n",
    "4. Depth is the key **hyper-parameter**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPaE3__iBpWP"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1qAHiBvWa19rqX-lI49DmmdZ4ATwoW9AB'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69Ik1Zaj8fQ_"
   },
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSUz2Vmj81s7"
   },
   "source": [
    "* The word ensemble in english means multiple things\n",
    "\n",
    "* Till now we have trained only one model\n",
    "\n",
    "* But what if we can **train multiple base learners or models** which are as different as possible and **combine them smartly**\n",
    " * Example: Instead of training one Decisioon tree we can train 100 decision trees and combine them\n",
    "* This is the key principle of ensembles\n",
    "\n",
    "* We can also train various machine learning models like Decision tree, logistic Regression, KNN and can combine them smartly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6reFKkJBvzc"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1PvJuXnvX6OY1_uAq8eOXFDjYvuxWh3vc'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tH7R4Lu-1um"
   },
   "source": [
    "There are four main types of ensemble\n",
    "1. Bagging\n",
    " * Example : Random Forest\n",
    "2. Boosting\n",
    " * Example : GBDT\n",
    "3. Stacking\n",
    "4. Cascading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHUIcfveB2vV"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1njR8DUne8941A1S9jwxGcG8JkzWxlVGM'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D93J6eW_-mv"
   },
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgq4u5EFABzh"
   },
   "source": [
    "* Bagging is simply the **Bootstrapped Aggregation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyXudRiEk1z6"
   },
   "source": [
    "### Intution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpgKZyAZlzhA"
   },
   "source": [
    "Let us assume a train data set $D$ with $n$ data points i.e $D_n$\n",
    "* Now, we **sample $m$ data points with replacement** to get $D'_m$\n",
    "* We do the sampling again for $m$ points to get $D'_2$, **Repeat the same for $k$ times** and we get $D'_k$\n",
    "* Now, we **train $k$ different models**($M_1,M_2,....M_k$) basing on the $k$ datasets obtained , there models are called **Base Learners**.\n",
    "* After training we **cross validate each model** **with remaining $n-m$ data points**\n",
    "* Now, we do **Aggregation**\n",
    " * We use majority vote for Classification\n",
    " * We use Mean/Median for Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4KxYF4Zo_Da"
   },
   "source": [
    "* **Working :**\n",
    "  * When a query point ($x_q$) is given, we pass that point through all the $k$ models and aggrigate the output of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snKHF1SxpzKE"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1G7iHV05GhViHb1t82gQJXze1NCfINMYh'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpqBZrQqpz1c"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?id=1oP6x44zrH2NvJ19EmCcBPuMEQBk1I9io'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZPIklyEqk7Y"
   },
   "source": [
    "**What is the challenge we are facing here?**\n",
    "\n",
    "* Here the major thing is the $k$ models should be different from one another as there is no use if all the models are same\n",
    "\n",
    "**But how to ensure that all the models are different?**\n",
    "* We can use different set of features or data points for different models which is **Feature Selection**\n",
    "* We can **tune the depth** of the tree\n",
    "* The models can be different if **number of points** $m$ in each sample  **is smaller**, This is **already hapenning in Row Sampling**  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
