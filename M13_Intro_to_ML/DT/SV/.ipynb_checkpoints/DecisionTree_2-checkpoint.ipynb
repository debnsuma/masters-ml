{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lecture link:**    https://www.scaler.com/meetings/i/dsml-advanced-decision-tree-2-2/archive"
      ],
      "metadata": {
        "id": "aFOVfk-lGI6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "\n",
        "1.   ROC and AUC (05:16 - 1:20:55)\n",
        "2.   PR-AUC (1:20:55 - 1:25:28)\n",
        "3. Decision Tree (1:29:50 - 2:28:56 )\n",
        "\n"
      ],
      "metadata": {
        "id": "bg1jtltRWF1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1rKiJsjBvD7HzQAu-2FtVlfCNlyP2EJJ3'>\n"
      ],
      "metadata": {
        "id": "6yGIVcucXlwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC and AUC"
      ],
      "metadata": {
        "id": "VNQd4Yn9WxgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   ROC is Receiver operating characteristic curve\n",
        "*   AUC is Area Under Curve"
      ],
      "metadata": {
        "id": "8CVd-HdXYISL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context"
      ],
      "metadata": {
        "id": "sf6SfXYPYsVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When you are working on a binary classification model, one of the metric to measure the model's performance is by visualising the ROC curve and compute AUC of the curve."
      ],
      "metadata": {
        "id": "kCPhdzGzYwCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps involved :"
      ],
      "metadata": {
        "id": "Pvf-ZbBOaq3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us assume a data set with $x_i$ and $y_i$ data points and $ŷ_i$ be the predicted value of y in the test data set."
      ],
      "metadata": {
        "id": "6TI1GV1Ga3of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:**\n",
        "* Sort the data in descending order of $ŷ$"
      ],
      "metadata": {
        "id": "NOZ1s6q_avG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1goX0u5KRGYyyvYPCqkm3_AYeQOt0WVza'>\n"
      ],
      "metadata": {
        "id": "DaqRWxXlbe0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:**\n",
        "* Set each $ŷ_i$ as threshold ($τ_i$) and classify as\n",
        " * If $ŷ_i > τ_i$ then classify as 1, else 0.\n",
        " * let these be $ỹ_{τ_i}$\n",
        "* continue doing the same for all $ŷ_i$ values.\n",
        "* Now calculate True Positive Rate$_i$ (TPR$_i$ ) and False Positive Rate$_i$  (FPR$_i$ ) for  $y$ and $ỹ_{τ_i}$"
      ],
      "metadata": {
        "id": "HYPYYDxtb2zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1npqw16RQ5CTAqsNheerRWdLMFtj6tCCo'>\n",
        "\n"
      ],
      "metadata": {
        "id": "deSB7z4mfcpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=12u21QmGmmzYbp5Vm_uxDG2FCNBeeHZ54'>\n",
        "\n"
      ],
      "metadata": {
        "id": "oanMSikNfm-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:**\n",
        "* Plot the graph between all TPR$_i$ and FPR$_i$ as we have $n$ pairs of TPR and FPR's"
      ],
      "metadata": {
        "id": "yzcVE9lagrAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1504V7rGGA5Ux5QJSuEtBWYzMRXKkJaYI'>\n",
        "\n"
      ],
      "metadata": {
        "id": "5UgCezO7jNQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This is how the graph looks for good model\n",
        "* TPR$_1$ is close to 0, as there is only 1 true positive because rest all values of $ỹ_{τ_1}$ are considered 0\n"
      ],
      "metadata": {
        "id": "Devj1yAejecZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* And now the AUC is the area under the curve.\n",
        "* Curve having more area represents good model.\n",
        "* The fundamental difference between other metrics and AU-ROC is that rest all the metrics like **precision, recall, F1-score** are calculated for **certain threshold** which is default=0.5, but **AU-ROC** gives the performance of model for **any set threshold.**\n"
      ],
      "metadata": {
        "id": "VOlbarWpkHSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The AUC for a random model is **0.5** and the curve will be **diagnol.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1h9YSC5bUvUqJwKRrNa5IU7HuH0JCmn0j'>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zjtcbcol2m7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Properties of AU-ROC"
      ],
      "metadata": {
        "id": "9jPeXd3i7VeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It does not work well for the **highly imbalanced data set**\n",
        "* AU-ROC does not depend on the actual values of the $ŷ_i$ but only depends on the order of them.\n",
        "* Random Model has AU-ROC of 0.5\n",
        "* When model's AOC is less than 0.5, the simple fix for the model is to invert your predictions. i.e After inversing you will get area of 1-(actual area value)"
      ],
      "metadata": {
        "id": "MPQdg2FG-ar7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1m5yrFECmTDxtOUMIJFobuQKn-OrWGBwD'>"
      ],
      "metadata": {
        "id": "WDwVeonnAPIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1WttXDqrDQCQg3_MIfw56sj49npF_DH6R'>"
      ],
      "metadata": {
        "id": "L_jigvVmAXIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision - Recall curve"
      ],
      "metadata": {
        "id": "h_FhbUDjR303"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Even though AU-ROC fails for highly imbalanced data set, Precision, Recall and F-1 Scores still are applicable for a set threshold.\n",
        "* Hence the precision-Recall curves are used.\n"
      ],
      "metadata": {
        "id": "V-JnBgUaSa_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  It is plotted by taking both precision and recall value for every threshold ($ŷ_i$) and plotting them.\n",
        "* The area under this curve is called AU-PRC, Area under precision recall curve, which is a very good metric for very highly imbalanced data."
      ],
      "metadata": {
        "id": "kqFzZUrqqon8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1jX5X6OYBZyTC_6WSkaugtgfZ4ve0EOQn'>"
      ],
      "metadata": {
        "id": "Nec2IkeEtzIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "cwUtlJZKtMAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Pure Node: If a node consists of points from only 1 class, it is called as **Pure node**\n",
        "* When there are points from 2 classes it is called as **Impure node**.\n"
      ],
      "metadata": {
        "id": "c-K9goYytTb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1dLxpMxVv_wRlUvPLhdycQafUBUctQ0-M'>"
      ],
      "metadata": {
        "id": "8Z2FSSY9yrBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Our main ***motive*** when creating a decision tree is to create decisions or conditions such that  we **have purer nodes as we keep splitting**.\n"
      ],
      "metadata": {
        "id": "3cT4k-MryKlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to measure the purity of the node.?**\n"
      ],
      "metadata": {
        "id": "YXikssK4yhh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy"
      ],
      "metadata": {
        "id": "-Xc3yBLCz_Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Entropy helps us measure purity which helps us to pick the best decision\n",
        "\n",
        "Let $y$ be the set of discrete random variable and $p(y_i) $ be the probability of $y_i$\n",
        "* Entropy is given by\n",
        " * $H(y) = -∑_{i=1}^k p(y_i)*log(p(y_i))$\n",
        "\n",
        "If $y$ is the set of discrete random variable and $p(y_i) $ be the probability of $y_i$\n",
        "* Entropy is given by\n",
        " * $H(y) = -\\int_{-∞}^{∞} p(y_i)*log(p(y_i) dy)$\n"
      ],
      "metadata": {
        "id": "RT2y9O8i0Dc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we calculate entropy for a set where $y=$ 0 or 1, by using the above formula wwe get **entropy is similar to negative of log-loss**\n",
        "* So, **minimising the log-loss is similar to maximising the entropy**, therefore logistic regression is also called as **maximum entropy model**"
      ],
      "metadata": {
        "id": "O__l0Y5oJFQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Dn9P1WsgkTwIIzhg_Wraayroe_vIdBon'>\n",
        "\n"
      ],
      "metadata": {
        "id": "thAaWvc-Jear"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now, plotting the graph you can see that,\n",
        " * Entropy is maximum when $p(y=1)$ is 0.5\n",
        " * Entropy is minimum when $p(y=1)$ is 1\n",
        " * Entropy is minimum when $p(y=1)$ is 0\n",
        "* So, minimum entropy means we only observe one class label, so this can be used to measure purity\n",
        " * i.e Our purest mode has minimum entropy\n",
        "\n",
        "So, we split so that we get minimum entropy.\n"
      ],
      "metadata": {
        "id": "WkfVHxQyJ4fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=17_WvonEZu652rgC93RvcsPwQY0bMKT5c'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Z--Alii-MIcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1OXhDsKAd7zFSeiJCmjpBdV41QRUMtIOj'>\n",
        "\n"
      ],
      "metadata": {
        "id": "vcmMTOsyMetv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see for the case of Binary classification where log of base 2 is considered\n",
        "* Entropy is 1, when the split is 50% each\n",
        "* Entropy is 0, when the node is pure\n",
        "* Entropy is 0.0801 when the split is as 99% and 1%"
      ],
      "metadata": {
        "id": "wD6YFT-TQNEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1S8vsmSWU2TqVt39yz1hfg4ueqj0F8ATi'>\n",
        "\n"
      ],
      "metadata": {
        "id": "55rjCHYkQ2Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So, by using this concept of entropy, we spilt the nodes of decicion tree by using a feature in such a way that **entropy decreases after each split**.\n",
        "* So that we can attain **maximum purity**.\n",
        "* And we keep doing this, which is called **Recursive partition.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1taHNhs13UsglTO8FzAAySAuzsjLvSgN_'>"
      ],
      "metadata": {
        "id": "q-ahgwmDRlv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play Tennis Example:\n"
      ],
      "metadata": {
        "id": "m9QWXQW6TcT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us consider four features like Outlook, Temperature, Humidity and Windy.\n",
        "\n",
        "Basing on these, we decide whether people play tennis in these situations or not."
      ],
      "metadata": {
        "id": "r-ewepcAVUAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1XIrYCftn07ScBTFmZ3_L-YHoZGpiFnic'>"
      ],
      "metadata": {
        "id": "uI5THEV9b7Xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Entropy at root node would be $-\\frac{9}{14}log\\frac{9}{14}- \\frac{5}{14}log\\frac{5}{14}$ from the data, which is 0.94"
      ],
      "metadata": {
        "id": "pgDOT-vJbHQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's split the data basing on Outlook data.\n",
        "and see the purity of the nodes and decide the features in which further split is made.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1a4jxkDzrJbFqeW1wHdk7GWZOkhW_xXaU'>"
      ],
      "metadata": {
        "id": "pdYHmClxcWfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now, do the same for other features also to find their entropy\n",
        "* We have to **choose the feature with less entropy** to classsify or split the data points further\n",
        "* In this case it's outlook as it has least entropy and more information gain.\n",
        "* **Information gain** = entropy of parent node - weighted entropy of child nodes\n",
        "* Information should  be high for an ideal model"
      ],
      "metadata": {
        "id": "sl7sqSgPqtQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SbZExs0ss2R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1UB0OZg1Kl5hOsBBrUjJwobBOn_zn0HN2'>"
      ],
      "metadata": {
        "id": "kok9M0ozrW4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=17GdFsLdbxvGWSy7Ag2ZI1ful57S0r0gR'>"
      ],
      "metadata": {
        "id": "tktiQ5cpsqPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have to do the **Recursive partition till the entropy becomes 0**."
      ],
      "metadata": {
        "id": "Ggj3nXbltPUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1AHstiYdRj31MvRFPyUQlVomTMQ7pwzfj'>"
      ],
      "metadata": {
        "id": "gRKV86oUtZly"
      }
    }
  ]
}